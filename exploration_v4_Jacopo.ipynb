{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Notebook v4 - Jacopo\n",
    "\n",
    "Writing this concurrently as v3 and v3_side\n",
    "\n",
    "Lets go back and work on word embedding.\n",
    "\n",
    "We have a god grasp of the whole problem and confidence that with the right amount of tweaking around we can pass the 0.9 treshold, nonetheless while we train a robust classifier we can also try to improve the word embedding here in this notebook. More precisely, we can leveage huge models like BERT and try to use them to improve our word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def build_feature_matrix(df, vocab, embeddings, mode='avg'):\n",
    "    X = np.zeros((df.shape[0], embeddings.shape[1]))\n",
    "    for i, tweet in enumerate(df['tweet']):\n",
    "        words = tweet.split()\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                X[i] += embeddings[vocab[word]]\n",
    "        if mode == 'avg':\n",
    "            X[i] /= len(words)\n",
    "        elif mode == 'sum':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(mode))\n",
    "    return X\n",
    "    \n",
    "def load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt'):\n",
    "    # Load data, txt as csv\n",
    "    #data_path = 'data/twitter-datasets/'\n",
    "    df_train_pos = pd.read_csv(path_pos, sep = '\\t', names = ['tweet'])\n",
    "    df_train_pos['label'] = 1\n",
    "    df_train_neg = pd.read_csv(path_neg, sep = '\\t', names = ['tweet'], on_bad_lines='skip')\n",
    "    df_train_neg['label'] = -1\n",
    "    df_train = pd.concat([df_train_pos, df_train_neg], ignore_index=True)\n",
    "    print('Train set: ', df_train.shape)\n",
    "    print('Train set positives: ', df_train_pos.shape)\n",
    "    print('Train set negatives: ', df_train_neg.shape)\n",
    "    return df_train   \n",
    "\n",
    "def load_test_data():\n",
    "    # Load test data: id, tweet for each row\n",
    "    data_path = 'data/twitter-datasets/'\n",
    "    df_test = pd.read_csv(data_path + 'test_data.txt', header=None, names=['line'], sep='\\t')\n",
    "    # Extract id and tweet, limit split by 1 so we don't split the tweet (this is v0, at least we keep it intact)\n",
    "    df_test['id'] = df_test['line'].apply(lambda x: x.split(',',1)[0]) \n",
    "    df_test['tweet'] = df_test['line'].apply(lambda x: x.split(',',1)[1])\n",
    "    df_test = df_test.drop('line', axis=1)\n",
    "    return df_test\n",
    "\n",
    "def predict_test_data(X_test, classifier, filename='submission.csv'):\n",
    "    # Predict test data and save to csv\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    df_test['Prediction'] = y_pred\n",
    "    df_test.rename(columns={'id': 'Id'}, inplace=True)\n",
    "    df_test['Prediction'] = df_test['Prediction'].apply(lambda x: -1 if x == 0 else x)\n",
    "    df_test.to_csv(filename, columns=['Id', 'Prediction'], index=False)\n",
    "    return df_test\n",
    "    \n",
    "def predict_test_data_pipeline(df_test, pipe, filename='submission.csv'):\n",
    "    # Predict test data and save to csv\n",
    "    y_pred = pipe.predict(df_test['tweet'])\n",
    "    df_test['Prediction'] = y_pred\n",
    "    df_test.rename(columns={'id': 'Id'}, inplace=True)\n",
    "    df_test['Prediction'] = df_test['Prediction'].apply(lambda x: -1 if x == 0 else x)\n",
    "    df_test.to_csv(filename, columns=['Id', 'Prediction'], index=False)\n",
    "    return df_test\n",
    "\n",
    "def train_test(clf, X_train, y_train, X_eval=None, y_eval=None, cv=None):\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    if X_eval is None:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_eval, y_train, y_eval = train_test_split(X_train, y_train, test_size=0.2)\n",
    "    if cv is not None:\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1, shuffle=True)\n",
    "        print('Cross validation Accuracy Scores: ', scores)\n",
    "        print('Cross validation mean score: ', scores.mean())\n",
    "        print('Cross validation std score: ', scores.std())\n",
    "        clf.fit(X_train, y_train)\n",
    "        return clf\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('Training set size: ', X_train.shape, ' Evaluation set size: ', X_eval.shape)\n",
    "    print('Metrics on evaluation set: ')\n",
    "    test(clf, X_eval, y_eval)\n",
    "    print('Metrics on training set to check overfitting/triviality of model: ')\n",
    "    test(clf, X_train, y_train)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print('Confusion matrix: ')\n",
    "    print(confusion_matrix(y_eval, clf.predict(X_eval)))\n",
    "    return clf\n",
    "\n",
    "def test(clf, X_eval, y_eval):\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    y_pred = clf.predict(X_eval)\n",
    "    print('Accuracy: ', accuracy_score(y_eval, y_pred))\n",
    "    print('F1 score: ', f1_score(y_eval, y_pred))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (196970, 2)\n",
      "Train set positives:  (97902, 2)\n",
      "Train set negatives:  (99068, 2)\n",
      "TfidfVectorizer(binary=True, min_df=3, ngram_range=(1, 4)) LinearSVC()\n"
     ]
    }
   ],
   "source": [
    "# Load data, lets work on partial data for now, we'll come back to this later\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "# Load vectorization and classifier obtained from the previous notebook as a reference\n",
    "with open('data/out/trained/tfidf_vectorizer-linSVC-pipeline-v2_4.pickle', 'rb') as f:\n",
    "    pipe = pickle.load(f)\n",
    "\n",
    "# we know that this achieves acc: 0.848\tf1: 0.850 and runs in less than 3min on full data from scratch\n",
    "# let's train a linear SVC on the partial data and see how it performs so we can compare and iterate\n",
    "svm = LinearSVC()\n",
    "vec_pipe = pipe.steps[0][1]\n",
    "svm_pipe = pipe.steps[1][1]\n",
    "print(vec_pipe, svm_pipe)\n",
    "\n",
    "# check svm_pipe performance on partial data, don't even modify the vectorizer\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(df_train['tweet'], df_train['label'], test_size=0.2)\n",
    "X_train = vec_pipe.transform(X_train)\n",
    "X_eval = vec_pipe.transform(X_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:22:25] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# straight up xgboost\u001b[39;00m\n\u001b[1;32m      2\u001b[0m xgb \u001b[39m=\u001b[39m XGBClassifier()\n\u001b[0;32m----> 3\u001b[0m xgb \u001b[39m=\u001b[39m train_test(xgb, X_train, y_train, X_eval, y_eval)\n",
      "Cell \u001b[0;32mIn [18], line 71\u001b[0m, in \u001b[0;36mtrain_test\u001b[0;34m(clf, X_train, y_train, X_eval, y_eval, cv)\u001b[0m\n\u001b[1;32m     69\u001b[0m     clf\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m clf\n\u001b[0;32m---> 71\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     72\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining set size: \u001b[39m\u001b[39m'\u001b[39m, X_train\u001b[39m.\u001b[39mshape, \u001b[39m'\u001b[39m\u001b[39m Evaluation set size: \u001b[39m\u001b[39m'\u001b[39m, X_eval\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     73\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMetrics on evaluation set: \u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/core.py:506\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    505\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 506\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/sklearn.py:1250\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1230\u001b[0m model, feval, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(xgb_model, eval_metric, params)\n\u001b[1;32m   1231\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1232\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1233\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     label_transform\u001b[39m=\u001b[39mlabel_transform,\n\u001b[1;32m   1248\u001b[0m )\n\u001b[0;32m-> 1250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1251\u001b[0m     params,\n\u001b[1;32m   1252\u001b[0m     train_dmatrix,\n\u001b[1;32m   1253\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1254\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1255\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1256\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1257\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1258\u001b[0m     feval\u001b[39m=\u001b[39;49mfeval,\n\u001b[1;32m   1259\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1260\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1261\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1262\u001b[0m )\n\u001b[1;32m   1264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/training.py:188\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(params, dtrain, num_boost_round\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, evals\u001b[39m=\u001b[39m(), obj\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, feval\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    116\u001b[0m           maximize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, evals_result\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m           verbose_eval\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, xgb_model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, callbacks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    118\u001b[0m     \u001b[39m# pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[39m\"\"\"Train a booster with given parameters.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m    Booster : a trained booster model\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     bst \u001b[39m=\u001b[39m _train_internal(params, dtrain,\n\u001b[1;32m    189\u001b[0m                           num_boost_round\u001b[39m=\u001b[39;49mnum_boost_round,\n\u001b[1;32m    190\u001b[0m                           evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m    191\u001b[0m                           obj\u001b[39m=\u001b[39;49mobj, feval\u001b[39m=\u001b[39;49mfeval,\n\u001b[1;32m    192\u001b[0m                           xgb_model\u001b[39m=\u001b[39;49mxgb_model, callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    193\u001b[0m                           verbose_eval\u001b[39m=\u001b[39;49mverbose_eval,\n\u001b[1;32m    194\u001b[0m                           evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m    195\u001b[0m                           maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    196\u001b[0m                           early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds)\n\u001b[1;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m bst\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/training.py:81\u001b[0m, in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m callbacks\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m     80\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m callbacks\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m     83\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/core.py:1680\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_features(dtrain)\n\u001b[1;32m   1679\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1680\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1681\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1682\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1683\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1684\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# straight up xgboost\n",
    "xgb = XGBClassifier()\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157576, 3018685)  Evaluation set size:  (39394, 3018685)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8108341371782505\n",
      "F1 score:  0.8196515004840271\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.930896837081789\n",
      "F1 score:  0.9329127415886785\n",
      "Confusion matrix: \n",
      "[[15008  4973]\n",
      " [ 2479 16934]]\n"
     ]
    }
   ],
   "source": [
    "# that's neat, fast and not overfitting, let's try to improve it\n",
    "# naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb = train_test(nb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:15:37] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (157576, 3018685)  Evaluation set size:  (39394, 3018685)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8019495354622531\n",
      "F1 score:  0.8114730330562537\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8461821597197543\n",
      "F1 score:  0.8539703578744426\n",
      "Confusion matrix: \n",
      "[[14801  5054]\n",
      " [ 2748 16791]]\n"
     ]
    }
   ],
   "source": [
    "# honestly bayes is fucking king\n",
    "# whatever, let's keep xgboost and try to improve it just for fun at this point\n",
    "# aparently xgboost is not advised when data is overparametrized but ok\n",
    "xgb = XGBClassifier(\n",
    "    max_depth=10,\n",
    "    alpha=0,\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:26:20] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:576: \n",
      "Parameters: { \"binary\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[00:26:20] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 360 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:21] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 388 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:22] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 314 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:22] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 376 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:23] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 296 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:23] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 280 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:24] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 266 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:24] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 328 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:25] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 174 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:25] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 186 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:26] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 190 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:27] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 190 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:27] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 134 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:28] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 248 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:28] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 136 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:29] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 260 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:30] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 136 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:30] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:31] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 202 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:31] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 122 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:32] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 142 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:32] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:33] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 238 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:33] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 150 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:34] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:34] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 170 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:34] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 184 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:35] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 238 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:35] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 138 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:36] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 150 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:36] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 146 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:36] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:37] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 146 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:37] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 136 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:38] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 160 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:38] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:39] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 204 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:39] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 132 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:39] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:40] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 146 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:40] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:40] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 140 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:41] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 170 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:41] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 150 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:42] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:42] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:42] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:43] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 178 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:43] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 156 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:43] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:44] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:44] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 156 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:45] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 134 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:45] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:45] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:46] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 112 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:46] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:46] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:47] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 178 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:47] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 132 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:47] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 150 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:48] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 128 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:48] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 140 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:49] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 138 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:49] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:49] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:50] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 144 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:50] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 144 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:50] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:51] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:51] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 128 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:52] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:52] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:52] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 198 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:53] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 116 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:53] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 128 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:54] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 138 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:54] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 190 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:54] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 144 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:55] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:55] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:55] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 136 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:56] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:56] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 162 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:56] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 112 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:57] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 170 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:57] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:58] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 188 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:58] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 160 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:59] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:59] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:26:59] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 164 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:27:00] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:27:00] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:27:00] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:27:01] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:27:01] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:27:02] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:27:02] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=10\n",
      "[00:27:02] INFO: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/tree/updater_prune.cc:101: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=10\n",
      "Training set size:  (157576, 3018685)  Evaluation set size:  (39394, 3018685)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8006549220693506\n",
      "F1 score:  0.8090084393316633\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8363519825354114\n",
      "F1 score:  0.8432887068446864\n",
      "Confusion matrix: \n",
      "[[14909  4946]\n",
      " [ 2907 16632]]\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    max_depth=10,\n",
    "    alpha=0.1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='error',\n",
    "    subsample=0.5,\n",
    "    verbosity=2,\n",
    "    num_parallel_tree=1,\n",
    "    binary='logistic',\n",
    "\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157576, 3018685)  Evaluation set size:  (39394, 3018685)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8049956846220236\n",
      "F1 score:  0.8127985183741105\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8289841092552165\n",
      "F1 score:  0.8357950667836599\n",
      "Confusion matrix: \n",
      "[[15035  4820]\n",
      " [ 2862 16677]]\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    max_depth=10,\n",
    "    alpha=0.1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='error',\n",
    "    subsample=0.2,\n",
    "    verbosity=0,\n",
    "    num_parallel_tree=10,\n",
    "    objective='binary:logistic',\n",
    "\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vec_pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m df_train \u001b[39m=\u001b[39m load_train_data(path_pos\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/twitter-datasets/train_pos_full.txt\u001b[39m\u001b[39m'\u001b[39m, path_neg\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/twitter-datasets/train_neg_full.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m X_train, X_eval, y_train, y_eval \u001b[39m=\u001b[39m train_test_split(df_train[\u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m], df_train[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m], test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m X_train_vec \u001b[39m=\u001b[39m vec_pipe\u001b[39m.\u001b[39mtransform(X_train)\n\u001b[1;32m      6\u001b[0m X_eval_vec \u001b[39m=\u001b[39m vec_pipe\u001b[39m.\u001b[39mtransform(X_eval)\n\u001b[1;32m      7\u001b[0m nb \u001b[39m=\u001b[39m MultinomialNB()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vec_pipe' is not defined"
     ]
    }
   ],
   "source": [
    "# Ok, let's try to improve the vectorizer\n",
    "# let's first use again the whole thing on full data\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt')\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(df_train['tweet'], df_train['label'], test_size=0.2)\n",
    "X_train_vec = vec_pipe.transform(X_train)\n",
    "X_eval_vec = vec_pipe.transform(X_eval)\n",
    "nb = MultinomialNB()\n",
    "nb = train_test(nb, X_train, y_train, X_eval, y_eval)\n",
    "linsvc = LinearSVC()\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (1966636, 3018685)  Evaluation set size:  (491659, 3018685)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7926164272392044\n",
      "F1 score:  0.8062163127846071\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.7955818972092447\n",
      "F1 score:  0.8090283253875811\n",
      "Confusion matrix: \n",
      "[[177596  70222]\n",
      " [ 31740 212101]]\n"
     ]
    }
   ],
   "source": [
    "# at least now we know bayes is king but still a bit less complex\n",
    "# than linear SVC,\n",
    "# and we also know linear svc is overfitting but not as much as it could be\n",
    "# let's try with xgboost\n",
    "xgb = XGBClassifier()\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (1966636, 3018685)  Evaluation set size:  (491659, 3018685)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7765768550967236\n",
      "F1 score:  0.7981469979676514\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.7829135640759144\n",
      "F1 score:  0.8038922967104545\n",
      "Confusion matrix: \n",
      "[[164636  83182]\n",
      " [ 26666 217175]]\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    max_depth=10,\n",
    "    alpha=0.1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='error',\n",
    "    subsample=0.2,\n",
    "    verbosity=0,\n",
    "    objective='binary:hinge',\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    max_depth=6,\n",
    "    alpha=0,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    subsample=0.5,\n",
    "    verbosity=0,\n",
    "    objective='binary:logistic',\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#word_vectors = gensim.models.KeyedVectors.load_word2vec_format('data/glove/glove.twitter.27B.200d.txt', binary=False)\n",
    "#word_vectors.save('data/glove_custom/glove.twitter.27B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101298, 150)\n",
      "(1193517, 200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "v0_embeddings = np.load('data/glove_custom/embeddings.npy')\n",
    "v4_embeddings = np.load('data/glove_custom/glove.twitter.27B.200d.txt.vectors.npy')\n",
    "print(v0_embeddings.shape)\n",
    "print(v4_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext\n",
    "from gensim.models import FastText\n",
    "#fasttexxt = FastText()\n",
    "#fasttexxt.build_vocab(corpus_iterable=df_train['tweet'])\n",
    "#fasttexxt.train(corpus_iterable=df_train['tweet'], total_examples=fasttexxt.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('you', 0.8460860252380371), ('much', 0.7890047430992126), ('always', 0.7601684331893921), ('know', 0.7598055005073547), ('my', 0.7519950270652771), ('and', 0.7513090372085571), ('loves', 0.7512385249137878), ('life', 0.7443934679031372), ('it', 0.7426838874816895), (\"n't\", 0.7408117055892944)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/gensim/models/keyedvectors.py:850: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    }
   ],
   "source": [
    "#fasttexxt.save('data/fasttext/fasttext.model')\n",
    "\n",
    "glove = gensim.models.KeyedVectors.load_word2vec_format('data/glove/glove.twitter.27B.200d.txt', binary=False)\n",
    "print(glove.most_similar('love'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('people', 0.7922294735908508), (\"n't\", 0.7866110801696777), ('why', 0.7847471237182617), ('fuck', 0.7806913256645203), ('dont', 0.770850419998169), ('when', 0.7697439193725586), ('swear', 0.7657259702682495), ('really', 0.7627530694007874), ('stupid', 0.7585508227348328), ('seriously', 0.7504260540008545)]\n",
      "[('birthday', 0.8998554944992065), ('day', 0.8070886731147766), ('bday', 0.7734537720680237), ('wish', 0.7571845650672913), ('merry', 0.726503849029541), ('love', 0.7250142693519592), ('year', 0.7109401226043701), ('you', 0.7065563797950745), ('hope', 0.7004973292350769), ('thank', 0.6997925043106079)]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key ':)' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(glove\u001b[39m.\u001b[39mmost_similar(\u001b[39m'\u001b[39m\u001b[39mhate\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(glove\u001b[39m.\u001b[39mmost_similar(\u001b[39m'\u001b[39m\u001b[39mhappy\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(glove\u001b[39m.\u001b[39;49mmost_similar(\u001b[39m'\u001b[39;49m\u001b[39m:)\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(glove\u001b[39m.\u001b[39mmost_similar(\u001b[39m'\u001b[39m\u001b[39m:(\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/gensim/models/keyedvectors.py:842\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m         weight[idx] \u001b[39m=\u001b[39m item[\u001b[39m1\u001b[39m]\n\u001b[1;32m    841\u001b[0m \u001b[39m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mean_vector(keys, weight, pre_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, post_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ignore_missing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    843\u001b[0m all_keys \u001b[39m=\u001b[39m [\n\u001b[1;32m    844\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, _KEY_TYPES) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    845\u001b[0m ]\n\u001b[1;32m    847\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/gensim/models/keyedvectors.py:519\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    517\u001b[0m         total_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(weights[idx])\n\u001b[1;32m    518\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_missing:\n\u001b[0;32m--> 519\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present in vocabulary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    521\u001b[0m \u001b[39mif\u001b[39;00m(total_weight \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m    522\u001b[0m     mean \u001b[39m=\u001b[39m mean \u001b[39m/\u001b[39m total_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key ':)' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "print(glove.most_similar('hate'))\n",
    "print(glove.most_similar('happy'))\n",
    "print(glove.most_similar(':)'))\n",
    "print(glove.most_similar(':('))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', ' ', 'l', 'o', 'v', 'e', ' ', 'y', 'o', 'u']\n"
     ]
    }
   ],
   "source": [
    "# augment glove with n-grams\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# add bigrams and trigrams to docs (only ones that appear 5 times or more).\n",
    "bigram = Phrases(df_train['tweet'], min_count=5)\n",
    "trigram = Phrases(bigram[df_train['tweet']], min_count=10)\n",
    "quadrigram = Phrases(trigram[bigram[df_train['tweet']]], min_count=100)\n",
    "\n",
    "print(trigram[bigram['i love you']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', ' ', 'l', 'o', 'v', 'e', ' ', 'y', 'o', 'u']\n",
      "['i', ' ', 'l', 'o', 'v', 'e', ' ', 'y', 'o', 'u', ' ', 'b', 'a', 'b', 'y']\n"
     ]
    }
   ],
   "source": [
    "print(bigram['i love you'])\n",
    "print(trigram['i love you baby'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15', 'vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>']\n"
     ]
    }
   ],
   "source": [
    "print(bigram[df_train['tweet'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', '_', 'l', 'o', 'v', 'e', '_', 'y', 'o', 'u']\n"
     ]
    }
   ],
   "source": [
    "print(bigram['i_love_you'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipping a step but trying directly to combine word embeddings with tfidf vectorizer\n",
    "idea: word embedding + tfidf vectorizer on top to have a phrase embedding/vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n",
      "[('', 0.5666552186012268), ('h', 0.3975861072540283), ('>', 0.3211832642555237), (':', 0.2996082007884979), ('x', 0.29522085189819336), ('{', 0.28220418095588684), ('}', 0.2754124701023102), ('r', 0.25764912366867065), ('(', 0.2549425959587097), ('^', 0.23890458047389984)]\n"
     ]
    }
   ],
   "source": [
    "# first lets settle on an embedding and make sure we can use it\n",
    "# let's try again the glove one, but at this point training a fasttext model on our data\n",
    "# seems less of a hussle than trying to make the glove one work\n",
    "import gensim\n",
    "from gensim.models import FastText\n",
    "# HERE we include grams and subword windows -- actualllly gemsim does not support this but the original faceboook fasttext does\n",
    "# whatever, with this word embedding we are still retaiing context and a certain 'meaning' of the words\n",
    "# the real context intended as phrases (tweet) will be recovered hopefully with a combination of word embeddings in tf-idf that we'll try\n",
    "# we can there use n-grams on the word embeddings :)\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt')\n",
    "# fasttext = FastText(\n",
    "#     corpus_file=None, # df_train['tweet'] ?\n",
    "#     vector_size=20, \n",
    "#     window=5, \n",
    "#     epochs=10,\n",
    "#     min_count=5,\n",
    "#     sg=0, # sg ({1, 0}, optional)  Training algorithm: skip-gram if sg=1, otherwise CBOW.\n",
    "#     negative=0, # negative (int, optional)  If > 0, negative sampling will be used, the int for negative specifies how many noise words should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "#     cbow_mean=0, # cbow_mean ({1,0}, optional)  If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
    "#     min_n=None, # min_n (int, optional)  Min length of char ngrams to be used for training word representations.\n",
    "#     max_n=None, # max_n (int, optional)  Max length of char ngrams to be used for training word representations. Set min_n to be greater than max_n to avoid char ngrams being used.\n",
    "#     word_ngrams=1, #  (int, optional)  In Facebooks FastText, max length of word ngram - but gensim only supports the default of 1 (regular unigram word handling).\n",
    "# )\n",
    "\n",
    "fasttext = FastText(vector_size=20, window=5, min_count=5, sg=0, negative=0, cbow_mean=0, word_ngrams=1)\n",
    "fasttext.build_vocab(corpus_iterable=df_train['tweet'])\n",
    "\n",
    "#fasttext.save('data/fasttext/fasttext_20.model')\n",
    "print(fasttext.wv.most_similar('love'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 0.5666552186012268), ('h', 0.3975861072540283), ('>', 0.3211832642555237), (':', 0.2996082007884979), ('x', 0.29522085189819336), ('{', 0.28220418095588684), ('}', 0.2754124701023102), ('r', 0.25764912366867065), ('(', 0.2549425959587097), ('^', 0.23890458047389984)]\n"
     ]
    }
   ],
   "source": [
    "# noisy as hell but at least we got it to work\n",
    "fasttext.train(corpus_iterable=df_train['tweet'], total_examples=fasttext.corpus_count, epochs=10)\n",
    "print(fasttext.wv.most_similar('love'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2', 0.41747480630874634), ('{', 0.33628779649734497), ('', 0.32545962929725647), ('r', 0.2515483796596527), (';', 0.24647396802902222), ('0', 0.23436297476291656), ('w', 0.2108132392168045), ('3', 0.19439783692359924), ('~', 0.19127322733402252), ('', 0.18923039734363556)]\n"
     ]
    }
   ],
   "source": [
    "# so absolutely nothing changed??\n",
    "print(fasttext.wv.most_similar('hate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[('', 0.5666552186012268), ('h', 0.3975861072540283), ('>', 0.3211832642555237), (':', 0.2996082007884979), ('x', 0.29522085189819336), ('{', 0.28220418095588684), ('}', 0.2754124701023102), ('r', 0.25764912366867065), ('(', 0.2549425959587097), ('^', 0.23890458047389984)]\n",
      "[('&', 0.5322758555412292), (' ', 0.4602575898170471), (',', 0.456903874874115), ('z', 0.4248853325843811), ('y', 0.42446547746658325), (\"'\", 0.41426488757133484), ('g', 0.3792472779750824), ('j', 0.3564785122871399), ('d', 0.3537110388278961), ('n', 0.3388735055923462)]\n"
     ]
    }
   ],
   "source": [
    "fasttext = FastText(\n",
    "    min_count = 5,\n",
    "    window = 5,\n",
    "    vector_size = 20,\n",
    "    min_n=3,\n",
    ")\n",
    "fasttext.build_vocab(corpus_iterable=df_train['tweet'])\n",
    "print(fasttext.max_vocab_size)\n",
    "print(fasttext.wv.most_similar('love'))\n",
    "fasttext.train(corpus_iterable=df_train['tweet'], total_examples=fasttext.corpus_count, epochs=10)\n",
    "print(fasttext.wv.most_similar('love'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'e', 'o', 't', 'a', 's', 'i', 'r', 'n', 'l']\n",
      "['@', '\\\\', '`', '}', '{', '', '', '', '\\x13', '']\n",
      "(2000000, 20)\n",
      "(75, 20)\n",
      "[ 0.03919867 -0.25482583  0.21448834  0.22237524  0.12658215  0.17024063\n",
      "  0.27242953  0.08672871  0.062164    0.08897415  0.01590549 -0.10961515\n",
      " -0.0808576   0.10215715  0.26773533  0.09722336 -0.32588804  0.15129927\n",
      "  0.1893925  -0.18371277]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'love'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(fasttext\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mshape) \u001b[39m# we have 75 words in the vocab?\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(fasttext\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mget_mean_vector(\u001b[39m'\u001b[39m\u001b[39mlove\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m \u001b[39mprint\u001b[39m(fasttext\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mkey_to_index[\u001b[39m'\u001b[39;49m\u001b[39mlove\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(fasttext\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mkey_to_index[\u001b[39m'\u001b[39m\u001b[39mhate\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'love'"
     ]
    }
   ],
   "source": [
    "# ok so, clearly we need to build a better vocab anyway\n",
    "# lets print a few words and see what we get\n",
    "print(fasttext.wv.index_to_key[:10])\n",
    "print(fasttext.wv.index_to_key[-10:])\n",
    "print(fasttext.wv.vectors_ngrams.shape)\n",
    "print(fasttext.wv.vectors.shape) # we have 75 words in the vocab?\n",
    "print(fasttext.wv.get_mean_vector('love'))\n",
    "print(fasttext.wv.key_to_index['love'])\n",
    "print(fasttext.wv.key_to_index['hate'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# tokenize\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m TweetTokenizer\n\u001b[0;32m----> 3\u001b[0m tokenized \u001b[39m=\u001b[39m TweetTokenizer()\u001b[39m.\u001b[39;49mtokenize(df_train[\u001b[39m'\u001b[39;49m\u001b[39mtweet\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(tokenized)\n\u001b[1;32m      5\u001b[0m fasttext\u001b[39m.\u001b[39mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39mdf_train[\u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/nltk/tokenize/casual.py:344\u001b[0m, in \u001b[0;36mTweetTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"Tokenize the input text.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[39m:param text: str\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mthe original string if `preserve_case=False`.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39m# Fix HTML character entities:\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m text \u001b[39m=\u001b[39m _replace_html_entities(text)\n\u001b[1;32m    345\u001b[0m \u001b[39m# Remove username handles\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrip_handles:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/nltk/tokenize/casual.py:275\u001b[0m, in \u001b[0;36m_replace_html_entities\u001b[0;34m(text, keep, remove_illegal, encoding)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m remove_illegal \u001b[39melse\u001b[39;00m match\u001b[39m.\u001b[39mgroup(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m ENT_RE\u001b[39m.\u001b[39;49msub(_convert_entity, _str_to_unicode(text, encoding))\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#tokenized = TweetTokenizer().tokenize(df_train['tweet'][])\n",
    "#print(tokenized)\n",
    "#fasttext.build_vocab(corpus_iterable=df_train['tweet'].apply(lambda x: x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2458295 entries, 0 to 1239639\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   tweet   object\n",
      " 1   label   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 88.5+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2458295 entries, 0 to 1239639\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   tweet   object\n",
      " 1   label   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 88.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_train['tweet']\n",
    "print(df_train.info())\n",
    "df_train['tweet'] = df_train['tweet'].astype(str)\n",
    "print(df_train.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<user>', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god', 'knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenized = TweetTokenizer().tokenize(df_train['tweet'].iloc[0])\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<user>', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god', 'knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer(\n",
    "    preserve_case=True,\n",
    "    reduce_len=True,\n",
    ")\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tokenizer.tokenize(x))\n",
    "print(df_train['tweet'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000000, 20)\n",
      "(101317, 20)\n"
     ]
    }
   ],
   "source": [
    "# let's go back to fasttext\n",
    "fasttext = FastText(\n",
    "    min_count = 5,\n",
    "    window = 5,\n",
    "    vector_size = 20,\n",
    "    min_n=3,\n",
    ")\n",
    "fasttext.build_vocab(corpus_iterable=df_train['tweet'])\n",
    "print(fasttext.wv.vectors_ngrams.shape)\n",
    "print(fasttext.wv.vectors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2458295\n",
      "39285957\n",
      "[(\"m'love\", 0.9681973457336426), ('i.love', 0.9674108624458313), ('jlove', 0.9663768410682678), ('1love', 0.9662020802497864), ('lovvve', 0.9587792158126831), ('iilove', 0.9561951160430908), ('llove', 0.9544649124145508), ('lovve', 0.9524462819099426), ('looove', 0.9511415362358093), ('loveeed', 0.9445867538452148)]\n"
     ]
    }
   ],
   "source": [
    "print(fasttext.corpus_count)\n",
    "print(fasttext.corpus_total_words)\n",
    "#fasttext.train(corpus_iterable=df_train['tweet'], total_examples=fasttext.corpus_count, epochs=10)\n",
    "print(fasttext.wv.most_similar('love'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ihate', 0.960334837436676), ('#nohate', 0.9171327352523804), ('dislike', 0.9118636250495911), ('hatee', 0.911791205406189), ('#ihate', 0.9037554860115051), ('haterz', 0.8958828449249268), ('haterzz', 0.8894347548484802), ('#dislike', 0.886340320110321), ('hateee', 0.8822832703590393), ('lovehate', 0.87751305103302)]\n"
     ]
    }
   ],
   "source": [
    "# now we are talking\n",
    "print(fasttext.wv.most_similar('hate'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<user>', '!', 'i', 'the', ',', '.', 'to', 'you', '(', '<url>']\n",
      "['tapers', 'penuh', '19001', 'boaters', 'frazzle', '#422', 'beantown', 'farida', 'christiana', 'freeney']\n",
      "[(\"8':\", 0.8468958735466003), ('#nss28', 0.8268639445304871), ('3q', 0.8250125646591187), ('6u', 0.8229467272758484), ('[=', 0.8187487721443176), ('_7', 0.8165414929389954), ('n_n', 0.8092526793479919), ('', 0.8038870096206665), (':{', 0.7867059707641602), ('pz', 0.7862887382507324)]\n",
      "[('9j', 0.9037322402000427), (':*)', 0.8949961066246033), (\"8':\", 0.8809409737586975), ('p0', 0.8714581727981567), ('6u', 0.8621630072593689), ('p:', 0.862082302570343), ('pq', 0.8602790236473083), ('r51e', 0.8591496348381042), ('7v', 0.8559269905090332), ('{:', 0.854961633682251)]\n",
      "[(':-p', 0.8968167901039124), (':-\\\\', 0.8957145810127258), (':-|', 0.8839971423149109), (':-@', 0.8579592704772949), (':-[', 0.8515523076057434), (':-d', 0.8502748608589172), ('eeyah', 0.8284174203872681), ('eeeh', 0.8256227374076843), ('aahw', 0.8028940558433533), ('okeh', 0.800393283367157)]\n",
      "[(':-[', 0.9782513976097107), (':-@', 0.9655786156654358), (':-\\\\', 0.9552002549171448), (':-|', 0.9385866522789001), (':-]', 0.9125661849975586), (':-p', 0.8882441520690918), (':p', 0.858615517616272), (':-/', 0.8565486073493958), ('eeeh', 0.8418540954589844), ('>:p', 0.8410717248916626)]\n"
     ]
    }
   ],
   "source": [
    "print(fasttext.wv.index_to_key[:10])\n",
    "print(fasttext.wv.index_to_key[-10:])\n",
    "print(fasttext.wv.most_similar(':)')) # these shouldn't be too acurate, afterall they should not be present\n",
    "print(fasttext.wv.most_similar(':('))\n",
    "print(fasttext.wv.most_similar(':-)')) # but this should, and is!\n",
    "print(fasttext.wv.most_similar(':-('))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weloveyou', 0.9377278089523315), ('loveyou', 0.9368027448654175), ('iloveyoussosomuch', 0.9240853190422058), ('#istillloveyou', 0.9231034517288208), ('loveyousomuch', 0.9222390651702881), ('loveeyou', 0.9198563694953918), ('iloveyou', 0.9183106422424316), ('iloveyousomuch', 0.9162368774414062), ('#iloveyousomuch', 0.9134881496429443), ('iloveyouguys', 0.9127840399742126)]\n"
     ]
    }
   ],
   "source": [
    "# Great, let's now expand this to include the ngrams\n",
    "print(fasttext.wv.most_similar('I love you'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"m'love\", 0.9359018206596375), ('1love', 0.9302533864974976), ('loveeed', 0.9280291795730591), ('jlove', 0.9263998866081238), ('i.love', 0.9255800247192383), ('loveed', 0.9235557317733765), ('lovehate', 0.9079862833023071), ('llove', 0.898972749710083), ('loveu', 0.8952510356903076), ('#1love', 0.8940567970275879)]\n",
      "[('loveeyou', 0.8116405010223389), ('loveyou', 0.8003208041191101), ('#youloveme', 0.7956029176712036), (\"idon't\", 0.7909086346626282), ('me.you', 0.7894436120986938), ('too.you', 0.7875821590423584), (\"don'ts\", 0.7851004004478455), (\"don't\", 0.7849775552749634), ('4you', 0.783201277256012), (\"d'you\", 0.782477617263794)]\n",
      "[('loveyou', 0.8963254690170288), ('loveeyou', 0.893829345703125), ('weloveyou', 0.8708831071853638), ('#istillloveyou', 0.858667254447937), ('iloveyou', 0.8488462567329407), ('loveyouu', 0.8444809317588806), ('me.you', 0.8437607288360596), ('#iloveyou', 0.8336917161941528), ('#weloveyou', 0.83107590675354), ('loveyouliam', 0.8277154564857483)]\n",
      "[('wanting', 0.8441185355186462), ('pleasing', 0.8278986811637878), ('allyoursheartsoulalwayslove', 0.8158441185951233), ('everyting', 0.8140416741371155), ('#10thingsihateaboutyou', 0.8125396370887756), ('goodthing', 0.8095529079437256), ('readhowyouwant', 0.8054481744766235), ('#freakythingsiwoulddotoyou', 0.8045513033866882), ('someting', 0.8037118911743164), ('mucking', 0.8014190196990967)]\n",
      "[('loveyousomuch', 0.8435161709785461), ('#feelingthelove', 0.8358202576637268), ('iloveyoussosomuch', 0.8278319239616394), ('#feelinthelove', 0.8162314295768738), ('iloveyousomuch', 0.81484454870224), ('allyoursheartsoulalwayslove', 0.8142730593681335), ('#sweetlove', 0.8086893558502197), ('#iloveyousomuch', 0.8056691884994507), ('lovemaking', 0.7979719042778015), ('#livelaughlove', 0.793809175491333)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fasttext = FastText(\n",
    "    min_count = 5,\n",
    "    window = 5,\n",
    "    vector_size = 100,\n",
    "    min_n=3, \n",
    "    max_n=6,\n",
    "    sg=0, #cbow\n",
    ")\n",
    "fasttext.build_vocab(corpus_iterable=df_train['tweet'])\n",
    "fasttext.train(corpus_iterable=df_train['tweet'], total_examples=fasttext.corpus_count, epochs=10)\n",
    "print(fasttext.wv.most_similar('love'))\n",
    "print(fasttext.wv.most_similar(\"I don't love you\"))\n",
    "print(fasttext.wv.most_similar('I love you'))\n",
    "print(fasttext.wv.most_similar('I hate wanting you so much'))\n",
    "print(fasttext.wv.most_similar('I hate loving you so much <3'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#fasttext.save('data/fasttext/fasttext_100.model')\n",
    "# load\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText.load('data/fasttext/fasttext_100.model')\n",
    "# let's use this with simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n",
      "(2458295, 100)\n",
      "[ 0.97016688  0.20003064 -4.03440614  0.45517573 -0.28249551  0.35427921\n",
      "  1.8911416   0.20286558  0.63157106 -2.87167854 -3.39766136  1.92036216\n",
      " -1.14403208  3.09840214  0.55869466 -1.10047139 -1.8481549   0.70465623\n",
      "  0.27437835  1.03170613 -2.53220977 -1.50201934 -0.24134134 -0.37451546\n",
      " -0.80676103 -1.66658795 -2.42787469 -0.95312001  2.69939766 -1.00389883\n",
      " -1.17416897 -0.82312816 -0.32662999  0.2079889   1.88435278 -0.76844877\n",
      " -0.02880284  2.11518678 -0.04662244  0.1312566   2.15882026  0.29642264\n",
      " -1.70965011 -0.59289174  0.21062202  2.45253244 -1.8116421  -0.15546217\n",
      "  1.31620138 -0.63307249 -0.22296252  1.92281728 -0.52396292  0.18152309\n",
      "  0.40592332 -1.27735976  1.10320234  0.878496   -0.15603699 -1.65937777\n",
      "  0.06945011 -0.87143017 -1.3542627   0.80353592  0.74737335 -0.04481394\n",
      " -2.40723957  2.40495597  0.0172052  -2.15293321  0.04629602 -2.45589297\n",
      "  2.59419857  0.73294131  1.24858165  0.60318745  0.57720474  2.59429912\n",
      "  0.30406093 -1.94348689 -1.42913941  0.32478389  2.96282319 -1.15905204\n",
      "  1.28591673 -0.21623457 -3.49125279  1.38952968 -1.75847515  0.51274329\n",
      "  1.84850147  1.26475753 -0.17085    -0.02467644  0.16082364 -1.76504702\n",
      "  0.26914881  2.79153202  2.14514363  0.63388945]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# let's test  this, later we will try to combine it to expand to ngrams for words\n",
    "# for now we do a simple average of the word vectors in the tweet\n",
    "def get_tweet_vector(tweet, vectorizer, dim=100):\n",
    "    tweet_vector = np.zeros(dim)\n",
    "    vec = vectorizer\n",
    "    for word in tweet:\n",
    "        tweet_vector += vec.get_vector(word)\n",
    "    tweet_vector /= len(tweet)\n",
    "    return tweet_vector\n",
    "\n",
    "def bld_feature_matrix(df, vectorizer, dim=100, tokenizer=None):\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    if tokenizer is None:\n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=True,\n",
    "            reduce_len=True,\n",
    "        )\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    X = np.zeros((df.shape[0], dim))\n",
    "    for i, tweet in enumerate(df['tweet']):\n",
    "        X[i] = get_tweet_vector(tweet, vectorizer, dim)\n",
    "    return X\n",
    "\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt')\n",
    "X_train = bld_feature_matrix(df_train, fasttext.wv, dim=100)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1966636, 100)\n",
      "(491659, 100)\n",
      "(1966636,)\n",
      "(491659,)\n"
     ]
    }
   ],
   "source": [
    "# now usual split and train and eval\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train, df_train['label'], test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_eval.shape)\n",
    "print(y_train.shape)\n",
    "print(y_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (1966636, 100)  Evaluation set size:  (491659, 100)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6260741692921313\n",
      "F1 score:  0.6828372885800447\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.6255956872547843\n",
      "F1 score:  0.682921703751583\n",
      "Confusion matrix: \n",
      "[[109911 138399]\n",
      " [ 45445 197904]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "nb = GaussianNB()\n",
    "nb = train_test(nb, X_train, y_train, X_eval, y_eval)\n",
    "linsvc = LinearSVC()\n",
    "#linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear svc usually is very fast but it's having problems with scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_eval_scaled = scaler.transform(X_eval)\n",
    "linsvc = LinearSVC()\n",
    "# still not working in reasonable time\n",
    "#linsvc = train_test(linsvc, X_train_scaled, y_train, X_eval_scaled, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1966636, 100)\n",
      "(491659, 100)\n",
      "[-0.30796533 -0.41258597 -1.46436788  1.65222853 -0.53770074 -0.09499411\n",
      "  0.71472956 -1.10555175  0.56355063 -0.78662993 -0.16649955  1.2987345\n",
      " -0.81053144  0.66438871  0.22367088 -1.16044089 -0.8451649   0.57563337\n",
      "  1.67690981  0.24567452 -0.46736754  0.58348306  0.74724952  1.26649956\n",
      " -0.1019182  -0.7953835  -0.22728432  0.4571953  -0.21853914 -0.78806941\n",
      "  0.33144348 -0.29990412 -0.26121149 -0.65861397 -0.22771421 -0.30817866\n",
      " -1.65371003 -0.52256453  0.3242577   0.48413643 -0.23213804 -0.89732991\n",
      " -0.10198301  1.19259531  0.83455859  0.90904943  0.40933952 -1.22993366\n",
      "  0.24259977  0.99966029 -2.03361107  0.54586261  1.51129443  1.11514888\n",
      "  1.38356981 -0.31188875  0.35826367  0.28131472 -0.34615435 -0.53410088\n",
      "  0.06221348  0.67561666 -0.01961437 -0.27392226 -1.49538253 -0.62687008\n",
      " -0.70066072  0.33942782  0.08098166 -0.14730184 -0.51601347  0.65913079\n",
      " -0.19285041  0.61625509  1.26309728 -0.47064715 -0.1737989   0.00784422\n",
      " -0.15227795 -0.35688401 -0.25492791 -0.01309686  1.00783958 -0.02379498\n",
      "  0.19260843  0.54959266  0.10201332  0.55823683  0.18499512 -0.29792009\n",
      " -0.30244377  0.5543319   0.75821574 -0.52173401  0.26907973 -1.01534639\n",
      " -0.01382119 -0.33421342 -0.70586734 -0.43668702]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled.shape)\n",
    "print(X_eval_scaled.shape)\n",
    "print(X_train_scaled[0])\n",
    "# map to -1, 1\n",
    "y_train_1 = y_train.apply(lambda x: -1 if x == 0 else 1)\n",
    "y_eval_1 = y_eval.apply(lambda x: -1 if x == 0 else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (1966636, 100)  Evaluation set size:  (491659, 100)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6260741692921313\n",
      "F1 score:  0.6828372885800447\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.6255956872547843\n",
      "F1 score:  0.682921703751583\n",
      "Confusion matrix: \n",
      "[[109911 138399]\n",
      " [ 45445 197904]]\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb = train_test(nb, X_train_scaled, y_train_1, X_eval_scaled, y_eval_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n",
      "<gensim.interfaces.TransformedCorpus object at 0x16c043df0>\n",
      "<gensim.interfaces.TransformedCorpus object at 0x16c041960>\n"
     ]
    }
   ],
   "source": [
    "# as of now this is definitely worse than the LSI on tfidf\n",
    "# let's build a better feature matrix which includes the ngrams\n",
    "# instead of doing it retroactively we should go back and extract phrases\n",
    "# we then join bi, tri and quadgrams from the phrases\n",
    "# and feed them back to the fasttext model with something like _ to separate the words\n",
    "# we can then actually expect vectors that should be close if they are similar, as phrases\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer(\n",
    "    preserve_case=True,\n",
    "    reduce_len=True,\n",
    ")\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt')\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tokenizer.tokenize(x))\n",
    "phrases = Phrases(df_train['tweet'], min_count=5, threshold=10)\n",
    "bigram = Phraser(phrases)\n",
    "print(bigram[df_train['tweet'][0]])\n",
    "print(bigram[df_train['tweet'][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    <user> i dunno justin read my mention or not ....\n",
      "0    vinco tresorpack 6 ( difficulty 10 of 10 objec...\n",
      "Name: tweet, dtype: object\n",
      "0    < u s e r >   i   d u n n o   j u s t i n   r ...\n",
      "0    v i n c o   t r e s o r p a c k   6   (   d i ...\n",
      "Name: tweet, dtype: object\n",
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n",
      "0    [<user>, i, dunno, justin, read, my, mention, ...\n",
      "0    [vinco, tresorpack, 6, (, difficulty, 10, of, ...\n",
      "Name: tweet, dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# let's join the various ngrams\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(tweets[\u001b[39m0\u001b[39m])\n\u001b[0;32m---> 19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(bigram[tweets[\u001b[39m0\u001b[39;49m]]))\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(trigram[bigram[tweets[\u001b[39m0\u001b[39m]]]))\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(quadgram[trigram[bigram[tweets[\u001b[39m0\u001b[39m]]]]))\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "print(df_train['tweet'][0])\n",
    "# revert them back to strings\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: ' '.join(x))\n",
    "print(df_train['tweet'][0])\n",
    "\n",
    "# now, let's build the phrases\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt')\n",
    "tweets = df_train['tweet'].apply(lambda x: tokenizer.tokenize(x))\n",
    "phrases = Phrases(tweets, min_count=5, threshold=10)\n",
    "bigram = Phraser(phrases)\n",
    "trigram = Phrases(bigram[tweets], min_count=5, threshold=10)\n",
    "trigram = Phraser(trigram)\n",
    "quadgram = Phrases(trigram[bigram[tweets]], min_count=5, threshold=10)\n",
    "quadgram = Phraser(quadgram)\n",
    "\n",
    "# let's join the various ngrams\n",
    "print(tweets[0])\n",
    "print(' '.join(bigram[tweets[0]]))\n",
    "print(' '.join(trigram[bigram[tweets[0]]]))\n",
    "print(' '.join(quadgram[trigram[bigram[tweets[0]]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15 vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n",
      "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15 vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n",
      "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15 vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>\n",
      "['<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15', 'vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>']\n",
      "2\n",
      "['<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15', 'vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>']\n",
      "2\n",
      "['<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15', 'vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>']\n",
      "2\n",
      "0    [<user>, i, dunno, justin, read, my, mention, ...\n",
      "0    [vinco, tresorpack, 6, (, difficulty, 10, of, ...\n",
      "Name: tweet, dtype: object\n",
      "2\n",
      "<gensim.interfaces.TransformedCorpus object at 0x2d7ca5ba0>\n",
      "2\n",
      "<gensim.interfaces.TransformedCorpus object at 0x2d7ca5180>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "tweets_ = tweets.apply(lambda x: ' '.join(x))\n",
    "print(' '.join(bigram[tweets_[0]]))\n",
    "print(' '.join(trigram[bigram[tweets_[0]]]))\n",
    "print(' '.join(quadgram[trigram[bigram[tweets_[0]]]]))\n",
    "print(bigram[tweets_[0]])\n",
    "print(len(bigram[tweets_[0]]))\n",
    "print(trigram[bigram[tweets_[0]]])\n",
    "print(len(trigram[bigram[tweets_[0]]]))\n",
    "print(quadgram[trigram[bigram[tweets_[0]]]])\n",
    "print(len(quadgram[trigram[bigram[tweets_[0]]]]))\n",
    "\n",
    "print(tweets[0])\n",
    "print(len(tweets[0]))\n",
    "print(bigram[tweets[0]])\n",
    "print(len(bigram[tweets[0]]))\n",
    "print(trigram[bigram[tweets[0]]])\n",
    "print(len(trigram[bigram[tweets[0]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x2d7ca63b0>\n",
      "2\n",
      "['<user>', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god_knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15']\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "bigram = Phrases(tweets, min_count=5, threshold=10)\n",
    "print(bigram[tweets[0]])\n",
    "print(len(bigram[tweets[0]]))\n",
    "txts = [bigram[tweet] for tweet in tweets]\n",
    "print(txts[0])\n",
    "print(len(txts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    <user> i dunno justin read my mention or not ....\n",
      "0    vinco tresorpack 6 ( difficulty 10 of 10 objec...\n",
      "Name: tweet, dtype: object\n",
      "0    [<user>, i, dunno, justin, read, my, mention, ...\n",
      "0    [vinco, tresorpack, 6, (, difficulty, 10, of, ...\n",
      "Name: tweet, dtype: object\n",
      "['<user>', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god_knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15']\n",
      "26\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(df_train['tweet'][0])\n",
    "print(tweets[0])\n",
    "print(txts[0]) # ok nice finnally, we see 'god_knows'\n",
    "print(len(txts[0]))\n",
    "print(len(tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15', 'vinco tresorpack 6 ( difficulty 10 of 10 object : disassemble and reassemble the wooden pieces this beautiful wo ... <url>']\n"
     ]
    }
   ],
   "source": [
    "#unigram = [w for w in df_train['tweet'].apply(lambda x: tokenizer.tokenize(x))]\n",
    "bigram_ = Phrases(df_train['tweet'])\n",
    "print(bigram_[df_train['tweet'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<user>': 1605564,\n",
       " 'i': 993728,\n",
       " '<user>_i': 130014,\n",
       " 'dunno': 1585,\n",
       " 'i_dunno': 908,\n",
       " 'justin': 7326,\n",
       " 'dunno_justin': 3,\n",
       " 'read': 9545,\n",
       " 'justin_read': 4,\n",
       " 'my': 401454,\n",
       " 'read_my': 977,\n",
       " 'mention': 3158,\n",
       " 'my_mention': 135,\n",
       " 'or': 48578,\n",
       " 'mention_or': 42,\n",
       " 'not': 123274,\n",
       " 'or_not': 1552,\n",
       " '.': 741613,\n",
       " 'not_.': 992,\n",
       " 'only': 42126,\n",
       " '._only': 807,\n",
       " 'only_justin': 20,\n",
       " 'and': 456724,\n",
       " 'justin_and': 876,\n",
       " 'god': 16044,\n",
       " 'and_god': 260,\n",
       " 'knows': 4329,\n",
       " 'god_knows': 194,\n",
       " 'about': 64390,\n",
       " 'knows_about': 57,\n",
       " 'that': 201425,\n",
       " 'about_that': 2167,\n",
       " ',': 745845,\n",
       " 'that_,': 2344,\n",
       " 'but': 157940,\n",
       " ',_but': 28853,\n",
       " 'but_i': 31660,\n",
       " 'hope': 36913,\n",
       " 'i_hope': 14351,\n",
       " 'you': 601241,\n",
       " 'hope_you': 8696,\n",
       " 'will': 82101,\n",
       " 'you_will': 5997,\n",
       " 'follow': 92157,\n",
       " 'will_follow': 1840,\n",
       " 'me': 342534,\n",
       " 'follow_me': 33612,\n",
       " '#believe': 1057,\n",
       " 'me_#believe': 39,\n",
       " '15': 5627,\n",
       " '#believe_15': 2,\n",
       " 'because': 26275,\n",
       " 'your': 138981,\n",
       " 'because_your': 423,\n",
       " 'logic': 795,\n",
       " 'your_logic': 6,\n",
       " 'is': 311182,\n",
       " 'logic_is': 94,\n",
       " 'so': 200921,\n",
       " 'is_so': 8893,\n",
       " 'dumb': 1673,\n",
       " 'so_dumb': 131,\n",
       " 'dumb_,': 59,\n",
       " ',_i': 54265,\n",
       " \"won't\": 14491,\n",
       " \"i_won't\": 4181,\n",
       " 'even': 29446,\n",
       " \"won't_even\": 277,\n",
       " 'crop': 152,\n",
       " 'even_crop': 1,\n",
       " 'out': 81691,\n",
       " 'crop_out': 1,\n",
       " 'out_your': 367,\n",
       " 'name': 12021,\n",
       " 'your_name': 1807,\n",
       " 'name_or': 46,\n",
       " 'or_your': 182,\n",
       " 'photo': 6005,\n",
       " 'your_photo': 86,\n",
       " 'photo_.': 112,\n",
       " 'tsk': 307,\n",
       " '._tsk': 61,\n",
       " 'tsk_.': 61,\n",
       " '<url>': 526859,\n",
       " '._<url>': 11232,\n",
       " 'just': 152683,\n",
       " '<user>_just': 8677,\n",
       " 'put': 12519,\n",
       " 'just_put': 488,\n",
       " 'casper': 59,\n",
       " 'put_casper': 1,\n",
       " 'in': 297185,\n",
       " 'casper_in': 2,\n",
       " 'a': 522593,\n",
       " 'in_a': 19651,\n",
       " 'box': 5292,\n",
       " 'a_box': 362,\n",
       " '!': 1001676,\n",
       " 'box_!': 109,\n",
       " 'looved': 4,\n",
       " '!_looved': 2,\n",
       " 'the': 761278,\n",
       " 'looved_the': 1,\n",
       " 'battle': 1186,\n",
       " 'the_battle': 253,\n",
       " 'battle_!': 23,\n",
       " '#crakkbitch': 1,\n",
       " '!_#crakkbitch': 1,\n",
       " '<user>_<user>': 201828,\n",
       " 'thanks': 68850,\n",
       " '<user>_thanks': 27420,\n",
       " 'sir': 1571,\n",
       " 'thanks_sir': 15,\n",
       " '>': 40634,\n",
       " 'sir_>': 4,\n",
       " '>_>': 13961,\n",
       " \"don't\": 84729,\n",
       " \">_don't\": 17,\n",
       " 'trip': 4035,\n",
       " \"don't_trip\": 51,\n",
       " 'lil': 6234,\n",
       " 'trip_lil': 1,\n",
       " 'mama': 2150,\n",
       " 'lil_mama': 57,\n",
       " '...': 513595,\n",
       " 'mama_...': 23,\n",
       " '..._just': 1677,\n",
       " 'keep': 17284,\n",
       " 'just_keep': 656,\n",
       " 'doin': 1612,\n",
       " 'keep_doin': 35,\n",
       " 'ya': 14018,\n",
       " 'doin_ya': 7,\n",
       " 'thang': 803,\n",
       " 'ya_thang': 26,\n",
       " 'thang_!': 84,\n",
       " 'visiting': 526,\n",
       " 'visiting_my': 53,\n",
       " 'brother': 7700,\n",
       " 'my_brother': 2865,\n",
       " 'tmr': 1128,\n",
       " 'brother_tmr': 1,\n",
       " 'tmr_is': 46,\n",
       " 'is_the': 18973,\n",
       " 'bestest': 461,\n",
       " 'the_bestest': 231,\n",
       " 'birthday': 24123,\n",
       " 'bestest_birthday': 1,\n",
       " 'gift': 3037,\n",
       " 'birthday_gift': 168,\n",
       " 'eveerrr': 6,\n",
       " 'gift_eveerrr': 1,\n",
       " 'eveerrr_!': 3,\n",
       " '!_!': 241857,\n",
       " 'yay': 6187,\n",
       " '<user>_yay': 1811,\n",
       " 'yay_!': 2544,\n",
       " '#lifecompleted': 1,\n",
       " '!_#lifecompleted': 1,\n",
       " '#lifecompleted_.': 1,\n",
       " 'tweet': 25984,\n",
       " '._tweet': 176,\n",
       " '/': 109240,\n",
       " 'tweet_/': 212,\n",
       " 'facebook': 4821,\n",
       " '/_facebook': 19,\n",
       " 'facebook_me': 14,\n",
       " 'to': 703723,\n",
       " 'me_to': 10741,\n",
       " 'let': 20187,\n",
       " 'to_let': 1514,\n",
       " 'let_me': 9000,\n",
       " 'know': 96038,\n",
       " 'me_know': 2960,\n",
       " 'please': 81690,\n",
       " 'know_please': 27,\n",
       " '#1dnextalbumtitle': 199,\n",
       " '<user>_#1dnextalbumtitle': 86,\n",
       " ':': 170358,\n",
       " '#1dnextalbumtitle_:': 9,\n",
       " 'feel': 36312,\n",
       " ':_feel': 16,\n",
       " 'for': 306234,\n",
       " 'feel_for': 280,\n",
       " 'for_you': 14102,\n",
       " 'you_/': 71,\n",
       " 'rollercoaster': 63,\n",
       " '/_rollercoaster': 4,\n",
       " 'of': 339771,\n",
       " 'rollercoaster_of': 7,\n",
       " 'life': 32178,\n",
       " 'of_life': 998,\n",
       " 'life_.': 2212,\n",
       " 'song': 13833,\n",
       " '._song': 34,\n",
       " 'cocept': 4,\n",
       " 'song_cocept': 4,\n",
       " 'cocept_:': 4,\n",
       " ':_life': 171,\n",
       " 'life_,': 1790,\n",
       " '#yolo': 698,\n",
       " ',_#yolo': 10,\n",
       " '#yolo_,': 5,\n",
       " 'becoming': 825,\n",
       " ',_becoming': 27,\n",
       " 'famous': 1906,\n",
       " 'becoming_famous': 13,\n",
       " '?': 334751,\n",
       " 'famous_?': 56,\n",
       " '<3': 57215,\n",
       " '?_<3': 1977,\n",
       " '14': 3324,\n",
       " '<3_14': 12,\n",
       " '#followmeplz': 3,\n",
       " '14_#followmeplz': 3,\n",
       " '#followmeplz_!': 3,\n",
       " '!_<3': 7607,\n",
       " 'x15': 49,\n",
       " '<3_x15': 1,\n",
       " 'workin': 539,\n",
       " 'hard': 14648,\n",
       " 'workin_hard': 15,\n",
       " 'hard_or': 38,\n",
       " 'hardly': 879,\n",
       " 'or_hardly': 9,\n",
       " 'hardly_workin': 4,\n",
       " 'rt': 142355,\n",
       " 'workin_rt': 1,\n",
       " 'rt_<user>': 104227,\n",
       " 'at': 116282,\n",
       " '<user>_at': 2619,\n",
       " \"hardee's\": 8,\n",
       " \"at_hardee's\": 2,\n",
       " 'with': 201658,\n",
       " \"hardee's_with\": 1,\n",
       " 'with_my': 16409,\n",
       " 'future': 3939,\n",
       " 'my_future': 529,\n",
       " 'coworker': 69,\n",
       " 'future_coworker': 1,\n",
       " 'coworker_<user>': 3,\n",
       " 'saw': 11000,\n",
       " 'i_saw': 4607,\n",
       " 'saw_.': 33,\n",
       " \"i'll\": 40014,\n",
       " \"._i'll\": 2772,\n",
       " 'be': 178880,\n",
       " \"i'll_be\": 8283,\n",
       " 'replying': 606,\n",
       " 'be_replying': 10,\n",
       " 'replying_in': 3,\n",
       " 'bit': 8793,\n",
       " 'a_bit': 4854,\n",
       " 'bit_.': 332,\n",
       " 'this': 223177,\n",
       " 'this_is': 22838,\n",
       " 'were': 24029,\n",
       " 'is_were': 20,\n",
       " 'were_i': 106,\n",
       " 'belong': 364,\n",
       " 'i_belong': 60,\n",
       " 'anddd': 170,\n",
       " '<user>_anddd': 29,\n",
       " 'anddd_to': 1,\n",
       " 'cheer': 2935,\n",
       " 'to_cheer': 682,\n",
       " '#nationals2013': 1,\n",
       " 'cheer_#nationals2013': 1,\n",
       " '#nationals2013_?': 1,\n",
       " 'we': 92259,\n",
       " 'send': 7288,\n",
       " 'we_send': 19,\n",
       " 'an': 51887,\n",
       " 'send_an': 27,\n",
       " 'invitation': 504,\n",
       " 'an_invitation': 54,\n",
       " 'invitation_to': 55,\n",
       " 'shop': 2107,\n",
       " 'to_shop': 180,\n",
       " 'on-line': 23,\n",
       " 'shop_on-line': 1,\n",
       " 'on-line_!': 2,\n",
       " 'here': 37633,\n",
       " '!_here': 232,\n",
       " 'here_you': 213,\n",
       " 'find': 14884,\n",
       " 'will_find': 509,\n",
       " 'everything': 15880,\n",
       " 'find_everything': 9,\n",
       " 'everything_you': 836,\n",
       " 'need': 49337,\n",
       " 'you_need': 5007,\n",
       " '-': 200486,\n",
       " 'need_-': 12,\n",
       " 'without': 13218,\n",
       " '-_without': 39,\n",
       " 'leaving': 4899,\n",
       " 'without_leaving': 6,\n",
       " 'home': 32926,\n",
       " 'leaving_home': 19,\n",
       " 'home_...': 512,\n",
       " '..._<url>': 332218,\n",
       " 'woke': 5594,\n",
       " 'just_woke': 1235,\n",
       " 'up': 100343,\n",
       " 'woke_up': 4941,\n",
       " 'up_,': 3104,\n",
       " 'finna': 1393,\n",
       " ',_finna': 23,\n",
       " 'go': 87989,\n",
       " 'finna_go': 165,\n",
       " 'go_to': 21358,\n",
       " 'church': 1822,\n",
       " 'to_church': 339,\n",
       " 'agreed': 698,\n",
       " '<user>_agreed': 317,\n",
       " 'agreed_!': 228,\n",
       " '12': 13383,\n",
       " '!_12': 61,\n",
       " 'more': 45132,\n",
       " '12_more': 41,\n",
       " 'days': 16834,\n",
       " 'more_days': 711,\n",
       " 'left': 11489,\n",
       " 'days_left': 329,\n",
       " 'tho': 16417,\n",
       " 'left_tho': 11,\n",
       " 'monet': 53,\n",
       " 'monet_with': 1,\n",
       " 'katemelo': 1,\n",
       " 'with_katemelo': 1,\n",
       " 'like': 120343,\n",
       " 'dammm': 59,\n",
       " 'like_dammm': 3,\n",
       " 'dammm_<user>': 2,\n",
       " 'lexis': 27,\n",
       " '<user>_lexis': 2,\n",
       " 'u': 97734,\n",
       " 'lexis_u': 1,\n",
       " 'got': 63864,\n",
       " 'u_got': 910,\n",
       " 'got_a': 8000,\n",
       " 'lot': 11417,\n",
       " 'a_lot': 10069,\n",
       " 'lot_to': 544,\n",
       " 'say': 33490,\n",
       " 'to_say': 7151,\n",
       " 'when': 94877,\n",
       " 'say_when': 95,\n",
       " 'ur': 18974,\n",
       " 'when_ur': 443,\n",
       " 'on': 204246,\n",
       " 'ur_on': 94,\n",
       " 'twitter': 24314,\n",
       " 'on_twitter': 4889,\n",
       " 'lol': 96661,\n",
       " 'twitter_lol': 127,\n",
       " 'grateful': 721,\n",
       " 'today': 60325,\n",
       " 'grateful_today': 1,\n",
       " 'today_for': 359,\n",
       " 'for_a': 21389,\n",
       " 'dream': 7913,\n",
       " 'a_dream': 1569,\n",
       " 'fulfilled': 45,\n",
       " 'dream_fulfilled': 2,\n",
       " 'fulfilled_!': 5,\n",
       " '!_my': 3776,\n",
       " 'heart': 12223,\n",
       " 'my_heart': 5154,\n",
       " 'heart_is': 629,\n",
       " 'full': 8125,\n",
       " 'so_full': 120,\n",
       " 'full_-': 14,\n",
       " 'first': 25328,\n",
       " '-_first': 56,\n",
       " '3': 38690,\n",
       " 'first_3': 35,\n",
       " 'completed': 285,\n",
       " '3_completed': 2,\n",
       " 'tracks': 958,\n",
       " 'completed_tracks': 1,\n",
       " 'have': 173660,\n",
       " 'tracks_have': 5,\n",
       " 'arrived': 934,\n",
       " 'have_arrived': 60,\n",
       " 'back': 71557,\n",
       " 'arrived_back': 10,\n",
       " 'from': 75532,\n",
       " 'back_from': 1717,\n",
       " 'new': 49972,\n",
       " 'from_new': 205,\n",
       " 'york': 2979,\n",
       " 'new_york': 2787,\n",
       " 'york_!': 49,\n",
       " '#yeslord': 2,\n",
       " '!_#yeslord': 1,\n",
       " '#yeslord_!': 2,\n",
       " 'at_home': 3289,\n",
       " 'affairs': 124,\n",
       " 'home_affairs': 4,\n",
       " 'shall': 3585,\n",
       " 'affairs_shall': 1,\n",
       " 'do': 95995,\n",
       " 'shall_do': 78,\n",
       " 'it': 279206,\n",
       " 'do_it': 7965,\n",
       " 'later': 7898,\n",
       " 'it_later': 279,\n",
       " 'barca': 1242,\n",
       " 'bout': 7215,\n",
       " 'barca_bout': 1,\n",
       " 'bout_to': 2337,\n",
       " 'beat': 3618,\n",
       " 'to_beat': 342,\n",
       " 'real': 12157,\n",
       " 'beat_real': 9,\n",
       " 'madrid': 974,\n",
       " 'real_madrid': 404,\n",
       " 'madrid_on': 4,\n",
       " 'saturday': 6658,\n",
       " 'on_saturday': 1508,\n",
       " 'doe': 1693,\n",
       " 'saturday_doe': 1,\n",
       " '<user>_a': 3600,\n",
       " 'lot_of': 4161,\n",
       " 'parts': 2111,\n",
       " 'of_parts': 10,\n",
       " 'parts_of': 207,\n",
       " 'asia': 879,\n",
       " 'of_asia': 72,\n",
       " 'asia_.': 42,\n",
       " 'especially': 2915,\n",
       " '._especially': 328,\n",
       " 'rats': 122,\n",
       " 'especially_rats': 1,\n",
       " 'rats_that': 2,\n",
       " 'live': 22567,\n",
       " 'that_live': 55,\n",
       " 'live_in': 2552,\n",
       " 'in_the': 60881,\n",
       " 'country': 3853,\n",
       " 'the_country': 491,\n",
       " 'country_and': 100,\n",
       " 'and_live': 200,\n",
       " 'live_on': 8479,\n",
       " 'grains': 56,\n",
       " 'on_grains': 1,\n",
       " 'grains_.': 3,\n",
       " 'supposed': 2432,\n",
       " '._supposed': 11,\n",
       " 'supposed_to': 2347,\n",
       " 'to_be': 40510,\n",
       " 'quite': 3204,\n",
       " 'be_quite': 119,\n",
       " 'tasty': 381,\n",
       " 'quite_tasty': 2,\n",
       " 'tasty_.': 20,\n",
       " \"wasn't\": 6348,\n",
       " \"i_wasn't\": 1916,\n",
       " \"wasn't_even\": 235,\n",
       " 'sleeping': 3880,\n",
       " 'even_sleeping': 3,\n",
       " 'sleeping_.': 153,\n",
       " '._so': 5481,\n",
       " 'shut': 2940,\n",
       " 'so_shut': 53,\n",
       " 'cho': 104,\n",
       " 'shut_cho': 3,\n",
       " 'ole': 374,\n",
       " 'cho_ole': 1,\n",
       " 'ole_go': 1,\n",
       " 'go_back': 3863,\n",
       " 'back_to': 10951,\n",
       " 'sleep': 24533,\n",
       " 'to_sleep': 9481,\n",
       " 'lookin': 1326,\n",
       " 'sleep_lookin': 1,\n",
       " 'ass': 12381,\n",
       " 'lookin_ass': 11,\n",
       " 'i_have': 46946,\n",
       " 'have_the': 5696,\n",
       " 'worlds': 883,\n",
       " 'the_worlds': 303,\n",
       " 'best': 39191,\n",
       " 'worlds_best': 26,\n",
       " 'dad': 6636,\n",
       " 'best_dad': 23,\n",
       " 'dad_.': 213,\n",
       " '._<3': 2625,\n",
       " 'ab': 474,\n",
       " '<user>_ab': 8,\n",
       " 'jaeyay': 1,\n",
       " 'ab_jaeyay': 1,\n",
       " 'werna': 1,\n",
       " 'jaeyay_werna': 1,\n",
       " 'meeting': 2795,\n",
       " 'werna_meeting': 1,\n",
       " 'khatam': 4,\n",
       " 'meeting_khatam': 1,\n",
       " 'hi': 15035,\n",
       " 'khatam_hi': 1,\n",
       " 'hojaeygi': 1,\n",
       " 'hi_hojaeygi': 1,\n",
       " 'baaqi': 1,\n",
       " 'hojaeygi_baaqi': 1,\n",
       " 'ki': 336,\n",
       " 'baaqi_ki': 1,\n",
       " 'buttering': 2,\n",
       " 'ki_buttering': 1,\n",
       " 'baad': 33,\n",
       " 'buttering_baad': 1,\n",
       " 'may': 13443,\n",
       " 'baad_may': 1,\n",
       " 'karaygay': 1,\n",
       " 'may_karaygay': 1,\n",
       " 'hum': 157,\n",
       " 'karaygay_hum': 1,\n",
       " 'sab': 57,\n",
       " 'hum_sab': 3,\n",
       " 'no': 89472,\n",
       " '<user>_no': 14007,\n",
       " 'one': 94777,\n",
       " 'no_one': 7084,\n",
       " 'doubts': 59,\n",
       " 'one_doubts': 1,\n",
       " 'doubts_that': 1,\n",
       " 'ability': 335,\n",
       " 'that_ability': 4,\n",
       " 'ability_!': 3,\n",
       " 'check': 9699,\n",
       " '<user>_check': 1013,\n",
       " 'check_my': 383,\n",
       " 'my_tweet': 928,\n",
       " 'pic': 6208,\n",
       " 'tweet_pic': 6,\n",
       " 'pic_out': 13,\n",
       " 'out_.': 2611,\n",
       " '._that': 2048,\n",
       " 'was': 123259,\n",
       " 'that_was': 6595,\n",
       " 'was_the': 4080,\n",
       " 'outfit': 1075,\n",
       " 'the_outfit': 63,\n",
       " 'before': 15414,\n",
       " 'outfit_before': 2,\n",
       " 'before_.': 408,\n",
       " '._this': 4743,\n",
       " 'is_it': 6070,\n",
       " 'after': 20750,\n",
       " 'it_after': 225,\n",
       " 'just_got': 5852,\n",
       " 'got_my': 3245,\n",
       " 'mid-term': 15,\n",
       " 'my_mid-term': 3,\n",
       " 'mid-term_and': 1,\n",
       " \"i'm\": 171643,\n",
       " \"and_i'm\": 6639,\n",
       " 'impressed': 424,\n",
       " \"i'm_impressed\": 45,\n",
       " 'impressed_!': 42,\n",
       " '#happy': 405,\n",
       " '!_#happy': 45,\n",
       " 'summer': 9464,\n",
       " 'my_summer': 215,\n",
       " 'summer_days': 35,\n",
       " 'days_:': 78,\n",
       " '1': 38624,\n",
       " ':_1': 1152,\n",
       " '1_.': 1150,\n",
       " ')': 185367,\n",
       " '._)': 5798,\n",
       " 'work': 37557,\n",
       " ')_work': 8,\n",
       " 'work_from': 187,\n",
       " '10:30-': 6,\n",
       " 'from_10:30-': 3,\n",
       " '2:30': 192,\n",
       " '10:30-_2:30': 1,\n",
       " 'ish': 668,\n",
       " '2:30_ish': 1,\n",
       " '..': 119694,\n",
       " 'ish_..': 17,\n",
       " '2': 60603,\n",
       " '.._2': 86,\n",
       " '2_.': 1169,\n",
       " ')_home': 10,\n",
       " 'home_,': 1391,\n",
       " 'shower': 4245,\n",
       " ',_shower': 88,\n",
       " 'shower_,': 360,\n",
       " '&': 98718,\n",
       " ',_&': 3261,\n",
       " 'eat': 10078,\n",
       " '&_eat': 104,\n",
       " 'eat_...': 96,\n",
       " '..._3': 73,\n",
       " '3_.': 852,\n",
       " ')_go': 103,\n",
       " 'go_out': 2823,\n",
       " 'till': 8489,\n",
       " 'out_till': 65,\n",
       " 'whenever': 2313,\n",
       " 'till_whenever': 4,\n",
       " 'whenever_and': 14,\n",
       " 'and_do': 1085,\n",
       " 'all': 114686,\n",
       " 'it_all': 3356,\n",
       " 'over': 25688,\n",
       " 'all_over': 2139,\n",
       " 'again': 25682,\n",
       " 'over_again': 526,\n",
       " '<user>_lol': 21656,\n",
       " 'nooo': 6175,\n",
       " 'lol_nooo': 82,\n",
       " 'nooo_..': 71,\n",
       " 'food': 9610,\n",
       " '.._food': 14,\n",
       " 'food_is': 272,\n",
       " 'is_ur': 221,\n",
       " 'friend': 17811,\n",
       " 'ur_friend': 91,\n",
       " '#16millionbritneyfan': 1,\n",
       " '<user>_#16millionbritneyfan': 1,\n",
       " '#16millionbritneyfan_rt': 1,\n",
       " 'rt_and': 999,\n",
       " 'and_tweet': 455,\n",
       " '<user>_but': 6777,\n",
       " 'seriously': 6604,\n",
       " 'but_seriously': 352,\n",
       " 'though': 26785,\n",
       " 'seriously_though': 120,\n",
       " 'though_..': 433,\n",
       " \"it's\": 68925,\n",
       " \".._it's\": 678,\n",
       " 'called': 6308,\n",
       " \"it's_called\": 265,\n",
       " 'vanity': 133,\n",
       " 'called_vanity': 1,\n",
       " 'fairest': 13,\n",
       " 'vanity_fairest': 1,\n",
       " 'chloe': 309,\n",
       " 'lol_chloe': 1,\n",
       " \":')\": 3982,\n",
       " \"chloe_:')\": 1,\n",
       " 'ill': 14611,\n",
       " \":')_ill\": 2,\n",
       " 'teach': 2139,\n",
       " 'ill_teach': 25,\n",
       " 'yous': 549,\n",
       " 'teach_yous': 1,\n",
       " 'yous_to': 5,\n",
       " 'cook': 1987,\n",
       " 'to_cook': 545,\n",
       " 'cook_,': 93,\n",
       " \"you'll\": 9273,\n",
       " \",_you'll\": 925,\n",
       " \"you'll_be\": 2083,\n",
       " 'pros': 162,\n",
       " 'be_pros': 6,\n",
       " 'by': 47660,\n",
       " 'pros_by': 11,\n",
       " 'by_the': 5512,\n",
       " 'end': 10875,\n",
       " 'the_end': 4464,\n",
       " 'end_of': 3152,\n",
       " 'of_it': 3666,\n",
       " 'it_!': 13831,\n",
       " 'lol_,': 4092,\n",
       " 'im': 57932,\n",
       " ',_im': 3364,\n",
       " 'im_finna': 165,\n",
       " 'finna_eat': 36,\n",
       " 'something': 20164,\n",
       " 'eat_something': 115,\n",
       " 'something_...': 277,\n",
       " 'chicken': 2961,\n",
       " '..._chicken': 8,\n",
       " 'any': 18637,\n",
       " 'questions': 2501,\n",
       " 'any_questions': 105,\n",
       " 'questions_for': 67,\n",
       " 'for_me': 17427,\n",
       " 'me_?': 13554,\n",
       " '?_just': 866,\n",
       " 'preston': 85,\n",
       " 'put_preston': 1,\n",
       " 'down': 21968,\n",
       " 'preston_down': 1,\n",
       " 'down_for': 695,\n",
       " 'short': 5353,\n",
       " 'a_short': 575,\n",
       " 'nap': 3253,\n",
       " 'short_nap': 19,\n",
       " 'nap_and': 138,\n",
       " 'mikes': 49,\n",
       " 'and_mikes': 5,\n",
       " 'mikes_not': 3,\n",
       " 'not_home': 244,\n",
       " 'home_for': 591,\n",
       " 'another': 14113,\n",
       " 'for_another': 1094,\n",
       " 'hour': 5777,\n",
       " 'another_hour': 101,\n",
       " 'hour_,': 166,\n",
       " ',_feel': 468,\n",
       " 'feel_like': 8740,\n",
       " 'answer': 4074,\n",
       " 'like_answer': 1,\n",
       " \"q's\": 51,\n",
       " \"answer_q's\": 2,\n",
       " \"q's_!\": 3,\n",
       " 'make': 45228,\n",
       " '!_make': 491,\n",
       " \"'\": 72316,\n",
       " \"make_'\": 24,\n",
       " 'em': 4080,\n",
       " \"'_em\": 552,\n",
       " 'original': 3828,\n",
       " 'em_original': 1,\n",
       " 'friday': 12247,\n",
       " 'friday_is': 238,\n",
       " 'payday': 239,\n",
       " 'is_payday': 19,\n",
       " 'payday_&': 2,\n",
       " '4/20': 3911,\n",
       " '&_4/20': 4,\n",
       " '4/20_?': 152,\n",
       " '?_?': 35331,\n",
       " 'hell': 7197,\n",
       " '?_hell': 63,\n",
       " 'yeah': 34487,\n",
       " 'hell_yeah': 927,\n",
       " '#reasons2dothebirdmanhandrub': 10,\n",
       " 'yeah_#reasons2dothebirdmanhandrub': 1,\n",
       " 'thank': 39901,\n",
       " '<user>_thank': 12482,\n",
       " 'thank_you': 31607,\n",
       " 'you_again': 1030,\n",
       " 'love': 124037,\n",
       " 'again_love': 24,\n",
       " 'love_&': 343,\n",
       " '&_&': 3673,\n",
       " '&_i': 5489,\n",
       " 'def': 1482,\n",
       " 'i_def': 129,\n",
       " 'def_will': 37,\n",
       " 'get': 107956,\n",
       " 'will_get': 1980,\n",
       " 'get_that': 1169,\n",
       " 'done': 18075,\n",
       " 'that_done': 28,\n",
       " 'done_)': 57,\n",
       " ')_i': 1942,\n",
       " 'plenty': 864,\n",
       " 'have_plenty': 100,\n",
       " 'plenty_i': 1,\n",
       " 'want': 72375,\n",
       " 'i_want': 30473,\n",
       " 'framed': 219,\n",
       " 'want_framed': 1,\n",
       " 'lols': 372,\n",
       " 'framed_lols': 1,\n",
       " \"<user>_i'm\": 23340,\n",
       " 'very': 22712,\n",
       " \"i'm_very\": 700,\n",
       " 'week': 19482,\n",
       " 'very_week': 2,\n",
       " 'thanksss': 584,\n",
       " 'week_thanksss': 1,\n",
       " 'thanksss_!': 82,\n",
       " 'what': 84021,\n",
       " '!_what': 2610,\n",
       " 'what_you': 6988,\n",
       " 'you_up': 1616,\n",
       " 'up_to': 6819,\n",
       " 'to_?': 947,\n",
       " 'xxx': 27537,\n",
       " '?_xxx': 2857,\n",
       " '#ff': 4375,\n",
       " '#ff_to': 363,\n",
       " 'to_my': 16113,\n",
       " 'good': 96346,\n",
       " 'my_good': 348,\n",
       " 'good_,': 2436,\n",
       " 'hilarious': 1570,\n",
       " ',_hilarious': 29,\n",
       " 'hilarious_,': 88,\n",
       " 'sweet': 9856,\n",
       " ',_sweet': 251,\n",
       " 'sweet_and': 376,\n",
       " 'kind': 5350,\n",
       " 'and_kind': 81,\n",
       " 'kind_friend': 3,\n",
       " 'friend_,': 865,\n",
       " 'who': 40450,\n",
       " ',_who': 1482,\n",
       " 'loves': 5569,\n",
       " 'who_loves': 340,\n",
       " 'toilet': 711,\n",
       " 'loves_toilet': 1,\n",
       " 'paper': 4978,\n",
       " 'toilet_paper': 91,\n",
       " 'paper_,': 208,\n",
       " ',_<user>': 4102,\n",
       " \"he's\": 14068,\n",
       " \"<user>_he's\": 1812,\n",
       " 'always': 31028,\n",
       " \"he's_always\": 108,\n",
       " 'made': 21933,\n",
       " 'always_made': 45,\n",
       " 'made_me': 4389,\n",
       " 'laugh': 5631,\n",
       " 'me_laugh': 1833,\n",
       " 'laugh_when': 140,\n",
       " 'when_i': 21813,\n",
       " 'needed': 2960,\n",
       " 'i_needed': 820,\n",
       " 'needed_to': 419,\n",
       " 'to_!': 1314,\n",
       " 'apparently': 1880,\n",
       " '<user>_apparently': 292,\n",
       " 'apparently_not': 53,\n",
       " 'not_)': 66,\n",
       " 'i_live': 1994,\n",
       " 'live_my': 140,\n",
       " 'my_life': 8988,\n",
       " 'life_on': 327,\n",
       " 'on_the': 32545,\n",
       " 'quote': 770,\n",
       " 'the_quote': 37,\n",
       " '\"': 208896,\n",
       " 'quote_\"': 20,\n",
       " '\"_live': 46,\n",
       " 'live_life': 192,\n",
       " 'life_for': 269,\n",
       " 'for_the': 40126,\n",
       " 'moment': 7180,\n",
       " 'the_moment': 1778,\n",
       " 'cos': 3417,\n",
       " 'moment_cos': 4,\n",
       " 'cos_everything': 5,\n",
       " 'else': 6968,\n",
       " 'everything_else': 228,\n",
       " 'else_is': 499,\n",
       " 'uncertain': 71,\n",
       " 'is_uncertain': 8,\n",
       " 'uncertain_\"': 3,\n",
       " '\"_-': 2437,\n",
       " 'louis': 4109,\n",
       " '-_louis': 16,\n",
       " 'tomlinson': 313,\n",
       " 'louis_tomlinson': 273,\n",
       " \"can't\": 59574,\n",
       " \"i_can't\": 24771,\n",
       " 'wait': 29690,\n",
       " \"can't_wait\": 15856,\n",
       " 'wait_to': 10801,\n",
       " 'see': 73236,\n",
       " 'to_see': 27170,\n",
       " 'see_it': 3386,\n",
       " 'can': 89292,\n",
       " 'it_can': 763,\n",
       " 'can_you': 9827,\n",
       " 'you_please': 3450,\n",
       " 'notice': 6817,\n",
       " 'please_notice': 1108,\n",
       " 'notice_or': 83,\n",
       " 'or_follow': 804,\n",
       " 'they': 50640,\n",
       " 'but_they': 1824,\n",
       " \"couldn't\": 5608,\n",
       " \"they_couldn't\": 51,\n",
       " \"couldn't_be\": 401,\n",
       " 'bigger': 1290,\n",
       " 'be_bigger': 41,\n",
       " 'than': 21107,\n",
       " 'bigger_than': 196,\n",
       " 'than_<user>': 118,\n",
       " \"<user>_'\": 2812,\n",
       " 's': 26336,\n",
       " \"'_s\": 5036,\n",
       " 'ones': 5003,\n",
       " 's_ones': 1,\n",
       " '#yougetmajorpointsif': 3380,\n",
       " '#yougetmajorpointsif_you': 2442,\n",
       " 'are': 121037,\n",
       " 'you_are': 17848,\n",
       " 'olivia': 243,\n",
       " 'are_olivia': 1,\n",
       " 'brown': 9424,\n",
       " 'olivia_brown': 1,\n",
       " 'ty': 1266,\n",
       " '<user>_ty': 211,\n",
       " 'as': 54254,\n",
       " 'ty_as': 2,\n",
       " 'well': 47038,\n",
       " 'as_well': 4223,\n",
       " 'well_!': 1455,\n",
       " '!_i': 45185,\n",
       " \"i_don't\": 34153,\n",
       " \"don't_know\": 8880,\n",
       " 'know_what': 8370,\n",
       " 'what_i': 8760,\n",
       " 'would': 46038,\n",
       " 'i_would': 12230,\n",
       " 'would_do': 1208,\n",
       " 'do_without': 502,\n",
       " 'without_you': 1989,\n",
       " 'too': 75707,\n",
       " 'you_too': 8215,\n",
       " 'too_my': 330,\n",
       " 'sister': 6502,\n",
       " 'my_sister': 2911,\n",
       " 'sister_in': 267,\n",
       " 'christ': 660,\n",
       " 'in_christ': 41,\n",
       " 'christ_!': 53,\n",
       " '!_you': 9125,\n",
       " 'truly': 1943,\n",
       " 'are_truly': 83,\n",
       " 'truly_a': 70,\n",
       " 'blessing': 652,\n",
       " 'a_blessing': 274,\n",
       " 'blessing_to': 69,\n",
       " 'to_the': 32850,\n",
       " 'kingdom': 472,\n",
       " 'the_kingdom': 85,\n",
       " 'kingdom_!': 4,\n",
       " 'i_love': 51058,\n",
       " 'southall': 4,\n",
       " 'love_southall': 1,\n",
       " 'travel': 3918,\n",
       " 'southall_travel': 1,\n",
       " 'travel_,': 60,\n",
       " ',_and': 28843,\n",
       " 'and_before': 47,\n",
       " 'before_i': 2479,\n",
       " 'visit': 3920,\n",
       " 'i_visit': 57,\n",
       " 'visit_southall': 1,\n",
       " 'southall_i': 1,\n",
       " 'visit_<url>': 101,\n",
       " '<url>_first': 17,\n",
       " 'care': 10291,\n",
       " 'we_care': 39,\n",
       " 'care_a': 25,\n",
       " 'lot_for': 238,\n",
       " 'the_ones': 1057,\n",
       " 'ones_you': 101,\n",
       " 'you_love': 3882,\n",
       " 'love_...': 393,\n",
       " '..._good': 488,\n",
       " 'morning': 24526,\n",
       " 'good_morning': 6729,\n",
       " 'morning_<url>': 141,\n",
       " 'share': 8952,\n",
       " '<url>_share': 62,\n",
       " 'share_it': 332,\n",
       " 'it_with': 1891,\n",
       " 'with_your': 3297,\n",
       " 'friends': 23320,\n",
       " 'your_friends': 992,\n",
       " 'friends_.': 1072,\n",
       " 'love_you': 30059,\n",
       " 'you_!': 21721,\n",
       " 'if': 90716,\n",
       " '!_if': 2137,\n",
       " 'if_you': 32103,\n",
       " 'love_me': 3107,\n",
       " 'to_please': 416,\n",
       " 'please_rt': 1658,\n",
       " '<user>_are': 4455,\n",
       " 'are_you': 21478,\n",
       " 'you_a': 4448,\n",
       " 'blonde': 1008,\n",
       " 'a_blonde': 70,\n",
       " 'yet': 11975,\n",
       " 'blonde_yet': 1,\n",
       " 'yet_?': 1358,\n",
       " '<user>_wait': 637,\n",
       " 'wait_my': 23,\n",
       " 'has': 35346,\n",
       " 'brother_has': 73,\n",
       " 'has_one': 191,\n",
       " 'one_!': 3299,\n",
       " \"let's\": 7677,\n",
       " \"!_let's\": 1015,\n",
       " \"let's_get\": 608,\n",
       " 'get_a': 9430,\n",
       " 'picture': 33462,\n",
       " 'a_picture': 1960,\n",
       " '<user>_if': 6753,\n",
       " 'if_i': 14467,\n",
       " 'dont': 25703,\n",
       " 'i_dont': 9994,\n",
       " 'dont_do': 343,\n",
       " 'it_first': 174,\n",
       " 'thing': 19009,\n",
       " 'first_thing': 468,\n",
       " ...}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.corpus_word_count\n",
    "bigram.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189894680"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_.corpus_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<': 2220023,\n",
       " 'u': 6352276,\n",
       " '<_u': 2132461,\n",
       " 's': 9423600,\n",
       " 'u_s': 2132379,\n",
       " 'e': 15722332,\n",
       " 's_e': 2352706,\n",
       " 'r': 9004658,\n",
       " 'e_r': 3202920,\n",
       " '>': 2177463,\n",
       " 'r_>': 1605608,\n",
       " ' ': 36855599,\n",
       " '>_ ': 1635158,\n",
       " 'i': 9326672,\n",
       " ' _i': 2503335,\n",
       " 'i_ ': 1116302,\n",
       " 'd': 4491249,\n",
       " ' _d': 978049,\n",
       " 'd_u': 71340,\n",
       " 'n': 8105634,\n",
       " 'u_n': 319824,\n",
       " 'n_n': 186712,\n",
       " 'o': 11239805,\n",
       " 'n_o': 612469,\n",
       " 'o_ ': 1638143,\n",
       " 'j': 386471,\n",
       " ' _j': 285612,\n",
       " 'j_u': 195989,\n",
       " 't': 11052088,\n",
       " 's_t': 1047468,\n",
       " 't_i': 716456,\n",
       " 'i_n': 2146456,\n",
       " 'n_ ': 1769823,\n",
       " ' _r': 662124,\n",
       " 'r_e': 1420409,\n",
       " 'a': 10585680,\n",
       " 'e_a': 927614,\n",
       " 'a_d': 401356,\n",
       " 'd_ ': 2034462,\n",
       " 'm': 4222840,\n",
       " ' _m': 1730630,\n",
       " 'y': 3869942,\n",
       " 'm_y': 443504,\n",
       " 'y_ ': 2049492,\n",
       " 'm_e': 1168077,\n",
       " 'e_n': 907948,\n",
       " 'n_t': 589392,\n",
       " 'i_o': 292769,\n",
       " 'o_n': 1394011,\n",
       " ' _o': 1234596,\n",
       " 'o_r': 1100030,\n",
       " 'r_ ': 1550022,\n",
       " ' _n': 809412,\n",
       " 'o_t': 484825,\n",
       " 't_ ': 3343784,\n",
       " '.': 2618227,\n",
       " ' _.': 1391286,\n",
       " '._ ': 1175769,\n",
       " 'l': 6697549,\n",
       " 'n_l': 66339,\n",
       " 'l_y': 368754,\n",
       " ' _a': 2376356,\n",
       " 'a_n': 1710540,\n",
       " 'n_d': 934442,\n",
       " 'g': 3192303,\n",
       " ' _g': 943841,\n",
       " 'g_o': 484841,\n",
       " 'o_d': 337881,\n",
       " 'k': 1951965,\n",
       " ' _k': 304184,\n",
       " 'k_n': 133627,\n",
       " 'w': 3239821,\n",
       " 'o_w': 662463,\n",
       " 'w_s': 37219,\n",
       " 's_ ': 2972427,\n",
       " 'b': 2377750,\n",
       " 'a_b': 211755,\n",
       " 'b_o': 307293,\n",
       " 'o_u': 1600140,\n",
       " 'u_t': 554952,\n",
       " ' _t': 3504799,\n",
       " 'h': 6308543,\n",
       " 't_h': 2486527,\n",
       " 'h_a': 1445848,\n",
       " 'a_t': 1098475,\n",
       " ',': 752020,\n",
       " ' _,': 744711,\n",
       " ',_ ': 742942,\n",
       " ' _b': 1522485,\n",
       " 'b_u': 266233,\n",
       " ' _h': 1380130,\n",
       " 'h_o': 686060,\n",
       " 'p': 2597313,\n",
       " 'o_p': 221453,\n",
       " 'p_e': 390049,\n",
       " 'e_ ': 5003975,\n",
       " ' _y': 1029143,\n",
       " 'y_o': 910694,\n",
       " 'u_ ': 734576,\n",
       " ' _w': 1826987,\n",
       " 'w_i': 534133,\n",
       " 'i_l': 427044,\n",
       " 'l_l': 1037404,\n",
       " 'l_ ': 1085226,\n",
       " 'f': 2554404,\n",
       " ' _f': 1336526,\n",
       " 'f_o': 605010,\n",
       " 'o_l': 570109,\n",
       " 'l_o': 781295,\n",
       " 'w_ ': 611620,\n",
       " '#': 364769,\n",
       " ' _#': 319593,\n",
       " '#_b': 21199,\n",
       " 'b_e': 613984,\n",
       " 'e_l': 534790,\n",
       " 'l_i': 602188,\n",
       " 'i_e': 322656,\n",
       " 'v': 1356948,\n",
       " 'e_v': 259442,\n",
       " 'v_e': 959404,\n",
       " '1': 391313,\n",
       " ' _1': 221022,\n",
       " '5': 178513,\n",
       " '1_5': 17237,\n",
       " 'c': 3470819,\n",
       " 'e_c': 257140,\n",
       " 'c_a': 540268,\n",
       " 'a_u': 152247,\n",
       " 'u_r': 1049859,\n",
       " ' _l': 1064241,\n",
       " 'o_g': 72542,\n",
       " 'g_i': 173438,\n",
       " 'i_c': 449394,\n",
       " 'c_ ': 161524,\n",
       " 'i_s': 993990,\n",
       " ' _s': 2053586,\n",
       " 's_o': 595837,\n",
       " 'u_m': 118512,\n",
       " 'm_b': 69918,\n",
       " 'b_ ': 97834,\n",
       " 'w_o': 299293,\n",
       " \"'\": 937301,\n",
       " \"n_'\": 260932,\n",
       " \"'_t\": 243002,\n",
       " ' _e': 505343,\n",
       " ' _c': 1277502,\n",
       " 'c_r': 143699,\n",
       " 'r_o': 524203,\n",
       " 'p_ ': 347934,\n",
       " 'n_a': 314688,\n",
       " 'a_m': 484827,\n",
       " ' _p': 1019590,\n",
       " 'p_h': 87104,\n",
       " 't_o': 1267012,\n",
       " 't_s': 297777,\n",
       " 's_k': 60231,\n",
       " 'k_ ': 697149,\n",
       " ' _<': 1223324,\n",
       " 'r_l': 656497,\n",
       " 'l_>': 526868,\n",
       " 'p_u': 67563,\n",
       " 'a_s': 691348,\n",
       " 's_p': 125492,\n",
       " 'a_ ': 1079013,\n",
       " 'x': 550247,\n",
       " 'o_x': 27703,\n",
       " 'x_ ': 103666,\n",
       " '!': 1001686,\n",
       " ' _!': 1000854,\n",
       " '!_ ': 786905,\n",
       " ' _ ': 24767,\n",
       " 'o_o': 690478,\n",
       " 'o_v': 286600,\n",
       " 'e_d': 675022,\n",
       " 'h_e': 1847643,\n",
       " 'b_a': 365858,\n",
       " 't_t': 281016,\n",
       " 't_l': 79515,\n",
       " 'l_e': 874960,\n",
       " '#_c': 16823,\n",
       " 'r_a': 535653,\n",
       " 'a_k': 216338,\n",
       " 'k_k': 11043,\n",
       " 'k_b': 5144,\n",
       " 'b_i': 143054,\n",
       " 'i_t': 1154324,\n",
       " 't_c': 102213,\n",
       " 'c_h': 524564,\n",
       " 'n_k': 243875,\n",
       " 'k_s': 147153,\n",
       " 's_i': 348737,\n",
       " 'i_r': 283468,\n",
       " ' _>': 40387,\n",
       " 'd_o': 408002,\n",
       " 't_r': 261712,\n",
       " 'r_i': 564081,\n",
       " 'i_p': 92027,\n",
       " 'm_a': 549345,\n",
       " '._.': 1154174,\n",
       " 'k_e': 460260,\n",
       " 'e_e': 721224,\n",
       " 'e_p': 157148,\n",
       " 'o_i': 137259,\n",
       " 'y_a': 60958,\n",
       " 'n_g': 1198698,\n",
       " 'g_ ': 1078481,\n",
       " 'v_i': 195339,\n",
       " 'b_r': 146379,\n",
       " 't_m': 25143,\n",
       " 'm_r': 11981,\n",
       " 'e_s': 887301,\n",
       " 't_e': 945880,\n",
       " 'r_t': 416920,\n",
       " 'h_d': 33269,\n",
       " 'd_a': 422327,\n",
       " 'a_y': 592201,\n",
       " 'i_f': 205112,\n",
       " 'f_t': 70593,\n",
       " 'r_r': 173547,\n",
       " '#_l': 16074,\n",
       " 'f_e': 212553,\n",
       " 'c_o': 556452,\n",
       " 'o_m': 653405,\n",
       " 'm_p': 138261,\n",
       " 'p_l': 332829,\n",
       " 'e_t': 652460,\n",
       " 't_w': 147743,\n",
       " 'w_e': 499267,\n",
       " '/': 143929,\n",
       " ' _/': 110794,\n",
       " '/_ ': 115682,\n",
       " 'f_a': 198005,\n",
       " 'a_c': 492930,\n",
       " 'c_e': 348410,\n",
       " 'e_b': 62996,\n",
       " 'o_k': 200965,\n",
       " '#_1': 5767,\n",
       " '1_d': 7595,\n",
       " 'd_n': 66395,\n",
       " 'n_e': 761515,\n",
       " 'e_x': 172369,\n",
       " 'x_t': 69843,\n",
       " 't_a': 392763,\n",
       " 'a_l': 865677,\n",
       " 'l_b': 19303,\n",
       " 'm_t': 5764,\n",
       " ':': 241514,\n",
       " ' _:': 225735,\n",
       " ':_ ': 173597,\n",
       " 'r_c': 43075,\n",
       " 'o_a': 58365,\n",
       " 'o_f': 429685,\n",
       " 'f_ ': 563452,\n",
       " 'o_c': 82123,\n",
       " 'p_t': 50798,\n",
       " '#_y': 11055,\n",
       " 'm_i': 333540,\n",
       " 'm_o': 372015,\n",
       " '?': 334751,\n",
       " ' _?': 334690,\n",
       " '?_ ': 240945,\n",
       " '3': 253321,\n",
       " '<_3': 57219,\n",
       " '3_ ': 98147,\n",
       " '4': 158636,\n",
       " '1_4': 13683,\n",
       " '4_ ': 59944,\n",
       " '#_f': 23924,\n",
       " 'w_m': 2799,\n",
       " 'z': 194755,\n",
       " 'l_z': 5039,\n",
       " 'z_ ': 45304,\n",
       " ' _x': 153561,\n",
       " 'x_1': 10905,\n",
       " 'r_k': 108305,\n",
       " 'k_i': 251322,\n",
       " 'a_r': 836688,\n",
       " 'r_d': 173201,\n",
       " 'd_l': 45763,\n",
       " 'd_e': 451683,\n",
       " \"e_'\": 70869,\n",
       " \"'_s\": 263554,\n",
       " 'h_ ': 860868,\n",
       " 'f_u': 146320,\n",
       " 't_u': 215981,\n",
       " 's_a': 271141,\n",
       " 'a_w': 131673,\n",
       " \"i_'\": 248745,\n",
       " \"'_l\": 63685,\n",
       " 'y_i': 66282,\n",
       " 'h_i': 766833,\n",
       " 'd_d': 63613,\n",
       " '#_n': 16935,\n",
       " 'l_s': 111276,\n",
       " '2': 342008,\n",
       " 's_2': 1420,\n",
       " '0': 382168,\n",
       " '2_0': 61309,\n",
       " '0_1': 24447,\n",
       " '1_3': 16151,\n",
       " 'n_v': 22710,\n",
       " 's_h': 445239,\n",
       " '-': 391876,\n",
       " 'n_-': 9151,\n",
       " '-_l': 5932,\n",
       " 'f_i': 199130,\n",
       " 'r_y': 321484,\n",
       " 'y_t': 45150,\n",
       " ' _-': 202479,\n",
       " '-_ ': 209669,\n",
       " 'a_v': 313034,\n",
       " ' _u': 404318,\n",
       " 'u_p': 196261,\n",
       " 'h_u': 92615,\n",
       " 'a_g': 157910,\n",
       " 'g_r': 158481,\n",
       " '1_2': 46816,\n",
       " '2_ ': 114333,\n",
       " 'y_s': 155696,\n",
       " 'e_f': 80447,\n",
       " 'k_a': 70653,\n",
       " 'e_m': 222503,\n",
       " 'i_k': 155914,\n",
       " 'm_m': 92020,\n",
       " 'm_ ': 717715,\n",
       " 'x_i': 15431,\n",
       " 'w_h': 417851,\n",
       " 'u_l': 266118,\n",
       " 'd_r': 104851,\n",
       " 'l_f': 45188,\n",
       " 'r_s': 300223,\n",
       " ' _3': 90117,\n",
       " 'c_k': 456116,\n",
       " 'i_v': 160251,\n",
       " 'f_r': 328849,\n",
       " 'e_w': 133440,\n",
       " 'y_e': 194221,\n",
       " 's_l': 86334,\n",
       " 'a_f': 58710,\n",
       " 'f_f': 113655,\n",
       " 'a_i': 284219,\n",
       " 'l_a': 467734,\n",
       " 'i_d': 315268,\n",
       " 'o_e': 59581,\n",
       " 'p_a': 308924,\n",
       " 'i_a': 174247,\n",
       " 'c_i': 119203,\n",
       " 'n_s': 206171,\n",
       " 's_u': 179021,\n",
       " 'p_p': 161077,\n",
       " 'p_o': 214603,\n",
       " 'o_s': 218066,\n",
       " 'q': 82787,\n",
       " ' _q': 38486,\n",
       " 'q_u': 61998,\n",
       " 'u_i': 67802,\n",
       " 't_y': 132393,\n",
       " 'w_a': 578742,\n",
       " 's_n': 53241,\n",
       " 'p_i': 185632,\n",
       " 's_s': 383007,\n",
       " 'l_d': 253137,\n",
       " 'd_s': 116713,\n",
       " 'j_a': 42976,\n",
       " 'a_e': 12339,\n",
       " 'e_y': 178971,\n",
       " 'r_n': 113494,\n",
       " 'k_h': 4503,\n",
       " 'o_j': 7363,\n",
       " 'y_g': 5072,\n",
       " 'a_a': 67990,\n",
       " 'a_q': 8651,\n",
       " 'q_i': 532,\n",
       " 'g_a': 145540,\n",
       " 'u_b': 48262,\n",
       " 'b_t': 15334,\n",
       " 't_f': 23159,\n",
       " 'd_-': 9245,\n",
       " '-_t': 14439,\n",
       " 'r_m': 63180,\n",
       " \"'_m\": 173347,\n",
       " 'i_m': 294777,\n",
       " 'p_r': 247751,\n",
       " '#_h': 14648,\n",
       " 'a_p': 260605,\n",
       " 'p_y': 58141,\n",
       " '1_ ': 76112,\n",
       " ')': 192573,\n",
       " ' _)': 184753,\n",
       " ')_ ': 148830,\n",
       " '1_0': 62891,\n",
       " '0_:': 761,\n",
       " ':_3': 2872,\n",
       " '3_0': 24266,\n",
       " '0_-': 5116,\n",
       " ' _2': 172673,\n",
       " '2_:': 1155,\n",
       " '0_ ': 142785,\n",
       " '&': 98718,\n",
       " ' _&': 97642,\n",
       " '&_ ': 98683,\n",
       " '6': 132960,\n",
       " '1_6': 18147,\n",
       " '6_m': 3080,\n",
       " 'n_b': 9864,\n",
       " 't_n': 13202,\n",
       " 'y_f': 14103,\n",
       " 'u_g': 157479,\n",
       " 'g_h': 333180,\n",
       " \"t_'\": 135759,\n",
       " ' _v': 163729,\n",
       " 'v_a': 54528,\n",
       " 'n_i': 368674,\n",
       " 'h_l': 11099,\n",
       " \":_'\": 4484,\n",
       " \"'_)\": 4048,\n",
       " \"u_'\": 54518,\n",
       " 'b_y': 97189,\n",
       " 'n_y': 128164,\n",
       " 'u_e': 99298,\n",
       " 'w_n': 71151,\n",
       " 's_w': 52169,\n",
       " \"q_'\": 133,\n",
       " \" _'\": 70756,\n",
       " \"'_ \": 69703,\n",
       " 'i_g': 310686,\n",
       " 'y_d': 12995,\n",
       " ' _4': 69502,\n",
       " '4_/': 5747,\n",
       " '/_2': 8519,\n",
       " 'a_h': 286757,\n",
       " '#_r': 10676,\n",
       " '2_d': 1705,\n",
       " 'd_m': 23099,\n",
       " 'n_h': 6589,\n",
       " 'r_u': 106972,\n",
       " 'g_e': 366817,\n",
       " 'e_k': 60727,\n",
       " 'x_x': 93559,\n",
       " 'l_w': 35145,\n",
       " 'u_o': 5522,\n",
       " '\"': 208908,\n",
       " ' _\"': 208895,\n",
       " '\"_ ': 186523,\n",
       " 'n_c': 192827,\n",
       " 'm_l': 10132,\n",
       " 'g_g': 47384,\n",
       " 'a_j': 13351,\n",
       " 'j_o': 69580,\n",
       " 'r_p': 28355,\n",
       " 'h_r': 54402,\n",
       " 'b_l': 207585,\n",
       " 'g_d': 3054,\n",
       " 'c_t': 237287,\n",
       " 'l_u': 120236,\n",
       " 'd_g': 18246,\n",
       " 'o_h': 68417,\n",
       " 'v_ ': 31303,\n",
       " 'd_i': 335929,\n",
       " 'o_y': 73322,\n",
       " ':_p': 13346,\n",
       " 'h_y': 74076,\n",
       " '7': 99548,\n",
       " '1_7': 12329,\n",
       " '7_ ': 31921,\n",
       " ' _5': 57101,\n",
       " '5_0': 26225,\n",
       " '0_0': 93898,\n",
       " '#_t': 34122,\n",
       " 'm_j': 887,\n",
       " 'j_e': 40245,\n",
       " 'v_o': 61331,\n",
       " 'e_u': 17407,\n",
       " 'u_k': 14270,\n",
       " 't_d': 6163,\n",
       " ';': 21088,\n",
       " ' _;': 20316,\n",
       " ';_ ': 15955,\n",
       " 's_c': 127992,\n",
       " 'h_m': 15121,\n",
       " 'd_t': 13162,\n",
       " 'h_t': 226021,\n",
       " 'a_z': 50634,\n",
       " 'z_i': 37523,\n",
       " 'o_b': 77296,\n",
       " 'b_s': 30246,\n",
       " 'e_h': 44400,\n",
       " 'b_b': 25456,\n",
       " 'e_o': 112047,\n",
       " 'y_b': 29876,\n",
       " 'c_u': 123011,\n",
       " 'u_d': 86864,\n",
       " 'd_y': 96531,\n",
       " 'h_h': 71593,\n",
       " '#_z': 747,\n",
       " 'z_u': 2158,\n",
       " 'm_s': 48856,\n",
       " 'u_c': 190516,\n",
       " 'c_c': 38235,\n",
       " '8': 109979,\n",
       " '4_8': 4485,\n",
       " '8_ ': 48833,\n",
       " 'w_f': 3784,\n",
       " 'n_m': 8459,\n",
       " 'g_u': 108201,\n",
       " 'u_a': 67459,\n",
       " '4_t': 3059,\n",
       " ':_d': 24636,\n",
       " 'x_o': 16165,\n",
       " '#_s': 35281,\n",
       " 's_m': 73370,\n",
       " 'i_z': 38066,\n",
       " 'z_e': 35641,\n",
       " 'u_h': 13745,\n",
       " 'w_w': 54331,\n",
       " 'm_u': 122062,\n",
       " 'r_f': 35860,\n",
       " 'l_k': 58044,\n",
       " '4_.': 3132,\n",
       " '._2': 10503,\n",
       " 'l_p': 30718,\n",
       " 'a_o': 29333,\n",
       " 'y_n': 25006,\n",
       " 'k_y': 33459,\n",
       " '4_0': 18127,\n",
       " '0_s': 2265,\n",
       " 'n_u': 70794,\n",
       " 'g_l': 66362,\n",
       " 'l_m': 49169,\n",
       " '+': 12732,\n",
       " ' _+': 12085,\n",
       " '+_ ': 12426,\n",
       " '2_4': 13220,\n",
       " 'x_y': 9937,\n",
       " 'n_f': 33488,\n",
       " \"m_'\": 3405,\n",
       " \"'_d\": 18251,\n",
       " 'p_w': 1911,\n",
       " '9': 96429,\n",
       " '9_:': 651,\n",
       " ':_1': 1600,\n",
       " '1_9': 21965,\n",
       " '9_ ': 27764,\n",
       " 't_4': 610,\n",
       " '#_e': 5791,\n",
       " 'x_c': 23875,\n",
       " 'r_g': 67575,\n",
       " 'm_f': 25191,\n",
       " 'f_g': 3473,\n",
       " \"y_'\": 25343,\n",
       " \"'_r\": 52576,\n",
       " 't_z': 4093,\n",
       " 'i_b': 50058,\n",
       " ';_d': 1934,\n",
       " '#_i': 20116,\n",
       " 'f_c': 6000,\n",
       " 'l_c': 18396,\n",
       " '#_d': 11697,\n",
       " 'd_f': 5735,\n",
       " 't_k': 2135,\n",
       " 'd_w': 9639,\n",
       " 'd_h': 7045,\n",
       " '<_-': 2789,\n",
       " '-_-': 10216,\n",
       " '#_w': 15928,\n",
       " 'h_f': 2352,\n",
       " 'l_t': 62556,\n",
       " 'j_k': 4885,\n",
       " 'g_s': 65349,\n",
       " 'g_n': 35456,\n",
       " 'r_v': 28007,\n",
       " 'm_h': 7682,\n",
       " 'e_i': 90130,\n",
       " 'z_y': 14850,\n",
       " 'u_f': 37994,\n",
       " '(': 589131,\n",
       " ' _(': 584113,\n",
       " '(_ ': 556803,\n",
       " 'p_b': 3700,\n",
       " 'k_u': 10362,\n",
       " 'u_y': 45003,\n",
       " 't_-': 11062,\n",
       " '-_s': 13309,\n",
       " 'p_s': 53778,\n",
       " 's_y': 43579,\n",
       " 'y_c': 10793,\n",
       " 'g_y': 21403,\n",
       " 'b_w': 2252,\n",
       " 'k_f': 6278,\n",
       " 'f_l': 55732,\n",
       " 'm_g': 26747,\n",
       " 'y_l': 31886,\n",
       " 'c_l': 109205,\n",
       " 'r_w': 14345,\n",
       " 'w_r': 33226,\n",
       " 's_r': 5171,\n",
       " 'd_v': 30539,\n",
       " 'f_s': 6583,\n",
       " ' _9': 27179,\n",
       " '9_5': 4577,\n",
       " '5_ ': 80817,\n",
       " '%': 7465,\n",
       " ' _%': 7460,\n",
       " '%_ ': 7263,\n",
       " 'u_z': 11973,\n",
       " 'e_g': 59723,\n",
       " '*': 53628,\n",
       " ' _*': 50658,\n",
       " '*_ ': 45825,\n",
       " '#_2': 1563,\n",
       " '2_8': 7914,\n",
       " '1_l': 312,\n",
       " 'y_r': 11220,\n",
       " 'c_s': 30194,\n",
       " \"w_'\": 3595,\n",
       " 'w_b': 10891,\n",
       " 'r_b': 63353,\n",
       " 't_b': 17368,\n",
       " '#_j': 7761,\n",
       " 'y_w': 13975,\n",
       " 'x_a': 17355,\n",
       " '#_p': 13741,\n",
       " 'o_z': 13726,\n",
       " 'l_r': 29014,\n",
       " 'd_k': 8885,\n",
       " 'j_ ': 9184,\n",
       " 'f_b': 6268,\n",
       " '[': 9773,\n",
       " ' _[': 8189,\n",
       " '[_:': 59,\n",
       " 'u_u': 21591,\n",
       " 'd_j': 3828,\n",
       " 'b_c': 6952,\n",
       " '5_a': 1084,\n",
       " '#_m': 15486,\n",
       " \"'_v\": 34399,\n",
       " '=': 12174,\n",
       " ' _=': 11433,\n",
       " '=_ ': 8062,\n",
       " 't_v': 10034,\n",
       " \"'_a\": 6243,\n",
       " 'd_p': 7259,\n",
       " '3_h': 379,\n",
       " '#_k': 3780,\n",
       " 'c_b': 4751,\n",
       " 'a_x': 14467,\n",
       " 'b_1': 776,\n",
       " '1_t': 687,\n",
       " 'h_3': 319,\n",
       " '3_z': 22,\n",
       " 'z_z': 15552,\n",
       " 'h_x': 3684,\n",
       " 'n_z': 5383,\n",
       " 'j_i': 6807,\n",
       " 'y_y': 32466,\n",
       " 'i_x': 18962,\n",
       " 'x_b': 1609,\n",
       " 'k_m': 3308,\n",
       " 'n_p': 6145,\n",
       " 'i_u': 12586,\n",
       " '1_1': 22446,\n",
       " 'p_m': 9492,\n",
       " 'g_t': 13849,\n",
       " '4_4': 5946,\n",
       " '#_5': 1054,\n",
       " '0_l': 871,\n",
       " 'i_w': 4788,\n",
       " 'e_1': 1047,\n",
       " 'd_b': 11145,\n",
       " 't_1': 743,\n",
       " '#_a': 12781,\n",
       " \"k_'\": 2365,\n",
       " 'i_-': 8739,\n",
       " '>_:': 309,\n",
       " '#_g': 13693,\n",
       " 'y_v': 993,\n",
       " 's_4': 2033,\n",
       " 'x_e': 17180,\n",
       " 'v_s': 5164,\n",
       " '4_o': 190,\n",
       " '5_t': 2166,\n",
       " 's_d': 22872,\n",
       " 'f_w': 2925,\n",
       " \"a_'\": 8377,\n",
       " \"g_'\": 2552,\n",
       " \"b_'\": 1099,\n",
       " '#_u': 3561,\n",
       " ' _6': 43298,\n",
       " '6_ ': 50736,\n",
       " 'k_c': 2090,\n",
       " 'n_3': 287,\n",
       " '3_p': 568,\n",
       " 'i_i': 21637,\n",
       " 'g_m': 5135,\n",
       " 'c_y': 17238,\n",
       " ' _z': 22103,\n",
       " 'm_w': 4081,\n",
       " '1_8': 16590,\n",
       " '8_y': 68,\n",
       " 'c_d': 22179,\n",
       " \"d_'\": 9798,\n",
       " 'v_y': 6693,\n",
       " 'y_u': 17534,\n",
       " 'p_d': 6769,\n",
       " 'n_j': 16383,\n",
       " '`': 2425,\n",
       " ' _`': 1890,\n",
       " '`_ ': 2374,\n",
       " '\\\\': 4004,\n",
       " ' _\\\\': 3410,\n",
       " '\\\\_ ': 3711,\n",
       " 'e_z': 10465,\n",
       " 'y_m': 24460,\n",
       " 'k_o': 24561,\n",
       " '-_>': 3699,\n",
       " '@': 5901,\n",
       " '@_ ': 5695,\n",
       " 'k_l': 13836,\n",
       " '9_0': 7814,\n",
       " '#_9': 418,\n",
       " \"0_'\": 13,\n",
       " 'f_m': 6297,\n",
       " 'u_9': 46,\n",
       " 'w_k': 8361,\n",
       " 'a_7': 738,\n",
       " '7_x': 2703,\n",
       " 'i_.': 444,\n",
       " '._h': 972,\n",
       " '2_3': 9210,\n",
       " 'w_l': 9222,\n",
       " 'r_h': 6809,\n",
       " '\\n': 41703,\n",
       " 'y_\\n': 1768,\n",
       " '\\n_s': 1533,\n",
       " ')_\\n': 1069,\n",
       " '\\n_w': 1288,\n",
       " '?_\\n': 1743,\n",
       " '\\n_t': 1938,\n",
       " '0_t': 4673,\n",
       " '3_t': 698,\n",
       " '\\n_<': 18986,\n",
       " 's_g': 5381,\n",
       " '\\n_i': 3442,\n",
       " 'g_\\n': 765,\n",
       " 'l_\\n': 1302,\n",
       " '\\n_-': 83,\n",
       " 'y_p': 16060,\n",
       " 'e_\\n': 3450,\n",
       " '\\n_q': 29,\n",
       " 't_\\n': 2062,\n",
       " '\\n_a': 1066,\n",
       " '>_\\n': 6184,\n",
       " '\\n_b': 682,\n",
       " 'i_\\n': 115,\n",
       " '\\n_g': 840,\n",
       " 'b_-': 2005,\n",
       " '-_d': 8916,\n",
       " '4_2': 7670,\n",
       " '3_5': 7508,\n",
       " '5_8': 3225,\n",
       " ' _7': 34003,\n",
       " '7_,': 335,\n",
       " ',_6': 182,\n",
       " '6_6': 6926,\n",
       " '6_5': 5840,\n",
       " '2_1': 13438,\n",
       " '1_s': 5424,\n",
       " 'g_b': 18046,\n",
       " 'n_x': 5122,\n",
       " 'k_r': 6647,\n",
       " 'k_p': 5483,\n",
       " 's_b': 14510,\n",
       " '-_o': 6465,\n",
       " 'h_w': 5385,\n",
       " ' _8': 30047,\n",
       " '8_3': 3247,\n",
       " '#_o': 11438,\n",
       " 'e_-': 15292,\n",
       " '-_n': 2595,\n",
       " '2_2': 9937,\n",
       " ' _0': 18059,\n",
       " '_': 28498,\n",
       " '0__': 288,\n",
       " '__o': 1205,\n",
       " '1_.': 18068,\n",
       " '2_9': 5833,\n",
       " '9_t': 1833,\n",
       " 'h_b': 5610,\n",
       " '2_a': 1492,\n",
       " 'e_.': 2209,\n",
       " '._t': 1741,\n",
       " 'v_.': 95,\n",
       " '._c': 1097,\n",
       " 'h_.': 487,\n",
       " '._b': 1035,\n",
       " '$': 10821,\n",
       " ' _$': 10490,\n",
       " '$_ ': 10101,\n",
       " 's_q': 6601,\n",
       " 'h_s': 17408,\n",
       " \"o_'\": 8392,\n",
       " '^': 13498,\n",
       " ' _^': 13355,\n",
       " '^_ ': 11416,\n",
       " 'w_t': 8831,\n",
       " 'b_m': 6355,\n",
       " '1_:': 1747,\n",
       " 'u_w': 1270,\n",
       " 'b_h': 3137,\n",
       " 'b_f': 4569,\n",
       " ']': 9425,\n",
       " ';_]': 71,\n",
       " ']_ ': 6650,\n",
       " '#_4': 1234,\n",
       " '9_-': 1747,\n",
       " '-_3': 2716,\n",
       " 'x_m': 1540,\n",
       " 'f_y': 4910,\n",
       " 'y_k': 1842,\n",
       " '-_u': 3456,\n",
       " 'i_j': 2788,\n",
       " 'z_o': 11402,\n",
       " 'm_d': 3599,\n",
       " 'k_t': 5954,\n",
       " '3_b': 546,\n",
       " 'b_p': 1421,\n",
       " 'p_3': 2230,\n",
       " '2_n': 5020,\n",
       " 'r_-': 7177,\n",
       " '6_.': 3164,\n",
       " '._3': 5196,\n",
       " 'b_d': 6533,\n",
       " 'b_k': 4519,\n",
       " '~': 10647,\n",
       " ' _~': 10198,\n",
       " '~_ ': 9258,\n",
       " 'x_d': 4586,\n",
       " 'n_q': 2689,\n",
       " 'q_ ': 11876,\n",
       " 'x_p': 21399,\n",
       " '3_3': 11587,\n",
       " 'r_\\n': 991,\n",
       " 'j_s': 2654,\n",
       " '3_r': 2430,\n",
       " '-_e': 3853,\n",
       " 'j_h': 654,\n",
       " \"'_e\": 819,\n",
       " '2_m': 8012,\n",
       " 'c_w': 861,\n",
       " 'y__': 291,\n",
       " '__m': 260,\n",
       " 's_z': 1008,\n",
       " 'z_a': 18765,\n",
       " '<_ ': 23716,\n",
       " 'u_\\n': 850,\n",
       " 'o_\\n': 883,\n",
       " '\\n_j': 406,\n",
       " '!_\\n': 4223,\n",
       " '\\n_m': 1082,\n",
       " '\\n_e': 339,\n",
       " 'x_\\n': 1842,\n",
       " \"z_'\": 283,\n",
       " '\\n_#': 994,\n",
       " 'h_\\n': 675,\n",
       " 'u_j': 3096,\n",
       " 'h_-': 4292,\n",
       " '8_.': 2057,\n",
       " '._5': 12701,\n",
       " 'f_k': 847,\n",
       " 'l_n': 6266,\n",
       " ':_/': 9704,\n",
       " 'x_3': 8613,\n",
       " '0_3': 7969,\n",
       " '3_2': 10175,\n",
       " 'v_l': 998,\n",
       " 'c_f': 2571,\n",
       " 'g_f': 2994,\n",
       " \"'_n\": 1148,\n",
       " 'y_j': 880,\n",
       " 'i_q': 7697,\n",
       " '0_.': 6343,\n",
       " '._0': 7944,\n",
       " 'f_h': 1038,\n",
       " '8_t': 1909,\n",
       " 'u_x': 6987,\n",
       " \"c_'\": 1565,\n",
       " 'c_j': 428,\n",
       " 's_1': 1407,\n",
       " '|': 16149,\n",
       " ' _|': 14098,\n",
       " '|_ ': 15553,\n",
       " '2_7': 9803,\n",
       " 'm_n': 18282,\n",
       " 'h_n': 16227,\n",
       " '8_7': 2148,\n",
       " '1_,': 1231,\n",
       " ',_7': 178,\n",
       " '7_0': 11014,\n",
       " 'e_q': 6557,\n",
       " \"'_o\": 482,\n",
       " \"r_'\": 12103,\n",
       " ' __': 13407,\n",
       " '__ ': 13030,\n",
       " 'k_z': 730,\n",
       " 'd_\\n': 1310,\n",
       " '\\n_l': 848,\n",
       " 'f_\\n': 193,\n",
       " 'a_\\n': 764,\n",
       " 'w_c': 1749,\n",
       " ';_p': 1093,\n",
       " 'u_v': 6081,\n",
       " '9_a': 811,\n",
       " 't_q': 281,\n",
       " 'q_3': 111,\n",
       " '8_1': 3656,\n",
       " '3_1': 7691,\n",
       " ':_\\\\': 412,\n",
       " '2_5': 22750,\n",
       " '0_k': 1394,\n",
       " 'o_5': 77,\n",
       " 'o_-': 10544,\n",
       " \"l_'\": 3857,\n",
       " 'y_h': 4412,\n",
       " 'v_m': 1614,\n",
       " 'm_c': 10531,\n",
       " 'j_l': 1357,\n",
       " 'h_k': 1564,\n",
       " 'k_w': 6586,\n",
       " 't_x': 4088,\n",
       " 'w_p': 1751,\n",
       " '2_6': 14564,\n",
       " '6_2': 4453,\n",
       " 't_g': 4634,\n",
       " '._8': 2532,\n",
       " ':_)': 8,\n",
       " 'r_q': 1114,\n",
       " '0_a': 3823,\n",
       " '9_9': 8447,\n",
       " 'x_-': 3261,\n",
       " '-_k': 1308,\n",
       " '-_i': 5193,\n",
       " '-_c': 7668,\n",
       " 'b_v': 3156,\n",
       " 'x_8': 852,\n",
       " '#_ ': 10739,\n",
       " 'i_h': 6232,\n",
       " 'p_f': 2664,\n",
       " '8_-': 3122,\n",
       " '-_1': 8842,\n",
       " '6_0': 15634,\n",
       " '\\n_y': 491,\n",
       " 'n_\\n': 1049,\n",
       " '._\\n': 3639,\n",
       " '\\n_d': 523,\n",
       " '\\n_c': 725,\n",
       " 's_\\n': 2269,\n",
       " '\\n_r': 1063,\n",
       " 'm_\\n': 454,\n",
       " 'v_v': 1433,\n",
       " '\\n_o': 717,\n",
       " '(_\\n': 309,\n",
       " '3_\\n': 662,\n",
       " '\\n_h': 1165,\n",
       " '\\n_n': 579,\n",
       " 'd_c': 24985,\n",
       " '6_:': 845,\n",
       " '3_8': 4527,\n",
       " \"'_k\": 211,\n",
       " 's_f': 9124,\n",
       " '._i': 880,\n",
       " 'w_d': 4453,\n",
       " 'b_j': 2319,\n",
       " 'k_g': 2981,\n",
       " 'w_g': 1332,\n",
       " 'l_x': 709,\n",
       " \"'_c\": 565,\n",
       " 'n_w': 6238,\n",
       " 'v_r': 2309,\n",
       " '-_a': 6169,\n",
       " 'a_-': 9898,\n",
       " 'l_v': 15250,\n",
       " 'y_4': 428,\n",
       " 'l_-': 6968,\n",
       " 'x_l': 2365,\n",
       " '2_t': 811,\n",
       " '=_)': 2610,\n",
       " 'h_j': 984,\n",
       " 'q_q': 633,\n",
       " 'q_y': 53,\n",
       " 'i_y': 4201,\n",
       " 'p_c': 15348,\n",
       " ' _@': 5409,\n",
       " '-_2': 4238,\n",
       " 'l_h': 2574,\n",
       " '=_d': 376,\n",
       " 'w_u': 3986,\n",
       " 'b_z': 1494,\n",
       " 'u_q': 675,\n",
       " 'q_w': 177,\n",
       " 'h_p': 10054,\n",
       " ...}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_.vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start here - tokenization and word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n",
      "<gensim.interfaces.TransformedCorpus object at 0x371c87220>\n",
      "<gensim.interfaces.TransformedCorpus object at 0x371c731c0>\n"
     ]
    }
   ],
   "source": [
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer(\n",
    "    preserve_case=True,\n",
    "    reduce_len=True,\n",
    ")\n",
    "from gensim.models import Phrases\n",
    "tkns = df_train['tweet'].apply(lambda x: tokenizer.tokenize(x))\n",
    "bigram = Phrases(tkns)\n",
    "bigram.save('data/grams/bigram.model')\n",
    "#ok so we want the first version, bigram from tokenized tweets\n",
    "# we should then add it to the tokenized tweets\n",
    "print(bigram[tkns[0]])\n",
    "print(bigram[tkns[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39285957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2458295"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bigram.corpus_word_count)\n",
    "#print(bigram.vocab) - same here\n",
    "tkns_bigram = [bigram[tweet] for tweet in tkns]\n",
    "#print(tkns_bigram[0])\n",
    "#print(len(tkns_bigram[0])) - same, huge output!\n",
    "#print(tweets) - I mitakenly left this running, its been printing for 50min lol\n",
    "\n",
    "len(tkns_bigram[0])\n",
    "len(tkns_bigram)\n",
    "len(tkns_bigram[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "26\n",
      "13\n",
      "2458295\n",
      "<gensim.interfaces.TransformedCorpus object at 0x32a43c040>\n",
      "['<user>', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god_knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15']\n",
      "0    [<user>, i, dunno, justin, read, my, mention, ...\n",
      "0    [vinco, tresorpack, 6, (, difficulty, 10, of, ...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(len(tkns_bigram[1]))\n",
    "print(len(tkns_bigram[0]))\n",
    "print(len(tkns_bigram[2]))\n",
    "print(len(tkns_bigram))\n",
    "print(bigram[tkns[0]])\n",
    "print(tkns_bigram[0])\n",
    "print(tkns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [<user>, i, dunno, justin, read, my, mention, ...\n",
      "0    [vinco, tresorpack, 6, (, difficulty, 10, of, ...\n",
      "Name: tweet, dtype: object\n",
      "['<user>', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god_knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15']\n",
      "1    [because, your, logic, is, so, dumb, ,, i, won...\n",
      "1    [glad, i, dot, have, taks, tomorrow, !, !, #th...\n",
      "Name: tweet, dtype: object\n",
      "['because', 'your', 'logic', 'is', 'so', 'dumb', ',', 'i', \"won't\", 'even', 'crop', 'out', 'your', 'name', 'or', 'your', 'photo', '.', 'tsk', '.', '<url>']\n",
      "2\n",
      "26\n",
      "2\n",
      "21\n",
      "0    <user> i dunno justin read my mention or not ....\n",
      "0    vinco tresorpack 6 ( difficulty 10 of 10 objec...\n",
      "Name: tweet, dtype: object\n",
      "1    because your logic is so dumb , i won't even c...\n",
      "1    glad i dot have taks tomorrow ! ! #thankful #s...\n",
      "Name: tweet, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                      tweet  label\n",
       "0        <user> i dunno justin read my mention or not ....      1\n",
       "1        because your logic is so dumb , i won't even c...      1\n",
       "2         <user> just put casper in a box !  looved the...      1\n",
       "3        <user> <user> thanks sir > > don't trip lil ma...      1\n",
       "4        visiting my brother tmr is the bestest birthda...      1\n",
       "...                                                    ...    ...\n",
       "1239635  im so sorry ! <user> & to <user> & <user> u gu...      0\n",
       "1239636                i can't find food coloring anywhere      0\n",
       "1239637  <user> same here ! ! but tort ! ! wonder why y...      0\n",
       "1239638  keyless entry remote fob clicker for 2005 buic...      0\n",
       "1239639  <user> yeap . doctor don't know what's wrong w...      0\n",
       "\n",
       "[2458295 rows x 2 columns]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tkns[0])\n",
    "print(tkns_bigram[0])\n",
    "print(tkns[1])\n",
    "print(tkns_bigram[1])\n",
    "print(len(tkns[0]))\n",
    "print(len(tkns_bigram[0]))\n",
    "print(len(tkns[1]))\n",
    "print(len(tkns_bigram[1]))\n",
    "print(df_train['tweet'][0])\n",
    "print(df_train['tweet'][1])\n",
    "\n",
    "# why are they twice per tweet? are we loding them badly?\n",
    "df_train.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  label\n",
      "0  <user> i dunno justin read my mention or not ....      1\n",
      "1  because your logic is so dumb , i won't even c...      1\n",
      "2   <user> just put casper in a box !  looved the...      1\n",
      "3  <user> <user> thanks sir > > don't trip lil ma...      1\n",
      "4  visiting my brother tmr is the bestest birthda...      1\n",
      "0    <user> i dunno justin read my mention or not ....\n",
      "0    vinco tresorpack 6 ( difficulty 10 of 10 objec...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.head())\n",
    "print(df_train['tweet'][0])\n",
    "# I think we might be loading the data in a weird way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n",
      "                                               tweet  label\n",
      "0  <user> i dunno justin read my mention or not ....      1\n",
      "1  because your logic is so dumb , i won't even c...      1\n",
      "2   <user> just put casper in a box !  looved the...      1\n",
      "3  <user> <user> thanks sir > > don't trip lil ma...      1\n",
      "4  visiting my brother tmr is the bestest birthda...      1\n",
      "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n"
     ]
    }
   ],
   "source": [
    "# I added 'ignore index=True' in the append function, oh my god we were\n",
    "# mixing the tweets, half of the phrase was from positive, half from negative. Jesus.\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt')\n",
    "print(df_train.head())\n",
    "print(df_train['tweet'][0])\n",
    "# holy, indeed we were literally mixing them up in the worst way possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<user>', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god_knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15']\n",
      "['<user>', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god_knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15']\n",
      "['<user>', 'i', 'dunno', 'justin', 'read', 'my', 'mention', 'or', 'not', '.', 'only', 'justin', 'and', 'god', 'knows', 'about', 'that', ',', 'but', 'i', 'hope', 'you', 'will', 'follow', 'me', '#believe', '15']\n",
      "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n"
     ]
    }
   ],
   "source": [
    "tkns = df_train['tweet'].apply(lambda x: tokenizer.tokenize(x))\n",
    "bigram = Phrases(tkns)\n",
    "bigram.save('data/grams/bigram.model')\n",
    "tkns_bigram = [bigram[tweet] for tweet in tkns]\n",
    "print(tkns_bigram[0])\n",
    "print(bigram[tkns[0]])\n",
    "print(tkns[0])\n",
    "print(df_train['tweet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37007683\n",
      "6280053\n",
      "35728076\n",
      "6563097\n"
     ]
    }
   ],
   "source": [
    "trigram = Phrases(tkns_bigram)\n",
    "trigram.save('data/grams/trigram.model')\n",
    "tkns_trigram = [trigram[tweet] for tweet in tkns_bigram]\n",
    "quadgram = Phrases(tkns_trigram)\n",
    "quadgram.save('data/grams/quadgram.model')\n",
    "tkns_quadgram = [quadgram[tweet] for tweet in tkns_trigram]\n",
    "print(trigram.corpus_word_count)\n",
    "print(len(trigram.vocab))\n",
    "print(quadgram.corpus_word_count)\n",
    "print(len(quadgram.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2458295\n",
      "26\n",
      "21\n",
      "13\n",
      "26\n",
      "21\n",
      "13\n",
      "26\n",
      "21\n",
      "13\n",
      "27\n",
      "21\n",
      "13\n",
      "50621\n",
      "None\n",
      "38\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "          bigram      score\n",
      "0      god_knows  15.648932\n",
      "1            >_>  48.607685\n",
      "2       lil_mama  22.311112\n",
      "3       ya_thang  10.728545\n",
      "4  birthday_gift  12.794833\n",
      "         trigram      score\n",
      "0      god_knows  69.396746\n",
      "1            >_>  82.314324\n",
      "2       lil_mama  36.256913\n",
      "3       ya_thang  19.838160\n",
      "4  birthday_gift  49.708501\n",
      "          quadgram      score\n",
      "0        god_knows  43.859900\n",
      "1              >_>  44.844841\n",
      "2         lil_mama  40.265468\n",
      "3    birthday_gift  49.967300\n",
      "4  becoming_famous  46.053912\n",
      "       w  v\n",
      "0   halo  1\n",
      "1  halo2  2\n",
      "2  halo3  3\n"
     ]
    }
   ],
   "source": [
    "# in theory they really should include the bigram and trigram and unigram as well\n",
    "# at this point, so we should have something similar to the (1,4) ngram in vectorizer of sklearn\n",
    "# let's now use this new vocabulary to train a fasttext model, with many more features like 300\n",
    "# or even 500 (maybe now let's keep it at 300) or even test with just 25 - 100\n",
    "# this should be a very dense matrix, so we should be able to use the same model as before\n",
    "# and now it would actually make sense to average the vectors of each word\n",
    "# we can also use the vectors of the phrases up to quadgrams since we should have them\n",
    "# if indeed they are considered worthy of being bigrams and trigrams etc\n",
    "print(len(tkns_quadgram))\n",
    "print(len(tkns_quadgram[0]))\n",
    "print(len(tkns_quadgram[1]))\n",
    "print(len(tkns_quadgram[2]))\n",
    "print(len(tkns_trigram[0]))\n",
    "print(len(tkns_trigram[1]))\n",
    "print(len(tkns_trigram[2]))\n",
    "print(len(tkns_bigram[0]))\n",
    "print(len(tkns_bigram[1]))\n",
    "print(len(tkns_bigram[2]))\n",
    "print(len(tkns[0]))\n",
    "print(len(tkns[1]))\n",
    "print(len(tkns[2]))\n",
    "# let's save the bigram, trigram and quadgram models\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText(\n",
    "    vector_size=100, \n",
    "    window=5, \n",
    "    min_n=3,\n",
    "    max_n=6,\n",
    "    sg=0,\n",
    "    )\n",
    "\n",
    "# at this stage i want to make sure the tri and quadgram are actually working\n",
    "# so i want to see if they are actually in the vocabulary\n",
    "\n",
    "#let's query the vocabulary\n",
    "print(trigram.vocab.get('i_love'))\n",
    "print(trigram.vocab.get('i_love_you'))\n",
    "print(trigram.vocab.get('only_god_knows')) # gotcha!\n",
    "# ok seems like they are working, let's now train the model\n",
    "# we can actually export the phrases at each stage!\n",
    "#print(len(trigram.export_phrases()))\n",
    "#print(len(quadgram.export_phrases()))\n",
    "\n",
    "# let's save theses phrases to dataframe to then export them to csv\n",
    "# dct_bigrams = dict(bigram.export_phrases())\n",
    "# dct_trigrams = dict(trigram.export_phrases())\n",
    "# dct_quadgrams = dict(quadgram.export_phrases())\n",
    "\n",
    "# why are they not working? they are empty\n",
    "# dct_bigrams = dict()\n",
    "# dct_trigrams = dict()\n",
    "# dct_quadgrams = dict()\n",
    "dct_bigrams = bigram.export_phrases()\n",
    "dct_trigrams = trigram.export_phrases()\n",
    "dct_quadgrams = quadgram.export_phrases()\n",
    "\n",
    "# dct_bigrams = {k:[v] for k,v in bigram.export_phrases().items()}\n",
    "# dct_trigrams = {k:[v] for k,v in trigram.export_phrases().items()}\n",
    "# dct_quadgrams = {k:[v] for k,v in quadgram.export_phrases().items()}\n",
    "print(dct_bigrams.get('i_love'))\n",
    "print(dct_trigrams.get('i_love_you'))\n",
    "print(dct_quadgrams.get('only_god_knows'))\n",
    "print(dct_trigrams.get('only_god_knows'))\n",
    "print(dct_quadgrams.get('i_love'))\n",
    "\n",
    "df_bigrams = pd.DataFrame(dct_bigrams.items(), columns=['bigram', 'score'])\n",
    "df_trigrams = pd.DataFrame(dct_trigrams.items(), columns=['trigram', 'score'])\n",
    "df_quadgrams = pd.DataFrame(dct_quadgrams.items(), columns=['quadgram', 'score'])\n",
    "print(df_bigrams.head())\n",
    "print(df_trigrams.head())\n",
    "print(df_quadgrams.head())\n",
    "# incredible I can't put the dictionary into a dataframe nor in a dictionary?????\n",
    "\n",
    "dictionary = {'halo': 1, 'halo2': 2, 'halo3': 3}\n",
    "df = pd.DataFrame(dictionary.items(), columns=['w', 'v'])\n",
    "print(df.head())\n",
    "\n",
    "#df_bigram = pd.DataFrame(bigram.export_phrases(), columns=['bigram', 'score'])\n",
    "#df_trigram = pd.DataFrame(trigram.export_phrases(), columns=['trigram', 'score'])\n",
    "\n",
    "# finally! let's save them in csv\n",
    "df_bigrams.to_csv('data/grams/bigrams_phrases.csv', index=False, sep= ' ')\n",
    "df_trigrams.to_csv('data/grams/trigrams_phrases.csv', index=False, sep= ' ')\n",
    "df_quadgrams.to_csv('data/grams/quadgrams_phrases.csv', index=False, sep= ' ')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idea for v5:\n",
    "The big remaining annoying thing to this moment is the following:\n",
    "tfidf does not givemmuch semantic meaning, but appllying it with ngrams it does, furthermore the big 'problems' with having this huge matrix are actually nullified by the optimizations of numpy and the various libraries we use. We won't really have to deal with a 8digit feature vector since it's a huge sparse matrix. Usually the non-zero values will be a few tens, this including 1 to 4 grams!\n",
    "\n",
    "Embeddings might be useful for more difficult tasks, but here we are really trying to divide the space in binary, not much. Furthermore everything falls the second we try to just 'average' the vectors over the sentences.\n",
    "\n",
    "Idea: let's leverage both methods. We have the luxury of having enough data and nowadays there are very complex models that can learn to ignore the zero-values in various features. Let's encode each word and the various grams with embeddings that are trained on the dataset such that themselves they even retain notion of similarities. Instead of alining them in the same space or 'squashing them' by averaging for every tweet, let's reduce the dimensionality and add them side by side.\n",
    "We leverage that twitter has a maximum number of characters per tweet, and more simply we can find an optimal balance. We recover the property of sparce matrices while maintaining the extra meaning given by the embeddings.\n",
    "\n",
    "The remaining element would be to normalize based on our own dataset. That is, our embeddings might have learned that 'love' is similar to 'iloveuyou', but we could waight them exactly in the same way that we would go from BOW to tfidf.\n",
    "\n",
    "\n",
    "It's fundamentally a different dimension than before:\n",
    "- before we had a very large, sparse vector, with features = vocabulary\n",
    "- now we have embeddings to encode each word in vocabulary, and features = embeddings of each word in the tweet\n",
    "\n",
    "By weighting the embeddings we can recover some of the fundamental info carried by the tfidf: each data point carries the info about its own words in comparison to the other words in the dataset. This is a very powerful property.\n",
    "\n",
    "The model will hopefully be able to learn that 'first 20 features = first word of the tweet' and so on.\n",
    "Technically it should have all the info to know which word it is as well as 'the meaning' of the word, 'its context' and so on, given that the embeddings are trained to capture this info.\n",
    "\n",
    "Let's see if this makes sense. the only final problem would be that tfidf can allow itself to just throw extra features for n-grams, while here dimensionality will be a problem so we can try different strategies. We built a model of embeddings that doesn't just randomly pick all grams, but decides which are worth bundling and which not. This handles word-grams. Using Fasttext we handle semantic meaning of grams inside the words themselves!\n",
    "\n",
    "Tying everything together we should finally have all the info necessary to split the ata in a meaningful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#1love', 0.960010290145874), ('jlove', 0.9505100250244141), ('llove', 0.9482436776161194), ('i.love', 0.9428744912147522), (\"m'love\", 0.9425817728042603), ('1love', 0.941887617111206), ('#alllove', 0.9286666512489319), ('yveslove', 0.9282400012016296), ('#mylove', 0.9260808825492859), ('#bfflove', 0.91941899061203)]\n",
      "[('?_love_youuu_xxx', 0.9408364295959473), ('love_you.let', 0.9395909309387207), ('love_youuu_xxx', 0.9277811646461487), ('love_your_icon', 0.9266449213027954), ('love_youuu', 0.9185110330581665), ('ya_love_your_#britishmahomies', 0.9147262573242188), ('love_yooou', 0.9137768745422363), ('love_youte_amo.belieber_forever', 0.9066047072410583), (\"you_love_niall's_laugh\", 0.9031429290771484), ('looove_youuu', 0.8993458151817322)]\n"
     ]
    }
   ],
   "source": [
    "# let's put it together with the grams and fasttext\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText(\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_n=3,\n",
    "    max_n=10,\n",
    "    sg=0,\n",
    ")\n",
    "fasttext.build_vocab(corpus_iterable=tkns_quadgram)\n",
    "fasttext.train(corpus_iterable=tkns_quadgram, total_examples=fasttext.corpus_count, epochs=fasttext.epochs)\n",
    "print(fasttext.wv.most_similar('i_love'))\n",
    "print(fasttext.wv.most_similar('i_love_you'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.save('data/fasttext/fasttext_100_4grams.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#1love', 0.9798915982246399), ('jlove', 0.9722762107849121), ('llove', 0.966873824596405), (\"m'love\", 0.9620018601417542), ('te_amooo_love', 0.9604809880256653), ('smilers_love', 0.9565942883491516), ('i.love', 0.9565492272377014), ('1love', 0.9565232396125793), ('#alllove', 0.9555274844169617), ('#mylove', 0.9511960744857788)]\n"
     ]
    }
   ],
   "source": [
    "fasttext = FastText(\n",
    "    vector_size=20,\n",
    "    window=5,\n",
    "    min_n=3,\n",
    "    max_n=10,\n",
    "    sg=0,\n",
    ")\n",
    "fasttext.build_vocab(corpus_iterable=tkns_quadgram)\n",
    "fasttext.train(corpus_iterable=tkns_quadgram, total_examples=fasttext.corpus_count, epochs=fasttext.epochs)\n",
    "print(fasttext.wv.most_similar('i_love'))\n",
    "\n",
    "fasttext.save('data/fasttext/fasttext_20_4grams.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#1love', 0.9569266438484192), ('jlove', 0.9501928091049194), ('llove', 0.9441788792610168), (\"m'love\", 0.9438387155532837), ('i.love', 0.9435019493103027), ('1love', 0.9431397318840027), ('#mylove', 0.9317726492881775), ('#alllove', 0.9172578454017639), ('#bfflove', 0.9172359108924866), ('yveslove', 0.9163663983345032)]\n"
     ]
    }
   ],
   "source": [
    "fasttext = FastText(\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_n=3,\n",
    "    max_n=10,\n",
    "    sg=0,\n",
    ")\n",
    "fasttext.build_vocab(corpus_iterable=tkns_quadgram)\n",
    "fasttext.train(corpus_iterable=tkns_quadgram, total_examples=fasttext.corpus_count, epochs=fasttext.epochs)\n",
    "print(fasttext.wv.most_similar('i_love'))\n",
    "\n",
    "fasttext.save('data/fasttext/fasttext_300_4grams.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n",
      "(20,)\n",
      "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext = FastText.load('data/fasttext/fasttext_20_4grams.model')\n",
    "df_train = load_train_data()\n",
    "# Great, let's see what we can do with the fasttext model\n",
    "# we could even do some crazy approssimation like entire tweets \n",
    "print(fasttext.wv.get_sentence_vector(df_train['tweet'][0]).shape)\n",
    "print(df_train['tweet'][0])\n",
    " # oh ok there's a built in method?? nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(\n",
    "    preserve_case=True,\n",
    "    reduce_len=True,\n",
    ")\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tknzr.tokenize(x)).apply(lambda x: ' '.join(x))\n",
    "print(df_train['tweet'][0])\n",
    "X_train_vec = np.zeros((df_train.shape[0], 20))\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    X_train_vec[i] = fasttext.wv.get_sentence_vector(tweet)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train_vec, df_train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (1966636, 20)  Evaluation set size:  (491659, 20)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.645256163316445\n",
      "F1 score:  0.6456151187527557\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.6451626025354972\n",
      "F1 score:  0.6441819768959666\n",
      "Confusion matrix: \n",
      "[[158374  88739]\n",
      " [ 85674 158872]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "#nb = train_test(nb, X_train, y_train, X_eval, y_eval)\n",
    "linsvc = LinearSVC()\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)\n",
    "# multinomialNB can't handle negative \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (1966636, 20)  Evaluation set size:  (491659, 20)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6264931588763757\n",
      "F1 score:  0.6233761972148733\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9985635369229486\n",
      "F1 score:  0.9985499770306095\n",
      "Confusion matrix: \n",
      "[[156045  91068]\n",
      " [ 92570 151976]]\n"
     ]
    }
   ],
   "source": [
    "# uhuhuh now its time for trees and forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "y_train = y_train.apply(lambda x: -1 if x == 0 else x)\n",
    "y_eval = y_eval.apply(lambda x: -1 if x == 0 else x)\n",
    "\n",
    "lil_tree = DecisionTreeClassifier(\n",
    "    splitter='best',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    ")\n",
    "lil_tree = train_test(lil_tree, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:36:54] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (1966636, 20)  Evaluation set size:  (491659, 20)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6717847125751791\n",
      "F1 score:  0.6801488951787455\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.6794490693753191\n",
      "F1 score:  0.6865292097928699\n",
      "Confusion matrix: \n",
      "[[158716  88397]\n",
      " [ 72973 171573]]\n"
     ]
    }
   ],
   "source": [
    "# noice, lets boost this lil tree\n",
    "xgb = XGBClassifier()\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary:logistic', 'use_label_encoder': True, 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'enable_categorical': False, 'gamma': 0, 'gpu_id': -1, 'importance_type': None, 'interaction_constraints': '', 'learning_rate': 0.300000012, 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 100, 'n_jobs': 10, 'num_parallel_tree': 1, 'predictor': 'auto', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "print(xgb.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:41:54] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (1966636, 20)  Evaluation set size:  (491659, 20)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6824384380231013\n",
      "F1 score:  0.6918017512968914\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.7420539438920064\n",
      "F1 score:  0.7484386700057029\n",
      "Confusion matrix: \n",
      "[[160295  86818]\n",
      " [ 69314 175232]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_train = y_train.apply(lambda x: 0 if x != 1 else x)\n",
    "y_eval = y_eval.apply(lambda x: 0 if x != 1 else x)\n",
    "xgb = XGBClassifier(\n",
    "    max_depth=10,\n",
    "    n_estimators=100,\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (196970, 2)\n",
      "Train set positives:  (97902, 2)\n",
      "Train set negatives:  (99068, 2)\n",
      "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n"
     ]
    }
   ],
   "source": [
    "#honestly since we are using 20 feautures this is prretty nice\n",
    "# lets work on small dataset to iterate faster\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(\n",
    "    preserve_case=True,\n",
    "    reduce_len=True,\n",
    ")\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText.load('data/fasttext/fasttext_300_4grams.model')\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tknzr.tokenize(x)).apply(lambda x: ' '.join(x))\n",
    "print(df_train['tweet'][0])\n",
    "X_train_vec = np.zeros((df_train.shape[0], 300))\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    X_train_vec[i] = fasttext.wv.get_sentence_vector(tweet)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train_vec, df_train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157576, 300)  Evaluation set size:  (39394, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6849774077270651\n",
      "F1 score:  0.6978182526541347\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.6815949129309031\n",
      "F1 score:  0.6958714456304971\n",
      "Confusion matrix: \n",
      "[[12655  7225]\n",
      " [ 5185 14329]]\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.apply(lambda x: -1 if x != 1 else x)\n",
    "y_eval = y_eval.apply(lambda x: -1 if x != 1 else x)\n",
    "linsvc = LinearSVC()\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:53:47] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (157576, 300)  Evaluation set size:  (39394, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7133065949129309\n",
      "F1 score:  0.7238225656575537\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9999175001269229\n",
      "F1 score:  0.9999170807314754\n",
      "Confusion matrix: \n",
      "[[13300  6580]\n",
      " [ 4714 14800]]\n"
     ]
    }
   ],
   "source": [
    "y_eval = y_eval.apply(lambda x: 0 if x != 1 else x)\n",
    "y_train = y_train.apply(lambda x: 0 if x != 1 else x)\n",
    "xgb = XGBClassifier(\n",
    "    max_depth=15,\n",
    "    n_estimators=100,\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:16] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (157576, 300)  Evaluation set size:  (39394, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7096512159212063\n",
      "F1 score:  0.718677750996114\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9999048078387571\n",
      "F1 score:  0.9999043263619142\n",
      "Confusion matrix: \n",
      "[[13346  6534]\n",
      " [ 4904 14610]]\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    max_depth=10,\n",
    "    n_estimators=150,\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    196970.000000\n",
       "mean         15.957329\n",
       "std          14.567898\n",
       "min           1.000000\n",
       "25%          10.000000\n",
       "50%          15.000000\n",
       "75%          22.000000\n",
       "max        1737.000000\n",
       "Name: tweet, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think we are reachin the limit on the info that a 300 dim vector can give us\n",
    "# let's see if the strategy mentioned could work\n",
    "df_train['tweet'].apply(lambda x: len(x.split(' '))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[&lt;user&gt;, i, dunno, justin, read, my, mention, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[because, your, logic, is, so, dumb, ,, i, won...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[&lt;user&gt;, just, put, casper, in, a, box, !, loo...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[&lt;user&gt;, &lt;user&gt;, thanks, sir, &gt;, &gt;, don't, tri...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[visiting, my, brother, tmr, is, the, bestest,...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196965</th>\n",
       "      <td>[can't, wait, to, fake, tan, tonight, !, hate,...</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196966</th>\n",
       "      <td>[&lt;user&gt;, darling, i, lost, my, internet, conne...</td>\n",
       "      <td>-1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196967</th>\n",
       "      <td>[kanguru, defender, basic, 4, gb, usb, 2.0, fl...</td>\n",
       "      <td>-1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196968</th>\n",
       "      <td>[rizan, is, sad, now]</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196969</th>\n",
       "      <td>[no, text, back, ?, yea, ,, he, mad]</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196970 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet  label  tweet_len\n",
       "0       [<user>, i, dunno, justin, read, my, mention, ...      1         27\n",
       "1       [because, your, logic, is, so, dumb, ,, i, won...      1         21\n",
       "2       [<user>, just, put, casper, in, a, box, !, loo...      1         13\n",
       "3       [<user>, <user>, thanks, sir, >, >, don't, tri...      1         17\n",
       "4       [visiting, my, brother, tmr, is, the, bestest,...      1         13\n",
       "...                                                   ...    ...        ...\n",
       "196965  [can't, wait, to, fake, tan, tonight, !, hate,...     -1         10\n",
       "196966  [<user>, darling, i, lost, my, internet, conne...     -1         33\n",
       "196967  [kanguru, defender, basic, 4, gb, usb, 2.0, fl...     -1         23\n",
       "196968                              [rizan, is, sad, now]     -1          4\n",
       "196969               [no, text, back, ?, yea, ,, he, mad]     -1          8\n",
       "\n",
       "[196970 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where the hell is a 1737 words tweet??\n",
    "df_train_eplore = df_train.copy()\n",
    "df_train_eplore['tweet'] = df_train_eplore['tweet'].apply(lambda x: tknzr.tokenize(x))\n",
    "df_train_eplore['tweet_len'] = df_train_eplore['tweet'].apply(lambda x: len(x))\n",
    "df_train_eplore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>196970.000000</td>\n",
       "      <td>196970.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.005920</td>\n",
       "      <td>15.952135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.999985</td>\n",
       "      <td>14.561441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1737.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label      tweet_len\n",
       "count  196970.000000  196970.000000\n",
       "mean       -0.005920      15.952135\n",
       "std         0.999985      14.561441\n",
       "min        -1.000000       1.000000\n",
       "25%        -1.000000      10.000000\n",
       "50%        -1.000000      15.000000\n",
       "75%         1.000000      22.000000\n",
       "max         1.000000    1737.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_eplore.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>[&lt;user&gt;, happy, #thongthursday, tweetybirds, &lt;...</td>\n",
       "      <td>1</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>[&lt;user&gt;, babe, came, over, to, comfort, me, sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>[&lt;user&gt;, today, is, national, stalking, awaren...</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>[&lt;user&gt;, looks, like, chicken, night, ., so, v...</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6198</th>\n",
       "      <td>[&lt;user&gt;, rt, if, you, get, it, &lt;url&gt;, this, is...</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180407</th>\n",
       "      <td>[&lt;user&gt;, what, up, bubbles, !, miss, ko, na, s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183478</th>\n",
       "      <td>[&lt;user&gt;, &lt;url&gt;, -, italy, probing, 51, mn, usd...</td>\n",
       "      <td>-1</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183609</th>\n",
       "      <td>[&lt;user&gt;, recovery, money, redirected, to, supe...</td>\n",
       "      <td>-1</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188539</th>\n",
       "      <td>[&lt;user&gt;, directionator, .., rt, when, you, see...</td>\n",
       "      <td>-1</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189633</th>\n",
       "      <td>[&lt;user&gt;, well, i, feel, like, i, have, no, fri...</td>\n",
       "      <td>-1</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet  label  tweet_len\n",
       "492     [<user>, happy, #thongthursday, tweetybirds, <...      1        215\n",
       "866     [<user>, babe, came, over, to, comfort, me, sh...      1        165\n",
       "1084    [<user>, today, is, national, stalking, awaren...      1        118\n",
       "1379    [<user>, looks, like, chicken, night, ., so, v...      1        600\n",
       "6198    [<user>, rt, if, you, get, it, <url>, this, is...      1        172\n",
       "...                                                   ...    ...        ...\n",
       "180407  [<user>, what, up, bubbles, !, miss, ko, na, s...     -1        205\n",
       "183478  [<user>, <url>, -, italy, probing, 51, mn, usd...     -1        159\n",
       "183609  [<user>, recovery, money, redirected, to, supe...     -1        491\n",
       "188539  [<user>, directionator, .., rt, when, you, see...     -1        108\n",
       "189633  [<user>, well, i, feel, like, i, have, no, fri...     -1        400\n",
       "\n",
       "[110 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see outliers\n",
    "lon_tweets = df_train_eplore[df_train_eplore['tweet_len'] > 100]\n",
    "lon_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>110.00000</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.20000</td>\n",
       "      <td>431.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.98428</td>\n",
       "      <td>334.756586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.00000</td>\n",
       "      <td>102.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.00000</td>\n",
       "      <td>177.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>327.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>570.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1737.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label    tweet_len\n",
       "count  110.00000   110.000000\n",
       "mean     0.20000   431.136364\n",
       "std      0.98428   334.756586\n",
       "min     -1.00000   102.000000\n",
       "25%     -1.00000   177.250000\n",
       "50%      1.00000   327.000000\n",
       "75%      1.00000   570.500000\n",
       "max      1.00000  1737.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lon_tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16066</th>\n",
       "      <td>[&lt;user&gt;, me, &amp;, my, mama, &lt;url&gt;, her, thick, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87657</th>\n",
       "      <td>[&lt;user&gt;, now, following, ), follow, back, .., ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48645</th>\n",
       "      <td>[&lt;user&gt;, &lt;url&gt;, damn, that's, a, sexc, ass, pi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30139</th>\n",
       "      <td>[&lt;user&gt;, #ff, to, the, amazing, &lt;user&gt;, becky,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90586</th>\n",
       "      <td>[&lt;user&gt;, u, cab, come, get, it, from, me, ill,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63673</th>\n",
       "      <td>[&lt;user&gt;, &lt;user&gt;, pretty, white, girl, tried, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86731</th>\n",
       "      <td>[&lt;user&gt;, hello, ,, ust, ., i, could, get, used...</td>\n",
       "      <td>1</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27204</th>\n",
       "      <td>[&lt;user&gt;, ahhahaahhaha, loools, i, put, my, pho...</td>\n",
       "      <td>1</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163499</th>\n",
       "      <td>[&lt;user&gt;, that's, a, wrap, !, end, of, an, era,...</td>\n",
       "      <td>-1</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81963</th>\n",
       "      <td>[&lt;user&gt;, i, been, thinking, bout, yah, .., &lt;ur...</td>\n",
       "      <td>1</td>\n",
       "      <td>922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet  label  tweet_len\n",
       "16066   [<user>, me, &, my, mama, <url>, her, thick, a...      1       1737\n",
       "87657   [<user>, now, following, ), follow, back, .., ...      1       1628\n",
       "48645   [<user>, <url>, damn, that's, a, sexc, ass, pi...      1       1527\n",
       "30139   [<user>, #ff, to, the, amazing, <user>, becky,...      1       1300\n",
       "90586   [<user>, u, cab, come, get, it, from, me, ill,...      1       1156\n",
       "63673   [<user>, <user>, pretty, white, girl, tried, t...      1       1083\n",
       "86731   [<user>, hello, ,, ust, ., i, could, get, used...      1       1031\n",
       "27204   [<user>, ahhahaahhaha, loools, i, put, my, pho...      1       1025\n",
       "163499  [<user>, that's, a, wrap, !, end, of, an, era,...     -1        977\n",
       "81963   [<user>, i, been, thinking, bout, yah, .., <ur...      1        922"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lon_tweets.sort_values(by='tweet_len', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/60pqr7fj1bd13wyw3nwdcbb80000gn/T/ipykernel_9173/600120645.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lon_tweets['tweet'] = lon_tweets['tweet'].apply(lambda x: ' '.join(x))\n"
     ]
    }
   ],
   "source": [
    "lon_tweets['tweet'] = lon_tweets['tweet'].apply(lambda x: ' '.join(x))\n",
    "lon_tweets.sort_values(by='tweet_len', ascending=False).to_csv('data/out/lon_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>138.000000</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>356.644928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.999841</td>\n",
       "      <td>333.408340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>114.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>223.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>527.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1737.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label    tweet_len\n",
       "count  138.000000   138.000000\n",
       "mean     0.086957   356.644928\n",
       "std      0.999841   333.408340\n",
       "min     -1.000000    51.000000\n",
       "25%     -1.000000   114.250000\n",
       "50%      1.000000   223.500000\n",
       "75%      1.000000   527.500000\n",
       "max      1.000000  1737.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see if theres more\n",
    "df_train_eplore['tweet'] = df_train_eplore['tweet'].apply(lambda x: ' '.join(x))\n",
    "lon_tweets = df_train_eplore[df_train_eplore['tweet_len'] > 50].sort_values(by='tweet_len', ascending=False)\n",
    "lon_tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.207207</td>\n",
       "      <td>237.761261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.980508</td>\n",
       "      <td>303.703984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>323.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1737.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label    tweet_len\n",
       "count  222.000000   222.000000\n",
       "mean     0.207207   237.761261\n",
       "std      0.980508   303.703984\n",
       "min     -1.000000    41.000000\n",
       "25%     -1.000000    42.000000\n",
       "50%      1.000000    95.000000\n",
       "75%      1.000000   323.250000\n",
       "max      1.000000  1737.000000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# great, i say we just drop all tweeets longer than 40 words\n",
    "lon_tweets = df_train_eplore[df_train_eplore['tweet_len'] > 40].sort_values(by='tweet_len', ascending=False)\n",
    "lon_tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>196748.000000</td>\n",
       "      <td>196748.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.006160</td>\n",
       "      <td>15.701857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.999984</td>\n",
       "      <td>7.286423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label      tweet_len\n",
       "count  196748.000000  196748.000000\n",
       "mean       -0.006160      15.701857\n",
       "std         0.999984       7.286423\n",
       "min        -1.000000       1.000000\n",
       "25%        -1.000000      10.000000\n",
       "50%        -1.000000      15.000000\n",
       "75%         1.000000      22.000000\n",
       "max         1.000000      40.000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_eplore[df_train_eplore['tweet_len'] <= 40].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.207207</td>\n",
       "      <td>237.761261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.980508</td>\n",
       "      <td>303.703984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>323.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1737.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label    tweet_len\n",
       "count  222.000000   222.000000\n",
       "mean     0.207207   237.761261\n",
       "std      0.980508   303.703984\n",
       "min     -1.000000    41.000000\n",
       "25%     -1.000000    42.000000\n",
       "50%      1.000000    95.000000\n",
       "75%      1.000000   323.250000\n",
       "max      1.000000  1737.000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lon_tweets = df_train_eplore[df_train_eplore['tweet_len'] > 40].sort_values(by='tweet_len', ascending=False)\n",
    "lon_tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (196970, 2)\n",
      "Train set positives:  (97902, 2)\n",
      "Train set negatives:  (99068, 2)\n"
     ]
    }
   ],
   "source": [
    "# 40 words is a good cut off point\n",
    "# lets see if we can get a better model with this\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tknzr.tokenize(x)).apply(lambda x: ' '.join(x))\n",
    "df_train = df_train[df_train['tweet'].apply(lambda x: len(x.split(' '))) <= 40]\n",
    "X_train_vec = np.zeros((df_train.shape[0], 300))\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    X_train_vec[i] = fasttext.wv.get_sentence_vector(tweet)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train_vec, df_train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157398, 300)  Evaluation set size:  (39350, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6818297331639136\n",
      "F1 score:  0.6966760344994669\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.6811458849540655\n",
      "F1 score:  0.6942743836692923\n",
      "Confusion matrix: \n",
      "[[12452  7323]\n",
      " [ 5197 14378]]\n"
     ]
    }
   ],
   "source": [
    "linsvc = LinearSVC()\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:28:50] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (157398, 300)  Evaluation set size:  (39350, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7122236340533672\n",
      "F1 score:  0.7228992316351002\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8413258110014105\n",
      "F1 score:  0.8459293897014823\n",
      "Confusion matrix: \n",
      "[[13255  6520]\n",
      " [ 4804 14771]]\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.apply(lambda x: 0 if x != 1 else x)\n",
    "y_eval = y_eval.apply(lambda x: 0 if x != 1 else x)\n",
    "xgb = XGBClassifier(\n",
    "    max_depth=6,\n",
    "    n_estimators=150,\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (196970, 2)\n",
      "Train set positives:  (97902, 2)\n",
      "Train set negatives:  (99068, 2)\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n"
     ]
    }
   ],
   "source": [
    "# and now lets try to use the strategy mentioned\n",
    "# we will use the first 30 words of the tweet to predict the label\n",
    "# lets encode them with the 20 dim fasttext model to check if it works - 30*20 = 600 dim with lots of sparse vectors\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tknzr.tokenize(x))\n",
    "X_eval_vec = np.zeros((df_train.shape[0], 600))\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText.load('data/fasttext/fasttext_20_4grams.model')\n",
    "iter = 0\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    for j, word in enumerate(tweet[:30]):\n",
    "        X_eval_vec[i][j*20:j*20+20] = fasttext.wv.get_vector(word)\n",
    "    iter += 1\n",
    "    if iter % 10000 == 0:\n",
    "        print(iter)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_eval_vec, df_train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157576, 600)  Evaluation set size:  (39394, 600)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.5950144692085089\n",
      "F1 score:  0.49313762866946254\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.5935104330608723\n",
      "F1 score:  0.49343192692474996\n",
      "Confusion matrix: \n",
      "[[15679  4236]\n",
      " [11718  7761]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "linsvc = LinearSVC()\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:41:49] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (157576, 600)  Evaluation set size:  (39394, 600)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7784941869320201\n",
      "F1 score:  0.7861169665179666\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8534421485505407\n",
      "F1 score:  0.859684298786045\n",
      "Confusion matrix: \n",
      "[[14632  5283]\n",
      " [ 3443 16036]]\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.apply(lambda x: 0 if x != 1 else x)\n",
    "y_eval = y_eval.apply(lambda x: 0 if x != 1 else x)\n",
    "xgb = XGBClassifier(\n",
    "    max_depth=6,\n",
    "    n_estimators=150,\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (196970, 2)\n",
      "Train set positives:  (97902, 2)\n",
      "Train set negatives:  (99068, 2)\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n"
     ]
    }
   ],
   "source": [
    "# ok so it makes sense!!!! next: \n",
    "# actually tokenize ngrams after the tokenization of words, \n",
    "# increaees the dim of the vector to 100 per word\n",
    "\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(\n",
    "    preserve_case=True,\n",
    "    reduce_len=True,\n",
    "\n",
    ")\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tknzr.tokenize(x))\n",
    "# recover the ngrams we saved and on which we trained the fasttext model\n",
    "# we saved a phrase object with the ngrams\n",
    "from gensim.models.phrases import Phrases\n",
    "quadgrams = Phrases.load('data/grams/quadgram.model')\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: quadgrams[x])\n",
    "X_eval_vec = np.zeros((df_train.shape[0], 600))\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText.load('data/fasttext/fasttext_20_4grams.model')\n",
    "iter = 0\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    for j, word in enumerate(tweet[:30]):\n",
    "        X_eval_vec[i][j*20:j*20+20] = fasttext.wv.get_vector(word)\n",
    "    iter += 1\n",
    "    if iter % 10000 == 0:\n",
    "        print(iter)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_eval_vec, df_train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:53:34] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (157576, 600)  Evaluation set size:  (39394, 600)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7742549626846728\n",
      "F1 score:  0.7827531452302431\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8566596436005484\n",
      "F1 score:  0.861798268424756\n",
      "Confusion matrix: \n",
      "[[14480  5342]\n",
      " [ 3551 16021]]\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.apply(lambda x: 0 if x != 1 else x)\n",
    "y_eval = y_eval.apply(lambda x: 0 if x != 1 else x)\n",
    "xgb = XGBClassifier(\n",
    "    max_depth=6,\n",
    "    n_estimators=150,\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (196970, 2)\n",
      "Train set positives:  (97902, 2)\n",
      "Train set negatives:  (99068, 2)\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n"
     ]
    }
   ],
   "source": [
    "# and now augment to 100 dim * 30 words = 3000 dim\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(\n",
    "    preserve_case=True,\n",
    "    reduce_len=True,\n",
    "\n",
    ")\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tknzr.tokenize(x))\n",
    "# recover the ngrams we saved and on which we trained the fasttext model\n",
    "# we saved a phrase object with the ngrams\n",
    "from gensim.models.phrases import Phrases\n",
    "quadgrams = Phrases.load('data/grams/quadgram.model')\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: quadgrams[x])\n",
    "X_eval_vec = np.zeros((df_train.shape[0], 3000))\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText.load('data/fasttext/fasttext_100_4grams.model')\n",
    "iter = 0\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    for j, word in enumerate(tweet[:30]):\n",
    "        X_eval_vec[i][j*100:j*100+100] = fasttext.wv.get_vector(word)\n",
    "    iter += 1\n",
    "    if iter % 10000 == 0:\n",
    "        print(iter)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_eval_vec, df_train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:57:10] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (157576, 3000)  Evaluation set size:  (39394, 3000)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.792049550692999\n",
      "F1 score:  0.79853425802961\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8927057419911661\n",
      "F1 score:  0.8953301924755612\n",
      "Confusion matrix: \n",
      "[[14967  4802]\n",
      " [ 3390 16235]]\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.apply(lambda x: 0 if x != 1 else x)\n",
    "y_eval = y_eval.apply(lambda x: 0 if x != 1 else x)\n",
    "xgb = XGBClassifier(\n",
    "    max_depth=6,\n",
    "    n_estimators=150,\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157576, 3000)  Evaluation set size:  (39394, 3000)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6909173985886176\n",
      "F1 score:  0.6634790779945829\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.6966796974158501\n",
      "F1 score:  0.6684241196547993\n",
      "Confusion matrix: \n",
      "[[15215  4554]\n",
      " [ 7622 12003]]\n"
     ]
    }
   ],
   "source": [
    "linsvc = LinearSVC()\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157576, 3000)  Evaluation set size:  (39394, 3000)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7721226582728334\n",
      "F1 score:  0.7859612312534275\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.7827587957556988\n",
      "F1 score:  0.7951798578369194\n",
      "Confusion matrix: \n",
      "[[13935  5834]\n",
      " [ 3143 16482]]\n"
     ]
    }
   ],
   "source": [
    "linsvc = LinearSVC(\n",
    "    C=0.01,\n",
    ")\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, out strategy makes sense but we need to implement the last pieces: adding the tfidf info to the embeddings, and then training a model on top of this.\n",
    "\n",
    "This should translate in adding a weight to each embedding that we add to the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157576, 3000)  Evaluation set size:  (39394, 3000)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6724120424430117\n",
      "F1 score:  0.6697714885232477\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9999365385591714\n",
      "F1 score:  0.9999361242766074\n",
      "Confusion matrix: \n",
      "[[13402  6367]\n",
      " [ 6538 13087]]\n"
     ]
    }
   ],
   "source": [
    "lil_tree = DecisionTreeClassifier(\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    ")\n",
    "lil_tree = train_test(lil_tree, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now augment to 100 dim * 30 words = 3000 dim\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(\n",
    "    preserve_case=True,\n",
    "    reduce_len=True,\n",
    "\n",
    ")\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tknzr.tokenize(x))\n",
    "# recover the ngrams we saved and on which we trained the fasttext model\n",
    "# we saved a phrase object with the ngrams\n",
    "from gensim.models.phrases import Phrases\n",
    "# tfidf = TfidfVectorizer()\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "# tfidf = TfidfVectorizer(\n",
    "#     ngram_range=(1, 1),\n",
    "#     min_df=5,\n",
    "#     tokenizer=lambda x: quadgrams[tknzr.tokenize(x)],\n",
    "# )\n",
    "# # compute tfidf values, want to use them as weights for the fasttext vectors\n",
    "# tfidf.fit(df_train['tweet'])\n",
    "# # check api again of this to make sure we have access, we are concucting a potion here\n",
    "\n",
    "quadgrams = Phrases.load('data/grams/quadgram.model')\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: quadgrams[x])\n",
    "df_train = df_train[df_train['tweet'].apply(lambda x: len(x.split(' '))) <= 40] # clean outliers already\n",
    "X_eval_vec = np.zeros((df_train.shape[0], 3000))\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText.load('data/fasttext/fasttext_100_4grams.model')\n",
    "iter = 0\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    for j, word in enumerate(tweet[:30]):\n",
    "        X_eval_vec[i][j*100:j*100+100] = fasttext.wv.get_vector(word)\n",
    "    iter += 1\n",
    "    if iter % 10000 == 0:\n",
    "        print(iter)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_eval_vec, df_train['label'], test_size=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c927aa12d5ce5f7072c92979fe584a1fce73005a0de16af9e5cbcd0d6c1397"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
