{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87528445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt'):\n",
    "    # Load data, txt as csv\n",
    "    #data_path = 'data/twitter-datasets/'\n",
    "    df_train_pos = pd.read_csv(path_pos, sep = '\\t', names = ['tweet'])\n",
    "    df_train_pos['label'] = 1\n",
    "    df_train_neg = pd.read_csv(path_neg, sep = '\\t', names = ['tweet'], on_bad_lines='skip')\n",
    "    df_train_neg['label'] = -1\n",
    "    df_train = pd.concat([df_train_pos, df_train_neg], ignore_index=True)\n",
    "    print('Train set: ', df_train.shape)\n",
    "    print('Train set positives: ', df_train_pos.shape)\n",
    "    print('Train set negatives: ', df_train_neg.shape)\n",
    "    return df_train   \n",
    "\n",
    "def load_test_data():\n",
    "    # Load test data: id, tweet for each row\n",
    "    data_path = 'data/twitter-datasets/'\n",
    "    df_test = pd.read_csv(data_path + 'test_data.txt', header=None, names=['line'], sep='\\t')\n",
    "    # Extract id and tweet, limit split by 1 so we don't split the tweet (this is v0, at least we keep it intact)\n",
    "    df_test['id'] = df_test['line'].apply(lambda x: x.split(',',1)[0]) \n",
    "    df_test['tweet'] = df_test['line'].apply(lambda x: x.split(',',1)[1])\n",
    "    df_test = df_test.drop('line', axis=1)\n",
    "    return df_test\n",
    "\n",
    "def predict_test_data(X_test, classifier, filename='submission.csv'):\n",
    "    # Predict test data and save to csv\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    df_test['Prediction'] = y_pred\n",
    "    df_test.rename(columns={'id': 'Id'}, inplace=True)\n",
    "    df_test['Prediction'] = df_test['Prediction'].apply(lambda x: -1 if x != 1 else x)\n",
    "    df_test.to_csv(filename, columns=['Id', 'Prediction'], index=False)\n",
    "    return df_test\n",
    "    \n",
    "def predict_test_data_pipeline(df_test, pipe, filename='submission.csv'):\n",
    "    # Predict test data and save to csv\n",
    "    y_pred = pipe.predict(df_test['tweet'])\n",
    "    df_test['Prediction'] = y_pred\n",
    "    df_test.rename(columns={'id': 'Id'}, inplace=True)\n",
    "    df_test['Prediction'] = df_test['Prediction'].apply(lambda x: -1 if x != 1 else x)\n",
    "    df_test.to_csv(filename, columns=['Id', 'Prediction'], index=False)\n",
    "    return df_test\n",
    "\n",
    "def train_test(clf, X_train, y_train, X_eval=None, y_eval=None, cv=None):\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    if X_eval is None:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_eval, y_train, y_eval = train_test_split(X_train, y_train, test_size=0.2)\n",
    "    if cv is not None:\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1, shuffle=True)\n",
    "        print('Cross validation Accuracy Scores: ', scores)\n",
    "        print('Cross validation mean score: ', scores.mean())\n",
    "        print('Cross validation std score: ', scores.std())\n",
    "        clf.fit(X_train, y_train)\n",
    "        return clf\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('Training set size: ', X_train.shape, ' Evaluation set size: ', X_eval.shape)\n",
    "    print('Metrics on evaluation set: ')\n",
    "    test(clf, X_eval, y_eval)\n",
    "    print('Metrics on training set to check overfitting/triviality of model: ')\n",
    "    test(clf, X_train, y_train)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print('Confusion matrix: ')\n",
    "    print(confusion_matrix(y_eval, clf.predict(X_eval)))\n",
    "    return clf\n",
    "\n",
    "def test(clf, X_eval, y_eval):\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    y_pred = clf.predict(X_eval)\n",
    "    print('Accuracy: ', accuracy_score(y_eval, y_pred))\n",
    "    print('F1 score: ', f1_score(y_eval, y_pred))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996ee233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (196970, 2)\n",
      "Train set positives:  (97902, 2)\n",
      "Train set negatives:  (99068, 2)\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n"
     ]
    }
   ],
   "source": [
    "# and now augment to 300 dim * 30 words = 9000 dim\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(\n",
    "    preserve_case=True,\n",
    "    reduce_len=True,\n",
    "\n",
    ")\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tknzr.tokenize(x))\n",
    "# recover the ngrams we saved and on which we trained the fasttext model\n",
    "# we saved a phrase object with the ngrams\n",
    "from gensim.models.phrases import Phrases\n",
    "# tfidf = TfidfVectorizer()\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "# tfidf = TfidfVectorizer(\n",
    "#     ngram_range=(1, 1),\n",
    "#     min_df=5,\n",
    "#     tokenizer=lambda x: quadgrams[tknzr.tokenize(x)],\n",
    "# )\n",
    "# # compute tfidf values, want to use them as weights for the fasttext vectors\n",
    "# tfidf.fit(df_train['tweet'])\n",
    "# # check api again of this to make sure we have access, we are concucting a potion here\n",
    "\n",
    "quadgrams = Phrases.load('data/grams/quadgram.model')\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: quadgrams[x])\n",
    "df_train = df_train[df_train['tweet'].apply(lambda x: len(x)) <= 40] # clean outliers already\n",
    "X_eval_vec = np.zeros((df_train.shape[0], 9000))\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText.load('data/fasttext/fasttext_300_4grams.model')\n",
    "iter = 0\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    for j, word in enumerate(tweet[:30]):\n",
    "        X_eval_vec[i][j*300:j*300+300] = fasttext.wv.get_vector(word)\n",
    "    iter += 1\n",
    "    if iter % 10000 == 0:\n",
    "        print(iter)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_eval_vec, df_train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e5cc394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157443, 9000)  Evaluation set size:  (39361, 9000)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8016310561215416\n",
      "F1 score:  0.8069524798496761\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8295065515773963\n",
      "F1 score:  0.8336916452402341\n",
      "Confusion matrix: \n",
      "[[15234  4506]\n",
      " [ 3302 16319]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "linsvc = LinearSVC(\n",
    "    C=0.001,\n",
    ")\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c92fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157443, 9000)  Evaluation set size:  (39361, 9000)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6769136962983664\n",
      "F1 score:  0.6735296382820323\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9999428364550981\n",
      "F1 score:  0.9999424511954166\n",
      "Confusion matrix: \n",
      "[[13526  6214]\n",
      " [ 6503 13118]]\n"
     ]
    }
   ],
   "source": [
    "lil_tree = DecisionTreeClassifier()\n",
    "lil_tree = train_test(lil_tree, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1648ddb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:33:29] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (157443, 9000)  Evaluation set size:  (39361, 9000)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7870734991489038\n",
      "F1 score:  0.7948196930007099\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8679395082664837\n",
      "F1 score:  0.8717224189627728\n",
      "Confusion matrix: \n",
      "[[14747  4993]\n",
      " [ 3388 16233]]\n"
     ]
    }
   ],
   "source": [
    "xgb = xgb.XGBClassifier()\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "685cde80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157443, 9000)\n",
      "(39361, 9000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.14510264,  0.21560919, -1.00325012, ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check how 'bad' the embeddings are and see what's the problem, let's also explore a bit what's going on and what the embeddings look like\n",
    "print(X_train.shape)\n",
    "print(X_eval.shape)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42df5170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196804, 30, 300)\n",
      "(30, 300)\n"
     ]
    }
   ],
   "source": [
    "# I think the fundamental idea is good: pick the first 30 words of the tweet (or less, padd with zeros the rest), \n",
    "# and then take the fasttext embedding of each word and concatenate them to get a 9000 dim vector, then\n",
    "# train a classifier on that, hoping it will realize that the features are actually the words of the tweet, each of 300 dim.\n",
    "\n",
    "# But why should the classifier make extra work to understand this substructure? Can't we just tell the model: each feature is a 300dim vector?\n",
    "# What if we wanted to add other features? Like a tfidf score for each word? Or a sentiment score that we provide?\n",
    "# There must be a way, why are we still working with a 2d matrix? We should be working with a 3d tensor, where the third dimension is the 300 dim vector of the word.\n",
    "\n",
    "# Let's try to do this, let's try to make a 3d tensor, where the third dimension is the 300 dim vector of the word.\n",
    "print(np.zeros((df_train.shape[0], 30, 300)).shape)\n",
    "print(np.zeros((df_train.shape[0], 30, 300))[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a282accf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (196970, 2)\n",
      "Train set positives:  (97902, 2)\n",
      "Train set negatives:  (99068, 2)\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "Train 3d tensor:  (157443, 30, 300)\n",
      "Eval 3d tensor:  (39361, 30, 300)\n"
     ]
    }
   ],
   "source": [
    "# 3D Tensors of shape (num_samples, num_words, num_features)\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(\n",
    "    preserve_case=True, # we can play around with this too, but for now we keep everything as with embeddings\n",
    "    reduce_len=True,\n",
    "\n",
    ")\n",
    "\n",
    "# words\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tknzr.tokenize(x))\n",
    "\n",
    "# ngrams\n",
    "from gensim.models.phrases import Phrases\n",
    "quadgrams = Phrases.load('data/grams/quadgram.model')\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: quadgrams[x])\n",
    "\n",
    "# clean outliers with too many words\n",
    "df_train = df_train[df_train['tweet'].apply(lambda x: len(x)) <= 40]\n",
    "\n",
    "# create 3d tensor\n",
    "X_eval_vec = np.zeros((df_train.shape[0], 30, 300))\n",
    "\n",
    "# load fasttext model x embeddings\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText.load('data/fasttext/fasttext_300_4grams.model')\n",
    "\n",
    "# fill tensor with embeddings\n",
    "iter = 0\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    for j, word in enumerate(tweet[:30]):\n",
    "        X_eval_vec[i][j] = fasttext.wv.get_vector(word)\n",
    "    iter += 1\n",
    "    if iter % 10000 == 0:\n",
    "        print(iter)\n",
    "\n",
    "# create train and eval sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_eval_vec, df_train['label'], test_size=0.2)\n",
    "print('Train 3d tensor: ', X_train.shape)\n",
    "print('Eval 3d tensor: ', X_eval.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbec89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "from sklearn.svm import LinearSVC\n",
    "linsvc = LinearSVC(\n",
    "    C=0.001,\n",
    ")\n",
    "#linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval) # so, very simply, this doesn't work with 3d tensors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e754fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we have 30 different trees, each of which is trained on a different word of the tweet? \n",
    "# We concuct a voting on the 30 trees, and the tree that wins the most votes wins the prediction.\n",
    "# is there a way of doing this in sklearn?\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# create 30 trees\n",
    "trees = []\n",
    "for i in range(30):\n",
    "    trees.append(DecisionTreeClassifier(\n",
    "        max_depth=15,\n",
    "        max_features=1,\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=2,\n",
    "        splitter='best',\n",
    "    ))\n",
    "\n",
    "# create voting classifier\n",
    "voting = VotingClassifier(\n",
    "    estimators=[('tree'+str(i), trees[i]) for i in range(30)],\n",
    "    voting='soft',\n",
    "    n_jobs=-1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# train voting classifier\n",
    "#voting = train_test(voting, X_train, y_train, X_eval, y_eval) # we need to train each tree on a different word of the tweet\n",
    "\n",
    "\n",
    "# We are going into Neural Network territory now, this still does not work with 3d tensors, we can do this by hand, start to play around with interaction terms etc etc\n",
    "# let's first do a digression and compare this to averaging tho...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d78034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157443, 300)\n"
     ]
    }
   ],
   "source": [
    "# Afterall, we don't need to immediately increase complexity like this, we can think of better ways to average the embeddings first.\n",
    "# simple averaging:\n",
    "X_train_avg = np.mean(X_train, axis=1)\n",
    "X_eval_avg = np.mean(X_eval, axis=1)\n",
    "print(X_train_avg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f4c2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157443, 300)  Evaluation set size:  (39361, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7932471227865145\n",
      "F1 score:  0.8040075140889168\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.7897334273356071\n",
      "F1 score:  0.800251006136376\n",
      "Confusion matrix: \n",
      "[[14531  5212]\n",
      " [ 2926 16692]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linsvc = LinearSVC(\n",
    "    C=0.001,\n",
    ")\n",
    "linsvc = train_test(linsvc, X_train_avg, y_train, X_eval_avg, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63cf2b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157443, 300)  Evaluation set size:  (39361, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6861868346840782\n",
      "F1 score:  0.682337208106162\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9999237819401308\n",
      "F1 score:  0.999923414683958\n",
      "Confusion matrix: \n",
      "[[13743  6146]\n",
      " [ 6206 13266]]\n"
     ]
    }
   ],
   "source": [
    "lil_tree = DecisionTreeClassifier()\n",
    "lil_tree = train_test(lil_tree, X_train_avg, y_train, X_eval_avg, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef39bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157443, 300)  Evaluation set size:  (39361, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7992429054139885\n",
      "F1 score:  0.8058381247235736\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.7993559573941046\n",
      "F1 score:  0.8064717702411291\n",
      "Confusion matrix: \n",
      "[[15061  4828]\n",
      " [ 3074 16398]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression # long time no see buddy, please don't give me problems\n",
    "logreg = LogisticRegression(\n",
    "    n_jobs=-1, # ooooooooooo didnt know u were chill like that bro\n",
    ")\n",
    "logreg = train_test(logreg, X_train_avg, y_train, X_eval_avg, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3763bc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157443, 300)  Evaluation set size:  (39361, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8011229389497218\n",
      "F1 score:  0.8076752985111296\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8012868149107931\n",
      "F1 score:  0.8081508008535897\n",
      "Confusion matrix: \n",
      "[[15096  4793]\n",
      " [ 3035 16437]]\n"
     ]
    }
   ],
   "source": [
    "# gotta admit logreg is pretty good\n",
    "logreg = LogisticRegression(\n",
    "    n_jobs=-1, \n",
    "    warm_start=True,\n",
    "    max_iter=1000,\n",
    ")\n",
    "logreg = train_test(logreg, X_train_avg, y_train, X_eval_avg, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15aef49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157443, 300)  Evaluation set size:  (39361, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7934503696552425\n",
      "F1 score:  0.8020742039146947\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.790438444389398\n",
      "F1 score:  0.7997304974870713\n",
      "Confusion matrix: \n",
      "[[14758  5131]\n",
      " [ 2999 16473]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(\n",
    "    n_jobs=-1,\n",
    "    warm_start=True,\n",
    "    max_iter=1000,\n",
    "    C=0.01,\n",
    ")\n",
    "logreg = train_test(logreg, X_train_avg, y_train, X_eval_avg, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20976488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157443, 300)  Evaluation set size:  (39361, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7934503696552425\n",
      "F1 score:  0.8020742039146947\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.790438444389398\n",
      "F1 score:  0.7997304974870713\n",
      "Confusion matrix: \n",
      "[[14758  5131]\n",
      " [ 2999 16473]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(\n",
    "    n_jobs=-1,\n",
    "    warm_start=False,\n",
    "    max_iter=1000,\n",
    "    C=0.01,\n",
    ")\n",
    "logreg = train_test(logreg, X_train_avg, y_train, X_eval_avg, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a02d6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157443, 300)  Evaluation set size:  (39361, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8006910393536749\n",
      "F1 score:  0.8076498712762045\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.799921241338135\n",
      "F1 score:  0.8072779331061528\n",
      "Confusion matrix: \n",
      "[[15046  4843]\n",
      " [ 3002 16470]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(\n",
    "    n_jobs=-1,\n",
    "    warm_start=False,\n",
    "    max_iter=1000,\n",
    "    C=0.1,\n",
    "    penalty='elasticnet',\n",
    "    l1_ratio=0.5,\n",
    "    solver='saga',\n",
    ")\n",
    "logreg = train_test(logreg, X_train_avg, y_train, X_eval_avg, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86a197df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157443, 300)  Evaluation set size:  (39361, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8011737506669038\n",
      "F1 score:  0.8077055383556933\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8012741119008149\n",
      "F1 score:  0.8081267707553996\n",
      "Confusion matrix: \n",
      "[[15099  4790]\n",
      " [ 3036 16436]]\n"
     ]
    }
   ],
   "source": [
    "# need more sophisticated logreg, we could augment the data with interaction terms\n",
    "logreg = LogisticRegression(\n",
    "    n_jobs=-1,\n",
    "    warm_start=False,\n",
    "    max_iter=1000,\n",
    "    C=1,\n",
    "    penalty='elasticnet',\n",
    "    l1_ratio=0.5,\n",
    "    solver='saga',\n",
    ")\n",
    "logreg = train_test(logreg, X_train_avg, y_train, X_eval_avg, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "407366bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157443, 45451)\n"
     ]
    }
   ],
   "source": [
    "# augment data with interaction terms\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "X_train_avg_aug = poly.fit_transform(X_train_avg)\n",
    "X_eval_avg_aug = poly.fit_transform(X_eval_avg)\n",
    "print(X_train_avg_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d92d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(\n",
    "    n_jobs=-1,\n",
    "    warm_start=False,\n",
    "    max_iter=1000,\n",
    "    C=0.1,\n",
    "    penalty='elasticnet',\n",
    "    l1_ratio=0.5,\n",
    "    solver='saga',\n",
    ")\n",
    "#logreg = train_test(logreg, X_train_avg_aug, y_train, X_eval_avg_aug, y_eval) #humongous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8032cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to linsvc which might benefit from interaction terms, bt fundamentally we are not really adding any information here...\n",
    "linsvc = LinearSVC(\n",
    "    C=0.001,\n",
    ")\n",
    "#linsvc = train_test(linsvc, X_train_avg_aug, y_train, X_eval_avg_aug, y_eval) # hopefully not too humongous - nope, 'error value too large to convert to npy_int32' after 8+min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d55438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (196970, 2)\n",
      "Train set positives:  (97902, 2)\n",
      "Train set negatives:  (99068, 2)\n",
      "Tokenized tweets:  count    196970.000000\n",
      "mean         15.952135\n",
      "std          14.561441\n",
      "min           1.000000\n",
      "25%          10.000000\n",
      "50%          15.000000\n",
      "75%          22.000000\n",
      "max        1737.000000\n",
      "Name: tweet_len, dtype: float64 Quadgrammed tweets:  count    196970.000000\n",
      "mean         14.971011\n",
      "std          13.791920\n",
      "min           1.000000\n",
      "25%           9.000000\n",
      "50%          14.000000\n",
      "75%          20.000000\n",
      "max        1660.000000\n",
      "Name: tweet_len, dtype: float64\n",
      "Stop Words: 248 Number of long tweets:  166 Number of short tweets (1-2 quadgrams):  559\n",
      "Removing long tweets...\n",
      "Removing stopwords...\n",
      "Number of null tweets:  20\n",
      "Removing null tweets...\n",
      "Final tweets:  count    196784.000000\n",
      "mean         10.781786\n",
      "std           5.427897\n",
      "min           1.000000\n",
      "25%           6.000000\n",
      "50%          10.000000\n",
      "75%          15.000000\n",
      "max          35.000000\n",
      "Name: tweet_len, dtype: float64 Final tweets, pos:  count    97802.000000\n",
      "mean         9.683626\n",
      "std          4.826443\n",
      "min          1.000000\n",
      "25%          6.000000\n",
      "50%          9.000000\n",
      "75%         13.000000\n",
      "max         35.000000\n",
      "Name: tweet_len, dtype: float64 Final tweets, neg:  count    98982.000000\n",
      "mean        11.866855\n",
      "std          5.760849\n",
      "min          1.000000\n",
      "25%          7.000000\n",
      "50%         12.000000\n",
      "75%         17.000000\n",
      "max         35.000000\n",
      "Name: tweet_len, dtype: float64\n",
      "Tweets processed:  25000\n",
      "Tweets processed:  50000\n",
      "Tweets processed:  75000\n",
      "Tweets processed:  100000\n",
      "Tweets processed:  125000\n",
      "Tweets processed:  150000\n",
      "Tweets processed:  175000\n"
     ]
    }
   ],
   "source": [
    "# let's improve the averaging. initial idea: parse out stopwords. We can calculate them as the top ones that are in both classes.\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#stop_words = set(stopwords.words('english')) # tweets are a bit more specific than english, but let's just go with it\n",
    "#print(len(stop_words))\n",
    "\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(\n",
    "    preserve_case=True, # we can play around with this too, but for now we keep everything as with embeddings\n",
    "    reduce_len=True,\n",
    "\n",
    ")\n",
    "\n",
    "# words\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tknzr.tokenize(x))\n",
    "df_train['tweet_len'] = df_train['tweet'].apply(lambda x: len(x))\n",
    "print(\"Tokenized tweets: \", df_train['tweet_len'].describe(), end=' ')\n",
    "\n",
    "# ngrams\n",
    "from gensim.models.phrases import Phrases\n",
    "quadgrams = Phrases.load('data/grams/quadgram.model')\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: quadgrams[x])\n",
    "df_train['tweet_len'] = df_train['tweet'].apply(lambda x: len(x))\n",
    "print(\"Quadgrammed tweets: \", df_train['tweet_len'].describe())\n",
    "\n",
    "# identify stopwords as the top ones that are in both classes, which basically translates in the onoes that give us less information\n",
    "from collections import Counter\n",
    "df_train_pos = df_train[df_train['label'] == 1]\n",
    "df_train_neg = df_train[df_train['label'] != 1]\n",
    "c_pos = Counter([word for tweet in df_train_pos['tweet'] for word in tweet])\n",
    "c_neg = Counter([word for tweet in df_train_neg['tweet'] for word in tweet])\n",
    "c_all = Counter([word for tweet in df_train['tweet'] for word in tweet])\n",
    "# basically a cooccurence matrix of the top 1000 words in both classes, we want the count to be high in both classes\n",
    "# actually we can do this by taking the most common words in one class and then check if the are in the other class's common words as well\n",
    "# and take the ones that are roughly present in both classes in equal amounts so they are both very common and very uninformative for classification\n",
    "# if this calculation is correct we have: say w has total count 100, we filter it out if there are more than 45 occurrances in pos AND in neg. \n",
    "# So the max unbalance is 45/55 = 0.82, which is the max ratio of occurrances in pos to occurrances in neg. after that we filter out words that are too common in one class\n",
    "stop_words = [word for word, count in c_all.most_common(1000) if count*0.45 < c_pos[word] and count*0.45 < c_neg[word]]\n",
    "\n",
    "print(\"Stop Words:\", len(stop_words),\n",
    "\"Number of long tweets: \", df_train[df_train['tweet_len'] > 40].shape[0], \n",
    "\"Number of short tweets (1-2 quadgrams): \", df_train[df_train['tweet_len'] < 3].shape[0] \n",
    ")\n",
    "\n",
    "# clean outliers with too many words\n",
    "print(\"Removing long tweets...\")\n",
    "df_train = df_train[df_train['tweet_len'] <= 40]\n",
    " # Consider creating a separate model for the ones with 1-3 words and train on those separately, we could inspect the results later\n",
    "\n",
    "# remove stopwords\n",
    "print(\"Removing stopwords...\")\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: [word for word in x if word not in stop_words]) # turns out there's some tweets that are just stopwords, so we need to filter those out\n",
    "\n",
    "df_train['tweet_len'] = df_train['tweet'].apply(lambda x: len(x))\n",
    "print(\"Number of null tweets: \", df_train[df_train['tweet_len'] == 0].shape[0])\n",
    "print(\"Removing null tweets...\")\n",
    "df_train = df_train[df_train['tweet_len'] >= 1] \n",
    "df_train['tweet_len'] = df_train['tweet'].apply(lambda x: len(x))\n",
    "\n",
    "# remove words that are too rare --- we kinda did with fasttext and they should not have a vector anyway so let's skip here\n",
    "#rare_stop_words = [word for word, count in c_all.most_common() if count < 3] # very slow with most_common, would be better to just have access to the least_common but ok\n",
    "#df_train['tweet'] = df_train['tweet'].apply(lambda x: [word for word in x if word not in rare_stop_words])\n",
    "#print(\"Rare words: \", len(rare_stop_words))\n",
    "\n",
    "df_train_pos = df_train[df_train['label'] == 1]\n",
    "df_train_neg = df_train[df_train['label'] != 1]\n",
    "print(\"Final tweets: \", df_train['tweet_len'].describe(), \"Final tweets, pos: \", df_train_pos['tweet_len'].describe(), \"Final tweets, neg: \", df_train_neg['tweet_len'].describe())\n",
    "\n",
    "\n",
    "# create 2d array of word vectors\n",
    "X_vec = np.zeros((df_train.shape[0], 300))\n",
    "\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText.load('data/fasttext/fasttext_300_4grams.model')\n",
    "iter = 0\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    #print(tweet)\n",
    "    #print(fasttext.wv.get_mean_vector(tweet))\n",
    "    try: \n",
    "        vc = fasttext.wv.get_mean_vector(tweet)\n",
    "    except ValueError:\n",
    "        print('ValueError at index ', i, ' tweet:')\n",
    "        print(tweet)\n",
    "        continue\n",
    "    X_vec[i] = vc # we could join and call get_phrase_vector, but lets try like this so maybe we use it for a weighting scheme later\n",
    "\n",
    "    iter += 1\n",
    "    if iter % 25000 == 0:\n",
    "        print('Tweets processed: ', iter)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_vec, df_train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "594a3c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 300)  Evaluation set size:  (39357, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.776227862896054\n",
      "F1 score:  0.7805765254005033\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.7794469817756802\n",
      "F1 score:  0.7849094006504569\n",
      "Confusion matrix: \n",
      "[[14885  5020]\n",
      " [ 3787 15665]]\n"
     ]
    }
   ],
   "source": [
    "# let's now train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(\n",
    "    C=0.1,\n",
    "    n_jobs=-1,\n",
    "    solver='saga',\n",
    "    max_iter=1000,\n",
    "    verbose=0,\n",
    "    penalty='elasticnet',\n",
    "    l1_ratio=0.5,\n",
    ")\n",
    "logreg = train_test(logreg, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b00fda4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 300)  Evaluation set size:  (39357, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7995782198846457\n",
      "F1 score:  0.8036638789326961\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8034136456897483\n",
      "F1 score:  0.8083359137920356\n",
      "Confusion matrix: \n",
      "[[15325  4580]\n",
      " [ 3308 16144]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linsvc = LinearSVC()\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7172ea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 300)  Evaluation set size:  (39357, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6986558934878166\n",
      "F1 score:  0.694156480478622\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9992377419375329\n",
      "F1 score:  0.9992341175119032\n",
      "Confusion matrix: \n",
      "[[14038  5867]\n",
      " [ 5993 13459]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "lil_tree = DecisionTreeClassifier()\n",
    "lil_tree = train_test(lil_tree, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3dc2fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:38:16] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (157427, 300)  Evaluation set size:  (39357, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8099702721243998\n",
      "F1 score:  0.8135981855793435\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8827456535410063\n",
      "F1 score:  0.8851230668699629\n",
      "Confusion matrix: \n",
      "[[15556  4349]\n",
      " [ 3130 16322]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "y_train = y_train.apply(lambda x: 0 if x != 1 else x)\n",
    "y_eval = y_eval.apply(lambda x: 0 if x != 1 else x)\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efd0df60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 300)  Evaluation set size:  (39357, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7939121376121148\n",
      "F1 score:  0.7975741845316828\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.7968391699009699\n",
      "F1 score:  0.8014452535712291\n",
      "Confusion matrix: \n",
      "[[15267  4638]\n",
      " [ 3473 15979]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "logreg = train_test(logreg, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "glove = KeyedVectors.load_word2vec_format('data/glove/glove.twitter.27B.200d.txt', binary=False)\n",
    "# let's create a smaller version of the glove embeddings conatining only the words we need\n",
    "# we can take the vocabulary we need directly form the fasttext model --- lets not complicate now, let's just use the whole thing for the moment \n",
    "# from gensim.models import FastText\n",
    "# fasttext = FastText.load('data/fasttext/fasttext_300_4grams.model')\n",
    "# glove_vocab = fasttext.wv.vocab.keys()\n",
    "# glove.build_vocab(glove_vocab, update=True)\n",
    "# glove.vocab = {k: v for k, v in glove.vocab.items() if k in glove_vocab}\n",
    "# glove.save('data/glove/glove.twitter.27B.200d_small.model')\n",
    "# glove = KeyedVectors.load('data/glove/glove.twitter.27B.200d_small.model')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd5c137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets processed:  25000\n",
      "Tweets processed:  50000\n",
      "Tweets processed:  75000\n",
      "Tweets processed:  100000\n",
      "Tweets processed:  125000\n",
      "Tweets processed:  150000\n",
      "Tweets processed:  175000\n"
     ]
    }
   ],
   "source": [
    "# let's now use te glove embeddings since we are just averaging\n",
    "X_vec_glove = np.zeros((df_train.shape[0], 200))\n",
    "iter = 0\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    #print(tweet)\n",
    "    #print(fasttext.wv.get_mean_vector(tweet))\n",
    "    try: \n",
    "        vc = glove.get_mean_vector(tweet)\n",
    "    except ValueError:\n",
    "        print('ValueError at index ', i, ' tweet:')\n",
    "        print(tweet)\n",
    "        continue\n",
    "    X_vec_glove[i] = vc # we could join and call get_phrase_vector, but lets try like this so maybe we use it for a weighting scheme later\n",
    "\n",
    "    iter += 1\n",
    "    if iter % 25000 == 0:\n",
    "        print('Tweets processed: ', iter)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_vec_glove, df_train['label'], test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d022b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 200)  Evaluation set size:  (39357, 200)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7624056711639606\n",
      "F1 score:  0.7700140190363756\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.7599522318280854\n",
      "F1 score:  0.7649466324981963\n",
      "Confusion matrix: \n",
      "[[14352  5294]\n",
      " [ 4057 15654]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=-1, max_iter=1000)\n",
    "logreg = train_test(logreg, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aedb642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 200)  Evaluation set size:  (39357, 200)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7635744594354245\n",
      "F1 score:  0.7726661943270382\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.7615466216087456\n",
      "F1 score:  0.7679784413224469\n",
      "Confusion matrix: \n",
      "[[14239  5407]\n",
      " [ 3898 15813]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linsvc = LinearSVC()\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b4ec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 200)  Evaluation set size:  (39357, 200)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6786594506695124\n",
      "F1 score:  0.6770676403748436\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9929364086211386\n",
      "F1 score:  0.9928781862431152\n",
      "Confusion matrix: \n",
      "[[13452  6194]\n",
      " [ 6453 13258]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "lil_tree = DecisionTreeClassifier()\n",
    "lil_tree = train_test(lil_tree, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8092fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:56:37] WARNING: /var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_eek2t0c4ro/croots/recipe/xgboost-split_1659548960591/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training set size:  (157427, 200)  Evaluation set size:  (39357, 200)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7833168178468888\n",
      "F1 score:  0.7903431999213294\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8556156186677\n",
      "F1 score:  0.8587409110682991\n",
      "Confusion matrix: \n",
      "[[14755  4891]\n",
      " [ 3637 16074]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "y_train = y_train.apply(lambda x: 0 if x != 1 else x)\n",
    "y_eval = y_eval.apply(lambda x: 0 if x != 1 else x)\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "xgb = train_test(xgb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f7dec64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(binary=True, min_df=3, ngram_range=(1, 4)) LinearSVC()\n"
     ]
    }
   ],
   "source": [
    "# Ok so, no matter how we do it we just have less info than the tfidf.\n",
    "# at this point we can try to do interactions from the embeddings or a weighted average\n",
    "# let's try a weighted average by loading the tfidf we saved before in the pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "#pipe = pickle.load(open('data/out/trained/tfidf_vectorizer-linSVC-pipeline-v2_4.pickle', 'rb'))\n",
    "\n",
    "with open('data/out/trained/tfidf_vectorizer-linSVC-pipeline-v2_4.pickle', 'rb') as f:\n",
    "    pipe = pickle.load(f)\n",
    "\n",
    "# we know that this achieves acc: 0.848\tf1: 0.850 and runs in less than 3min on full data from scratch\n",
    "# let's train a linear SVC on the partial data and see how it performs so we can compare and iterate\n",
    "vec_pipe = pipe.steps[0][1]\n",
    "svm_pipe = pipe.steps[1][1]\n",
    "print(vec_pipe, svm_pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "747b45bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 3018685)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "['00', '00 00', '00 00 00', '00 00 am', '00 00 am kst', '00 00 battery', '00 00 battery biz', '00 00 happy', '00 00 in', '00 000']\n",
      "['00 bid end', '00 bid end date', '00 bids', '00 bids end', '00 bids end date', '00 black', '00 black box', '00 black box url', '00 bst', '00 bst white']\n",
      "[2593752  672735 1281430 1497736 1651918 1281430   76098 2264125 1080966\n",
      " 2944833 2856707  787940  334095   18729]\n",
      "[ 0  1  2  3  4  5  7  8  9 10 11 12 13 14]\n",
      "(14,)\n",
      "(14,)\n"
     ]
    }
   ],
   "source": [
    "# let's first check the output of the tfidf vectorizer\n",
    "tweet0_vecs = vec_pipe.transform(df_train['tweet'][0])\n",
    "print(tweet0_vecs.shape)\n",
    "print(tweet0_vecs.toarray())\n",
    "print(vec_pipe.get_feature_names()[0:10])\n",
    "print(vec_pipe.get_feature_names()[100:110])\n",
    "print(vec_pipe.transform(df_train['tweet'][0]).toarray().nonzero()[1])\n",
    "print(vec_pipe.transform(df_train['tweet'][0]).toarray().nonzero()[0]) \n",
    "print(vec_pipe.transform(df_train['tweet'][0]).toarray().nonzero()[1].shape)\n",
    "print(vec_pipe.transform(df_train['tweet'][0]).toarray().nonzero()[0].shape)\n",
    "\n",
    "# so we can indeed use these as weights for the glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87c56721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We should actually re-train a tfidf vectorizer that we are sure has our vocabulary...\n",
    "# since we now tokenized and cleaned the tweets differently we don't need ngrams anymore\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    vocabulary= None,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1,1),\n",
    "    tokenizer=tknzr.tokenize,\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(df_train['tweet'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# import pickle\n",
    "# with open('data/out/trained/tfidf_vectorizer_cleaned-v5.pickle', 'wb') as f:\n",
    "#     pickle.dump(tfidf, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03e5cf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1968319,)\n",
      "2121683\n",
      "-153364\n",
      "0         [<user>, dunno, justin, mention, not, justin, ...\n",
      "1         [your, logic, is, so, dumb, won't, even, crop,...\n",
      "2         [<user>, put, casper, in, box, !, looved, the,...\n",
      "3         [<user>, <user>, thanks, sir, >_>, don't, trip...\n",
      "4         [visiting, tmr, is, the, bestest, birthday_gif...\n",
      "                                ...                        \n",
      "196965    [can't_wait, fake_tan, tonight, !, hate_being,...\n",
      "196966    [<user>, darling, lost, internet_connection, i...\n",
      "196967    [kanguru, defender, basic, 4_gb, usb_2.0, flas...\n",
      "196968                                     [rizan, is, sad]\n",
      "196969                                    [no, ?, yea, mad]\n",
      "Name: tweet, Length: 196784, dtype: object\n",
      "101096\n"
     ]
    }
   ],
   "source": [
    "# let's check inconsistencies between the two vocabularies\n",
    "# we can count the non-zero elements in the tfidf matrix and compare it to the len of tweets\n",
    "print(X_tfidf.nonzero()[1].shape)\n",
    "print(df_train['tweet_len'].sum())\n",
    "\n",
    "# so we have less words in the tfidf than in the tweets, let's check the difference\n",
    "print(X_tfidf.nonzero()[1].shape[0] - df_train['tweet_len'].sum())\n",
    "# in which tweets are these words?\n",
    "print(df_train['tweet'][df_train['tweet_len'] != X_tfidf.nonzero()[1].shape[0]])\n",
    "\n",
    "# <user> not in tfidf?\n",
    "print(df_train['tweet'][df_train['tweet_len'] != X_tfidf.nonzero()[1].shape[0]].apply(lambda x: '<user>' in x).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6fdedee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['/', 'w_'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    vocabulary= None,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1,4),\n",
    "    tokenizer=tknzr.tokenize,\n",
    "    stop_words=stop_words\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(df_train['tweet'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8d4fbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7158536,)\n",
      "2121683\n",
      "5036853\n",
      "Series([], Name: tweet, dtype: object)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# let's check inconsistencies between the two vocabularies\n",
    "# we can count the non-zero elements in the tfidf matrix and compare it to the len of tweets\n",
    "print(X_tfidf.nonzero()[1].shape)\n",
    "print(df_train['tweet_len'].sum())\n",
    "\n",
    "# so we have less words in the tfidf than in the tweets, let's check the difference\n",
    "print(X_tfidf.nonzero()[1].shape[0] - df_train['tweet_len'].sum())\n",
    "# in which tweets are these words?\n",
    "print(df_train['tweet'][df_train['tweet_len'] > X_tfidf.nonzero()[1].shape[0]]) # we looked for the tweets that have more words than the tfidf entries now\n",
    "\n",
    "# <user> not in tfidf?\n",
    "print(df_train['tweet'][df_train['tweet_len'] > X_tfidf.nonzero()[1].shape[0]].apply(lambda x: '<user>' in x).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "346a7c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets processed:  25000\n",
      "Tweets processed:  50000\n",
      "Tweets processed:  75000\n",
      "Tweets processed:  100000\n",
      "Tweets processed:  125000\n",
      "Tweets processed:  150000\n",
      "Tweets processed:  175000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# lets use the tfidf vectorizer values to weight the glove embeddings or the fasttext embeddings\n",
    "# let's try with the glove embeddings - ok nah lets try with the fasttext embeddings that we are sure are working, else too many keyerrors to fix\n",
    "from gensim.models import FastText\n",
    "fasttext = FastText.load('data/fasttext/fasttext_300_4grams.model')\n",
    "\n",
    "X_vec_fasttext_tfidf = np.zeros((df_train.shape[0], 300))\n",
    "iter = 0\n",
    "for i, tweet in enumerate(df_train['tweet']):\n",
    "    #print(tweet)\n",
    "    #print(fasttext.wv.get_mean_vector(tweet))\n",
    "    try: \n",
    "        weights = X_tfidf[i].nonzero()[1]\n",
    "    except ValueError:\n",
    "        print('ValueError at index ', i, ' tweet:')\n",
    "        print(tweet)\n",
    "        continue\n",
    "    \n",
    "    for j, word in enumerate(tweet):\n",
    "        try:\n",
    "            if len(weights) > j:\n",
    "                X_vec_fasttext_tfidf[i] += fasttext.wv.get_vector(word) * weights[j]\n",
    "            else:\n",
    "                X_vec_fasttext_tfidf[i] += fasttext.wv.get_vector(word) * weights.mean()\n",
    "        except KeyError:\n",
    "            print('KeyError at index ', j, ' of tweet: ', i, ' word: ', word)\n",
    "            print(tweet)\n",
    "            continue\n",
    "\n",
    "    X_vec_fasttext_tfidf[i] = X_vec_fasttext_tfidf[i] / len(tweet)\n",
    "    iter += 1\n",
    "    if iter % 25000 == 0:\n",
    "        print('Tweets processed: ', iter)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_vec_fasttext_tfidf, df_train['label'], test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff5c4966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 300)  Evaluation set size:  (39357, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.6673272861244506\n",
      "F1 score:  0.7391468929930469\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.6737980143177472\n",
      "F1 score:  0.7445442109190398\n",
      "Confusion matrix: \n",
      "[[ 7714 12157]\n",
      " [  936 18550]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linsvc = LinearSVC()\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c31cd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 300)  Evaluation set size:  (39357, 300)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7755926518789542\n",
      "F1 score:  0.7803531459835862\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.779967858118366\n",
      "F1 score:  0.7847533353632392\n",
      "Confusion matrix: \n",
      "[[14836  5035]\n",
      " [ 3797 15689]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(\n",
    "    n_jobs=-1,\n",
    "    max_iter=1000,\n",
    "    C=0.1,\n",
    ")\n",
    "logreg = train_test(logreg, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30eac460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 3435629)  Evaluation set size:  (39357, 3435629)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8386055847752624\n",
      "F1 score:  0.842881171465321\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9984564274235043\n",
      "F1 score:  0.9984491770427145\n",
      "Confusion matrix: \n",
      "[[15967  3902]\n",
      " [ 2450 17038]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_tfidf, X_eval_tfidf, y_train_tfidf, y_eval_tfidf = train_test_split(X_tfidf, df_train['label'], test_size=0.2)\n",
    "from sklearn.svm import LinearSVC\n",
    "linsvc_tfidf = LinearSVC()\n",
    "linsvc_tfidf = train_test(linsvc_tfidf, X_train_tfidf, y_train_tfidf, X_eval_tfidf, y_eval_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0196fd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 3435629)  Evaluation set size:  (39357, 3435629)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8056508372081206\n",
      "F1 score:  0.8199854086747782\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9688617581482211\n",
      "F1 score:  0.9694765812774754\n",
      "Confusion matrix: \n",
      "[[14287  5582]\n",
      " [ 2067 17421]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb = train_test(nb, X_train_tfidf, y_train_tfidf, X_eval_tfidf, y_eval_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b430904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 3435629)  Evaluation set size:  (39357, 3435629)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8220900983306655\n",
      "F1 score:  0.8280789628756628\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.918883037852465\n",
      "F1 score:  0.920906264322965\n",
      "Confusion matrix: \n",
      "[[15492  4377]\n",
      " [ 2625 16863]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_tfidf = LogisticRegression(\n",
    "    n_jobs=-1,\n",
    "    max_iter=1000,\n",
    ")\n",
    "logreg_tfidf = train_test(logreg_tfidf, X_train_tfidf, y_train_tfidf, X_eval_tfidf, y_eval_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aafb1780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 3435629)  Evaluation set size:  (39357, 3435629)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8226999009070813\n",
      "F1 score:  0.8300784103638045\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.919588126560247\n",
      "F1 score:  0.9218893653780891\n",
      "Confusion matrix: \n",
      "[[15335  4534]\n",
      " [ 2444 17044]]\n"
     ]
    }
   ],
   "source": [
    "linsvc_tfidf = LinearSVC(\n",
    "    C=0.1,\n",
    ")\n",
    "linsvc_tfidf = train_test(linsvc_tfidf, X_train_tfidf, y_train_tfidf, X_eval_tfidf, y_eval_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "315f4e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 3435629)  Evaluation set size:  (39357, 3435629)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.7725944558782427\n",
      "F1 score:  0.7835236068111456\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.7990941833357683\n",
      "F1 score:  0.80975410230499\n",
      "Confusion matrix: \n",
      "[[14210  5659]\n",
      " [ 3291 16197]]\n"
     ]
    }
   ],
   "source": [
    "linsvc_tfidf = LinearSVC(\n",
    "    C=0.01,\n",
    ")\n",
    "linsvc_tfidf = train_test(linsvc_tfidf, X_train_tfidf, y_train_tfidf, X_eval_tfidf, y_eval_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b7229832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 3435629)  Evaluation set size:  (39357, 3435629)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8384023172497904\n",
      "F1 score:  0.8427999406792228\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9981451720479968\n",
      "F1 score:  0.9981364239763098\n",
      "Confusion matrix: \n",
      "[[15948  3921]\n",
      " [ 2439 17049]]\n"
     ]
    }
   ],
   "source": [
    "linsvc_tfidf = LinearSVC(\n",
    "    C=0.85,\n",
    ")\n",
    "linsvc_tfidf = train_test(linsvc_tfidf, X_train_tfidf, y_train_tfidf, X_eval_tfidf, y_eval_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf6ab889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# honestly with a bit of preprocessing tfidf is king... we could even add the 5-grams and call it a day now that we preprocessed the tweets\n",
    "# we can literally scrap the fasttext embeddings and just use the tfidf vectorizer\n",
    "# note that we are working out of the partial dataset here, so we should probably retrain the tfidf vectorizer on the full dataset\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    vocabulary= None,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1,5),\n",
    "    tokenizer=tknzr.tokenize,\n",
    "    stop_words=None,\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(df_train['tweet'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_tfidf, X_eval_tfidf, y_train_tfidf, y_eval_tfidf = train_test_split(X_tfidf, df_train['label'], test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "87c56872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 4613061)  Evaluation set size:  (39357, 4613061)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8403079503010901\n",
      "F1 score:  0.8465613632479676\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9982976236604902\n",
      "F1 score:  0.9982850195175018\n",
      "Confusion matrix: \n",
      "[[15734  3917]\n",
      " [ 2368 17338]]\n"
     ]
    }
   ],
   "source": [
    "linsvc_tfidf = LinearSVC(\n",
    "    C=0.85,\n",
    ")\n",
    "linsvc_tfidf = train_test(linsvc_tfidf, X_train_tfidf, y_train_tfidf, X_eval_tfidf, y_eval_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "48f3125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    vocabulary= None,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1,6),\n",
    "    tokenizer=tknzr.tokenize,\n",
    "    stop_words=None,\n",
    ")\n",
    "X_tfidf = tfidf.fit_transform(df_train['tweet'].apply(lambda x: ' '.join(x)))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_tfidf, X_eval_tfidf, y_train_tfidf, y_eval_tfidf = train_test_split(X_tfidf, df_train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ba00101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 5644917)  Evaluation set size:  (39357, 5644917)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8370556698935386\n",
      "F1 score:  0.8439165672840557\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9983547930151753\n",
      "F1 score:  0.9983446142439872\n",
      "Confusion matrix: \n",
      "[[15607  4155]\n",
      " [ 2258 17337]]\n"
     ]
    }
   ],
   "source": [
    "# disgustingly good and fast\n",
    "linsvc_tfidf = LinearSVC(\n",
    "    C=0.85,\n",
    ")\n",
    "linsvc_tfidf = train_test(linsvc_tfidf, X_train_tfidf, y_train_tfidf, X_eval_tfidf, y_eval_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "df2692a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['/', 'w_'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    vocabulary= None,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1,6),\n",
    "    tokenizer=tknzr.tokenize,\n",
    "    stop_words=stop_words,\n",
    ")\n",
    "X_tfidf = tfidf.fit_transform(df_train['tweet'].apply(lambda x: ' '.join(x)))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_tfidf, X_eval_tfidf, y_train_tfidf, y_eval_tfidf = train_test_split(X_tfidf, df_train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6ad75c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 5621570)  Evaluation set size:  (39357, 5621570)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8389613029448383\n",
      "F1 score:  0.8447557928770881\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.998329384413093\n",
      "F1 score:  0.998320229419617\n",
      "Confusion matrix: \n",
      "[[15775  4038]\n",
      " [ 2300 17244]]\n"
     ]
    }
   ],
   "source": [
    "linsvc_tfidf = LinearSVC(\n",
    "    C=0.85,\n",
    ")\n",
    "linsvc_tfidf = train_test(linsvc_tfidf, X_train_tfidf, y_train_tfidf, X_eval_tfidf, y_eval_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ccbae75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (157427, 5621570)  Evaluation set size:  (39357, 5621570)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8391391620296262\n",
      "F1 score:  0.8447561364360854\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9985961747349565\n",
      "F1 score:  0.9985885538744514\n",
      "Confusion matrix: \n",
      "[[15801  4012]\n",
      " [ 2319 17225]]\n"
     ]
    }
   ],
   "source": [
    "linsvc_tfidf = LinearSVC(\n",
    "    C=1,\n",
    ")\n",
    "linsvc_tfidf = train_test(linsvc_tfidf, X_train_tfidf, y_train_tfidf, X_eval_tfidf, y_eval_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1d9e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n",
      "Tokenized tweets:  count    2.458295e+06\n",
      "mean     1.598098e+01\n",
      "std      1.491048e+01\n",
      "min      1.000000e+00\n",
      "25%      1.000000e+01\n",
      "50%      1.500000e+01\n",
      "75%      2.200000e+01\n",
      "max      3.304000e+03\n",
      "Name: tweet_len, dtype: float64 Quadgrammed tweets:  count    2.458295e+06\n",
      "mean     1.499350e+01\n",
      "std      1.410102e+01\n",
      "min      1.000000e+00\n",
      "25%      9.000000e+00\n",
      "50%      1.400000e+01\n",
      "75%      2.000000e+01\n",
      "max      3.229000e+03\n",
      "Name: tweet_len, dtype: float64\n",
      "Stop Words: 256 Number of long tweets:  2461 Number of short tweets (1-2 quadgrams):  12722\n",
      "Removing long tweets...\n",
      "Removing stopwords...\n",
      "Number of null tweets:  276\n",
      "Removing null tweets...\n",
      "Final tweets:  count    2.455558e+06\n",
      "mean     1.071552e+01\n",
      "std      5.452992e+00\n",
      "min      1.000000e+00\n",
      "25%      6.000000e+00\n",
      "50%      1.000000e+01\n",
      "75%      1.500000e+01\n",
      "max      3.900000e+01\n",
      "Name: tweet_len, dtype: float64 Final tweets, pos:  count    1.216924e+06\n",
      "mean     9.599223e+00\n",
      "std      4.868598e+00\n",
      "min      1.000000e+00\n",
      "25%      6.000000e+00\n",
      "50%      9.000000e+00\n",
      "75%      1.300000e+01\n",
      "max      3.900000e+01\n",
      "Name: tweet_len, dtype: float64 Final tweets, neg:  count    1.238634e+06\n",
      "mean     1.181226e+01\n",
      "std      5.764909e+00\n",
      "min      1.000000e+00\n",
      "25%      7.000000e+00\n",
      "50%      1.200000e+01\n",
      "75%      1.700000e+01\n",
      "max      3.800000e+01\n",
      "Name: tweet_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# so let's just do this for the full dataset\n",
    "\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(\n",
    "    preserve_case=True, # we can play around with this too, but for now we keep everything as with embeddings\n",
    "    reduce_len=True,\n",
    ")\n",
    "\n",
    "# words\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: tknzr.tokenize(x))\n",
    "df_train['tweet_len'] = df_train['tweet'].apply(lambda x: len(x))\n",
    "print(\"Tokenized tweets: \", df_train['tweet_len'].describe(), end=' ')\n",
    "\n",
    "# ngrams\n",
    "from gensim.models.phrases import Phrases\n",
    "quadgrams = Phrases.load('data/grams/quadgram.model')\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: quadgrams[x])\n",
    "df_train['tweet_len'] = df_train['tweet'].apply(lambda x: len(x))\n",
    "print(\"Quadgrammed tweets: \", df_train['tweet_len'].describe())\n",
    "\n",
    "# identify stopwords as the top ones that are in both classes, which basically translates in the onoes that give us less information\n",
    "from collections import Counter\n",
    "df_train_pos = df_train[df_train['label'] == 1]\n",
    "df_train_neg = df_train[df_train['label'] != 1]\n",
    "c_pos = Counter([word for tweet in df_train_pos['tweet'] for word in tweet])\n",
    "c_neg = Counter([word for tweet in df_train_neg['tweet'] for word in tweet])\n",
    "c_all = Counter([word for tweet in df_train['tweet'] for word in tweet])\n",
    "# basically a cooccurence matrix of the top 1000 words in both classes, we want the count to be high in both classes\n",
    "# actually we can do this by taking the most common words in one class and then check if the are in the other class's common words as well\n",
    "# and take the ones that are roughly present in both classes in equal amounts so they are both very common and very uninformative for classification\n",
    "# if this calculation is correct we have: say w has total count 100, we filter it out if there are more than 45 occurrances in pos AND in neg. \n",
    "# So the max unbalance is 45/55 = 0.82, which is the max ratio of occurrances in pos to occurrances in neg. after that we filter out words that are too common in one class\n",
    "stop_words = [word for word, count in c_all.most_common(1000) if count*0.45 < c_pos[word] and count*0.45 < c_neg[word]]\n",
    "\n",
    "print(\"Stop Words:\", len(stop_words),\n",
    "\"Number of long tweets: \", df_train[df_train['tweet_len'] > 40].shape[0], \n",
    "\"Number of short tweets (1-2 quadgrams): \", df_train[df_train['tweet_len'] < 3].shape[0] \n",
    ")\n",
    "\n",
    "# clean outliers with too many words\n",
    "print(\"Removing long tweets...\")\n",
    "df_train = df_train[df_train['tweet_len'] <= 40]\n",
    " # Consider creating a separate model for the ones with 1-3 words and train on those separately, we could inspect the results later\n",
    "\n",
    "# remove stopwords\n",
    "print(\"Removing stopwords...\")\n",
    "df_train['tweet'] = df_train['tweet'].apply(lambda x: [word for word in x if word not in stop_words]) # turns out there's some tweets that are just stopwords, so we need to filter those out\n",
    "\n",
    "df_train['tweet_len'] = df_train['tweet'].apply(lambda x: len(x))\n",
    "print(\"Number of null tweets: \", df_train[df_train['tweet_len'] == 0].shape[0])\n",
    "print(\"Removing null tweets...\")\n",
    "df_train = df_train[df_train['tweet_len'] >= 1] \n",
    "df_train['tweet_len'] = df_train['tweet'].apply(lambda x: len(x))\n",
    "\n",
    "# remove words that are too rare --- we kinda did with fasttext and they should not have a vector anyway so let's skip here\n",
    "#rare_stop_words = [word for word, count in c_all.most_common() if count < 3] # very slow with most_common, would be better to just have access to the least_common but ok\n",
    "#df_train['tweet'] = df_train['tweet'].apply(lambda x: [word for word in x if word not in rare_stop_words])\n",
    "#print(\"Rare words: \", len(rare_stop_words))\n",
    "\n",
    "df_train_pos = df_train[df_train['label'] == 1]\n",
    "df_train_neg = df_train[df_train['label'] != 1]\n",
    "print(\"Final tweets: \", df_train['tweet_len'].describe(), \"Final tweets, pos: \", df_train_pos['tweet_len'].describe(), \"Final tweets, neg: \", df_train_neg['tweet_len'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d7608e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# add quadgrams on top of tokenizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "qutknzr = TweetTokenizer(\n",
    "    preserve_case=True, \n",
    "    reduce_len=True,\n",
    ")\n",
    "\n",
    "def quadgram_tokenizer(tweet):\n",
    "    return quadgrams[qutknzr.tokenize(tweet)]\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    tokenizer=quadgram_tokenizer,\n",
    "    min_df=3,\n",
    "    stop_words=None,\n",
    ")\n",
    "X_train_tfidf = tfidf.fit_transform(df_train['tweet'].apply(lambda x: ' '.join(x)))\n",
    "y_train_tfidf = df_train['label']\n",
    "\n",
    "# save the tfidf model\n",
    "import pickle\n",
    "with open('data/out/trained/tfidf-v5.model', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d831aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (1964446, 1080613)  Evaluation set size:  (491112, 1080613)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8524267376891626\n",
      "F1 score:  0.8534831487931943\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9332279940502309\n",
      "F1 score:  0.9335508285249673\n",
      "Confusion matrix: \n",
      "[[207548  40799]\n",
      " [ 31676 211089]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train_tfidf, y_train_tfidf, test_size=0.2)\n",
    "linsvc = LinearSVC()\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b77dce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (1964446, 1080613)  Evaluation set size:  (491112, 1080613)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8184385639121016\n",
      "F1 score:  0.8320610830795425\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8477545323210717\n",
      "F1 score:  0.8587643831502157\n",
      "Confusion matrix: \n",
      "[[181054  67293]\n",
      " [ 21874 220891]]\n"
     ]
    }
   ],
   "source": [
    "# disgustingly good\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb = train_test(nb, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "612ab932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add grams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# add quadgrams on top of tokenizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "qutknzr = TweetTokenizer(\n",
    "    preserve_case=True, \n",
    "    reduce_len=True,\n",
    ")\n",
    "\n",
    "def quadgram_tokenizer(tweet):\n",
    "    return quadgrams[qutknzr.tokenize(tweet)]\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 5),\n",
    "    tokenizer=qutknzr.tokenize,\n",
    "    min_df=3,\n",
    "    stop_words=None,\n",
    ")\n",
    "X_train_tfidf = tfidf.fit_transform(df_train['tweet'].apply(lambda x: ' '.join(x)))\n",
    "y_train_tfidf = df_train['label']\n",
    "\n",
    "# save the tfidf model\n",
    "import pickle\n",
    "with open('data/out/trained/tfidf-v5-5gram.model', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8e9324fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (1964446, 2798649)  Evaluation set size:  (491112, 2798649)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8554891755852025\n",
      "F1 score:  0.8565926638391079\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9582157005079295\n",
      "F1 score:  0.9581954626826145\n",
      "Confusion matrix: \n",
      "[[208181  39709]\n",
      " [ 31262 211960]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train_tfidf, y_train_tfidf, test_size=0.2)\n",
    "linsvc = LinearSVC()\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a8f5afaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (1964446, 2798649)  Evaluation set size:  (491112, 2798649)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8605389402010132\n",
      "F1 score:  0.8634769903584151\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8934748015471028\n",
      "F1 score:  0.8954068333955605\n",
      "Confusion matrix: \n",
      "[[206026  41864]\n",
      " [ 26627 216595]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "linsvc = LinearSVC(\n",
    "    C=0.1,\n",
    ")\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71913d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vectorizer\n",
    "import pickle\n",
    "with open('data/out/trained/tfidf-v5-5gram.model', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "X_train_tfidf = tfidf.transform(df_train['tweet'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c228a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "{'C': 0.5}\n",
      "0.8540905974111057\n",
      "LinearSVC(C=0.5)\n",
      "{'mean_fit_time': array([18.19162003, 52.87049405, 18.09167592, 16.33018398, 13.88037475,\n",
      "       28.05644806, 43.33823458, 38.91799521, 12.90611966, 29.49451756]), 'std_fit_time': array([0.87336903, 8.08964656, 0.99546791, 1.76063246, 0.77400606,\n",
      "       1.33520405, 4.219617  , 4.60410238, 0.50013605, 0.61895113]), 'mean_score_time': array([0.21216003, 0.23504821, 0.42306733, 0.20025937, 0.20223029,\n",
      "       0.20049866, 0.15699005, 0.16616281, 0.18446406, 0.12592276]), 'std_score_time': array([0.02278829, 0.04378246, 0.33036282, 0.01055319, 0.02842651,\n",
      "       0.01498232, 0.0024077 , 0.02891647, 0.02576474, 0.01276247]), 'param_C': masked_array(data=[0.01, 1, 0.005, 0.01, 0.03, 0.5, 0.95, 0.85, 0.05, 0.8],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01}, {'C': 1}, {'C': 0.005}, {'C': 0.01}, {'C': 0.03}, {'C': 0.5}, {'C': 0.95}, {'C': 0.85}, {'C': 0.05}, {'C': 0.8}], 'split0_test_score': array([0.83333944, 0.84869154, 0.82358281, 0.83333944, 0.84673313,\n",
      "       0.85489298, 0.84919855, 0.8503995 , 0.85146728, 0.85096027]), 'split1_test_score': array([0.8313625 , 0.84670728, 0.82077997, 0.8313625 , 0.84413068,\n",
      "       0.85272914, 0.84727172, 0.84838104, 0.8489076 , 0.84904321]), 'split2_test_score': array([0.83326594, 0.84880009, 0.82276282, 0.83326594, 0.84652525,\n",
      "       0.85464968, 0.8493462 , 0.85043719, 0.8511971 , 0.85098452]), 'mean_test_score': array([0.83265596, 0.8480663 , 0.8223752 , 0.83265596, 0.84579635,\n",
      "       0.8540906 , 0.84860549, 0.84973924, 0.85052399, 0.85032933]), 'std_test_score': array([0.00091511, 0.00096199, 0.00117662, 0.00091511, 0.00118086,\n",
      "       0.00096781, 0.00094504, 0.00096052, 0.00114827, 0.00090948]), 'rank_test_score': array([ 8,  6, 10,  8,  7,  1,  5,  4,  2,  3], dtype=int32)}\n",
      "RandomizedSearchCV(cv=3, estimator=LinearSVC(), n_jobs=-1,\n",
      "                   param_distributions={'C': [0.001, 0.005, 0.01, 0.05, 0.01,\n",
      "                                              0.03, 0.5, 0.8, 0.85, 0.95, 1]},\n",
      "                   scoring='accuracy', verbose=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# random search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.005, 0.01, 0.05, 0.01, 0.03, 0.5, 0.8, 0.85, 0.95, 1]\n",
    "}\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "linsvc = LinearSVC()\n",
    "grid_search = RandomizedSearchCV(linsvc, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_tfidf, df_train['label'])\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.cv_results_)\n",
    "print(grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5d3b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (1964446, 2798649)  Evaluation set size:  (491112, 2798649)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8558475459772924\n",
      "F1 score:  0.8571549635093936\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9568244685779095\n",
      "F1 score:  0.9568137843947941\n",
      "Confusion matrix: \n",
      "[[207911  39763]\n",
      " [ 31032 212406]]\n",
      "Training set size:  (1964446, 2798649)  Evaluation set size:  (491112, 2798649)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8603088501197282\n",
      "F1 score:  0.8621379825894042\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.9388753877683581\n",
      "F1 score:  0.9391490310548933\n",
      "Confusion matrix: \n",
      "[[207996  39678]\n",
      " [ 28926 214512]]\n",
      "Training set size:  (1964446, 2798649)  Evaluation set size:  (491112, 2798649)\n",
      "Metrics on evaluation set: \n",
      "Accuracy:  0.8603149587059571\n",
      "F1 score:  0.8634123711545469\n",
      "Metrics on training set to check overfitting/triviality of model: \n",
      "Accuracy:  0.8937512153553725\n",
      "F1 score:  0.8956330074114965\n",
      "Confusion matrix: \n",
      "[[205687  41987]\n",
      " [ 26614 216824]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "linsvc = LinearSVC(\n",
    "    C=0.95,\n",
    ")\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train_tfidf, df_train['label'], test_size=0.2)\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)\n",
    "linsvc = LinearSVC(\n",
    "    C=0.5,\n",
    ")\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)\n",
    "\n",
    "linsvc = LinearSVC(\n",
    "    C=0.1,\n",
    ")\n",
    "linsvc = train_test(linsvc, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d18c4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1   -1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: pred, dtype: int64\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: label, dtype: int64\n",
      "277321\n",
      "count    277321.000000\n",
      "mean          9.783049\n",
      "std           4.952792\n",
      "min           1.000000\n",
      "25%           6.000000\n",
      "50%           9.000000\n",
      "75%          13.000000\n",
      "max          38.000000\n",
      "Name: tweet_len, dtype: float64\n",
      "count    2.178237e+06\n",
      "mean     1.083424e+01\n",
      "std      5.502091e+00\n",
      "min      1.000000e+00\n",
      "25%      6.000000e+00\n",
      "50%      1.000000e+01\n",
      "75%      1.500000e+01\n",
      "max      3.900000e+01\n",
      "Name: tweet_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# let's check average length of tweets we missclassified\n",
    "df_train['pred'] = linsvc.predict(X_train_tfidf)\n",
    "print(df_train['pred'].head())\n",
    "print(df_train['label'].head())\n",
    "print(len(df_train[(df_train['pred'] != df_train['label'])]))\n",
    "print(df_train[(df_train['pred'] != df_train['label'])]['tweet_len'].describe())\n",
    "print(df_train[(df_train['pred'] == df_train['label'])]['tweet_len'].describe()) # they are indeed slightly longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c4dfffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.783049246180418\n",
      "10.834239341265436\n",
      "9.0\n",
      "10.0\n",
      "10.715522093145427\n",
      "11.658679041455194\n",
      "0\n",
      "104527\n",
      "0\n",
      "1112397\n"
     ]
    }
   ],
   "source": [
    "print(df_train[(df_train['pred'] != df_train['label'])]['tweet_len'].mean())\n",
    "print(df_train[(df_train['pred'] == df_train['label'])]['tweet_len'].mean())\n",
    "print(df_train[(df_train['pred'] != df_train['label'])]['tweet_len'].median())\n",
    "print(df_train[(df_train['pred'] == df_train['label'])]['tweet_len'].median())\n",
    "print(df_train[(df_train['pred'] != df_train['label'] & (df_train['label'] == 0))]['tweet_len'].mean())\n",
    "print(df_train[(df_train['pred'] != df_train['label'] & (df_train['label'] == 1))]['tweet_len'].mean()) # in particular we missclasify positive longer tweets\n",
    "\n",
    "print(len(df_train[(df_train['pred'] != df_train['label']) & (df_train['label'] == 0)])) # what? it's 0 tweets?\n",
    "print(len(df_train[(df_train['pred'] != df_train['label']) & (df_train['label'] == 1)]))\n",
    "print(len(df_train[(df_train['pred'] == df_train['label']) & (df_train['label'] == 0)]))\n",
    "print(len(df_train[(df_train['pred'] == df_train['label']) & (df_train['label'] == 1)])) # we only classify positive??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd584cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet  label  tweet_len  pred\n",
      "1   [your, logic, is, dumb, won't, crop, your, nam...      1         11    -1\n",
      "10          [<user>, anddd, cheer, #nationals2013, ?]      1          5    -1\n",
      "11  [we, send, an_invitation, shop, on-line, !, yo...      1         17    -1\n",
      "23             [<user>, no, doubts, that, ability, !]      1          6    -1\n",
      "29         [<user>, seriously, it's, vanity, fairest]      1          5    -1\n",
      "                                               tweet  label  tweet_len  pred\n",
      "0  [<user>, dunno, justin, read, mention, not, ju...      1         16     1\n",
      "1  [your, logic, is, dumb, won't, crop, your, nam...      1         11    -1\n",
      "2  [<user>, just, put, casper, in, box, !, looved...      1         12     1\n",
      "3  [<user>, <user>, thanks, sir, >_>, don't, trip...      1         15     1\n",
      "4  [visiting, tmr, is, the, bestest, birthday_gif...      1         10     1\n"
     ]
    }
   ],
   "source": [
    "print(df_train[df_train['pred'] != 1].head())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26f28bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    your logic is dumb won't crop your name your t...\n",
      "Name: tweet, dtype: object\n",
      "your logic is dumb won't crop your name your tsk <url>\n"
     ]
    }
   ],
   "source": [
    "# honestly I'd missclassify them too wtf? Are we sure of the labels?\n",
    "print(df_train[1:2]['tweet'].apply(lambda x: ' '.join(x)))\n",
    "print(df_train[1:2]['tweet'].apply(lambda x: ' '.join(x)).values[0]) # honestly this is a very negative tweet, why is it in positive dataset?\n",
    "\n",
    "# How to detect sarcasm?? ahahahah if this is sentiment analysis it actually performs well, but this is just missclassification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e6ed502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       0\n",
      "tweet    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[sea, doo, pro, sea, scooter, (, sports, the, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[&lt;user&gt;, shucks, well, work, can't, cheer, you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[cant, away, from, bug, thats]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[&lt;user&gt;, no, ma'am, !, !, !, lol, perfectly, f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[whenever, fall, asleep, watching, the, tv, al...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>[had, nice, time, w, /, friend, lastnite]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>[&lt;user&gt;, no, it's, not, !, please, stop, !]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>[not, without, daughter, (, dvd, two-time, osc...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>[&lt;user&gt;, fun, in, sweetcheeks]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>[r, e, l, difference, (, r, e, l, recreational...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              tweet  Prediction\n",
       "0         1  [sea, doo, pro, sea, scooter, (, sports, the, ...          -1\n",
       "1         2  [<user>, shucks, well, work, can't, cheer, you...           1\n",
       "2         3                     [cant, away, from, bug, thats]          -1\n",
       "3         4  [<user>, no, ma'am, !, !, !, lol, perfectly, f...           1\n",
       "4         5  [whenever, fall, asleep, watching, the, tv, al...          -1\n",
       "...     ...                                                ...         ...\n",
       "9995   9996          [had, nice, time, w, /, friend, lastnite]           1\n",
       "9996   9997        [<user>, no, it's, not, !, please, stop, !]          -1\n",
       "9997   9998  [not, without, daughter, (, dvd, two-time, osc...          -1\n",
       "9998   9999                     [<user>, fun, in, sweetcheeks]           1\n",
       "9999  10000  [r, e, l, difference, (, r, e, l, recreational...          -1\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final model\n",
    "linsvc = LinearSVC(\n",
    "    C=0.1,\n",
    ")\n",
    "linsvc.fit(X_train_tfidf, df_train['label'])\n",
    "# load test data\n",
    "df_test = load_test_data()\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(\n",
    "    preserve_case=True,\n",
    "    reduce_len=True,\n",
    ")\n",
    "\n",
    "# apply same preprocessing to test data\n",
    "df_test['tweet'] = df_test['tweet'].apply(lambda x: tknzr.tokenize(x))\n",
    "df_test['tweet'] = df_test['tweet'].apply(lambda x: [w for w in x if w not in stop_words])\n",
    "\n",
    "# let's check if we have nulls...\n",
    "print(df_test.isnull().sum())\n",
    "\n",
    "# apply same vectorizer to test data\n",
    "X_test_tfidf = tfidf.transform(df_test['tweet'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# make predictions and save them\n",
    "predict_test_data(X_test_tfidf,linsvc, 'data/out/submission-v5.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e289560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc: 0.845\t f1 score: 0.848\n",
    "\n",
    "# so slightly worse than v2..... all that embedding work uff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next: two different models mased on length of tweets. Embeddings for less than 3-4 words, tfidf for longer tweets maybe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "66c927aa12d5ce5f7072c92979fe584a1fce73005a0de16af9e5cbcd0d6c1397"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
