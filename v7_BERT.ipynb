{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v7 - BERT used as feature extractor and tokenizer \n",
    "\n",
    "But not directly as a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertweetTokenizer\n",
    "import torch \n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt'):\n",
    "    # Load data, txt as csv\n",
    "    #data_path = 'data/twitter-datasets/'\n",
    "    df_train_pos = pd.read_csv(path_pos, sep = '\\t', names = ['tweet'])\n",
    "    df_train_pos['label'] = 1\n",
    "    df_train_neg = pd.read_csv(path_neg, sep = '\\t', names = ['tweet'], on_bad_lines='skip')\n",
    "    df_train_neg['label'] = -1\n",
    "    df_train = pd.concat([df_train_pos, df_train_neg], ignore_index=True)\n",
    "    print('Train set: ', df_train.shape)\n",
    "    print('Train set positives: ', df_train_pos.shape)\n",
    "    print('Train set negatives: ', df_train_neg.shape)\n",
    "    return df_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (196970, 2)\n",
      "Train set positives:  (97902, 2)\n",
      "Train set negatives:  (99068, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m# Convert the training and test sets to tensors\u001b[39;00m\n\u001b[1;32m     24\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m train_set \u001b[39m=\u001b[39m TensorDataset(X_train, y_train)\n\u001b[1;32m     26\u001b[0m test_set \u001b[39m=\u001b[39m TensorDataset(X_eval, y_eval)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Create dataloaders for the training and test sets\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/dataset.py:184\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTensorDataset\u001b[39;00m(Dataset[Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]):\n\u001b[1;32m    179\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Dataset wrapping tensors.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[39m    Each sample will be retrieved by indexing tensors along the first dimension.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m \u001b[39m        *tensors (Tensor): tensors that have the same size of the first dimension.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     tensors: Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n\u001b[1;32m    188\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/dataset.py:184\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTensorDataset\u001b[39;00m(Dataset[Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]):\n\u001b[1;32m    179\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Dataset wrapping tensors.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[39m    Each sample will be retrieved by indexing tensors along the first dimension.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m \u001b[39m        *tensors (Tensor): tensors that have the same size of the first dimension.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     tensors: Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n\u001b[1;32m    188\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "# Read in the dataset\n",
    "df = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "\n",
    "# Preprocessing: Tokenize the tweets using BERT\n",
    "def preprocess(text, tokenizer):\n",
    "    # Tokenize the text using BERT\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "    # Convert the input ids to a tensor\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "    # Return the input ids\n",
    "    return input_ids\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertweetTokenizer.from_pretrained('vinai/bertweet-base')\n",
    "\n",
    "# Tokenize the tweets\n",
    "df['tweet'] = df['tweet'].apply(lambda x: preprocess(x, tokenizer))\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(df['tweet'], df['label'], test_size=0.2)\n",
    "\n",
    "# Convert the training and test sets to tensors\n",
    "device = torch.device('mps')\n",
    "train_set = TensorDataset(X_train, y_train)\n",
    "test_set = TensorDataset(X_eval, y_eval)\n",
    "# Create dataloaders for the training and test sets\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train a BERT model\n",
    "#model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = BertForSequenceClassification.from_pretrained('vinai/bertweet-base', num_labels=2)\n",
    "model.train()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Train the model on the training set\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = []\n",
    "for inputs, labels in test_dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    y_pred.extend(predicted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c927aa12d5ce5f7072c92979fe584a1fce73005a0de16af9e5cbcd0d6c1397"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
