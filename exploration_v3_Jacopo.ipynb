{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Notebook v3 - Jacopo\n",
    "Hi, it's me again\n",
    "\n",
    "### Goal\n",
    "Pass the 0.9 threshold for both accuracy and F1 score\n",
    "\n",
    "### Plan\n",
    "- Start to use external datasets, embeddings etc\n",
    "- Initially Keep same vectorization found in v2, start by incrementing the complexity of the model\n",
    "- We can tweak both together by using pipelines, grid and randomized search with (stratified) k-fold cross-validation as introduced in last notebook\n",
    "- Keep track of the best model and its parameters, start saving them as pickle files as introduced in v2\n",
    "- Start thinking about generalizing the prpoblem to sentiment analysis instead of just detection of smiley face which we don't even have access to.\n",
    "\n",
    "### Next\n",
    "With v3 we plan on passing the 0.9 threshold, with v3 we can aggregate the results of the best model and the best parameters and tweak them to get the best possible result in a cleaner way to prepare a final submission. Modularize, improve helpers etc. Keeping this self contained turned out to be better and the Helper file just created more friction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(df, vocab, embeddings, mode='avg'):\n",
    "    X = np.zeros((df.shape[0], embeddings.shape[1]))\n",
    "    for i, tweet in enumerate(df['tweet']):\n",
    "        words = tweet.split()\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                X[i] += embeddings[vocab[word]]\n",
    "        if mode == 'avg':\n",
    "            X[i] /= len(words)\n",
    "        elif mode == 'sum':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(mode))\n",
    "    return X\n",
    "    \n",
    "def load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt'):\n",
    "    # Load data, txt as csv\n",
    "    #data_path = 'data/twitter-datasets/'\n",
    "    df_train_pos = pd.read_csv(path_pos, sep = '\\t', names = ['tweet'])\n",
    "    df_train_pos['label'] = 1\n",
    "    df_train_neg = pd.read_csv(path_neg, sep = '\\t', names = ['tweet'], on_bad_lines='skip')\n",
    "    df_train_neg['label'] = 0\n",
    "    df_train = pd.concat([df_train_pos, df_train_neg])\n",
    "    print('Train set: ', df_train.shape)\n",
    "    print('Train set positives: ', df_train_pos.shape)\n",
    "    print('Train set negatives: ', df_train_neg.shape)\n",
    "    return df_train   \n",
    "\n",
    "def load_test_data():\n",
    "    # Load test data: id, tweet for each row\n",
    "    data_path = 'data/twitter-datasets/'\n",
    "    df_test = pd.read_csv(data_path + 'test_data.txt', header=None, names=['line'], sep='\\t')\n",
    "    # Extract id and tweet, limit split by 1 so we don't split the tweet (this is v0, at least we keep it intact)\n",
    "    df_test['id'] = df_test['line'].apply(lambda x: x.split(',',1)[0]) \n",
    "    df_test['tweet'] = df_test['line'].apply(lambda x: x.split(',',1)[1])\n",
    "    df_test = df_test.drop('line', axis=1)\n",
    "    return df_test\n",
    "\n",
    "def predict_test_data(X_test, classifier, filename='submission.csv'):\n",
    "    # Predict test data and save to csv\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    df_test['Prediction'] = y_pred\n",
    "    df_test.rename(columns={'id': 'Id'}, inplace=True)\n",
    "    df_test['Prediction'] = df_test['Prediction'].apply(lambda x: -1 if x == 0 else x)\n",
    "    df_test.to_csv(filename, columns=['Id', 'Prediction'], index=False)\n",
    "    return df_test\n",
    "    \n",
    "def predict_test_data_pipeline(df_test, pipe, filename='submission.csv'):\n",
    "    # Predict test data and save to csv\n",
    "    y_pred = pipe.predict(df_test['tweet'])\n",
    "    df_test['Prediction'] = y_pred\n",
    "    df_test.rename(columns={'id': 'Id'}, inplace=True)\n",
    "    df_test['Prediction'] = df_test['Prediction'].apply(lambda x: -1 if x == 0 else x)\n",
    "    df_test.to_csv(filename, columns=['Id', 'Prediction'], index=False)\n",
    "    return df_test\n",
    "\n",
    "def train_test(clf, X_train, y_train, X_eval=None, y_eval=None, cv=None):\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    if X_eval is None:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_eval, y_train, y_eval = train_test_split(X_train, y_train, test_size=0.2)\n",
    "    if cv is not None:\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1, shuffle=True)\n",
    "        print('Cross validation Accuracy Scores: ', scores)\n",
    "        print('Cross validation mean score: ', scores.mean())\n",
    "        print('Cross validation std score: ', scores.std())\n",
    "        clf.fit(X_train, y_train)\n",
    "        return clf\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_eval)\n",
    "    print('Accuracy: ', accuracy_score(y_eval, y_pred))\n",
    "    print('F1 score: ', f1_score(y_eval, y_pred))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, f1_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (196970, 2)\n",
      "Train set positives:  (97902, 2)\n",
      "Train set negatives:  (99068, 2)\n",
      "TfidfVectorizer(binary=True, min_df=3, ngram_range=(1, 4)) LinearSVC()\n"
     ]
    }
   ],
   "source": [
    "# Load data, lets work on partial data for now, we'll come back to this later\n",
    "df_train = load_train_data(path_pos='data/twitter-datasets/train_pos.txt', path_neg='data/twitter-datasets/train_neg.txt')\n",
    "# Load vectorization and classifier obtained from the previous notebook as a reference\n",
    "with open('data/out/trained/tfidf_vectorizer-linSVC-pipeline-v2_4.pickle', 'rb') as f:\n",
    "    pipe = pickle.load(f)\n",
    "\n",
    "# we know that this achieves acc: 0.848\tf1: 0.850 and runs in less than 3min on full data from scratch\n",
    "# let's train a linear SVC on the partial data and see how it performs so we can compare and iterate\n",
    "svm = LinearSVC()\n",
    "vec_pipe = pipe.steps[0][1]\n",
    "svm_pipe = pipe.steps[1][1]\n",
    "print(vec_pipe, svm_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simple check to compare the fresh one fitted on partial data with the one fitted on full data\n",
    "# # note that the full one will overfit because we are using data it was trained on to test it\n",
    "# # but we can still compare the results as a rule of thumb (also because we have the real test data results)\n",
    "# X_train, X_eval, y_train, y_eval = train_test_split(df_train['tweet'], df_train['label'], test_size=0.2, random_state=42)\n",
    "# X_train = vec.fit_transform(X_train)\n",
    "# X_eval = vec.transform(X_eval)\n",
    "# svm.fit(X_train, y_train)\n",
    "# y_pred = svm.predict(X_eval)\n",
    "# print('Accuracy fresh svm: ', accuracy_score(y_eval, y_pred))\n",
    "# print('F1 score fresh svm: ', f1_score(y_eval, y_pred))\n",
    "# y_pred = svm_pipe.predict(X_eval)\n",
    "# print('Accuracy: ', accuracy_score(y_eval, y_pred))\n",
    "# print('F1 score: ', f1_score(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think a misconception I had was that for a second I thought the vectorizer was 'saved' as well\n",
    "# but it's not, it's just a step in the pipeline, so we need to re-fit it on the partial data\n",
    "# the fitting of the vectorizer is not the expensive part, the actual transformation is\n",
    "# let's thus re-fit the vectorizer on the partial data and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9642331319490278\n"
     ]
    }
   ],
   "source": [
    "# check svm_pipe performance on partial data, don't even modify the vectorizer\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(df_train['tweet'], df_train['label'], test_size=0.2)\n",
    "X_train = vec_pipe.transform(X_train)\n",
    "X_eval = vec_pipe.transform(X_eval)\n",
    "y_pred = svm_pipe.predict(X_eval)\n",
    "print('Accuracy: ', accuracy_score(y_eval, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8333248718078895\n",
      "F1 score:  0.8358335833583358\n"
     ]
    }
   ],
   "source": [
    "# check svm performance on partial data\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_eval)\n",
    "print('Accuracy: ', accuracy_score(y_eval, y_pred))\n",
    "print('F1 score: ', f1_score(y_eval, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the partial data is indeed a good representation of the full data and as of now\n",
    "There won't be a huge difference. To be even more fair in the comparison we could make an actual \n",
    "submission just with the partial data but for now it's fine. Let's now work on improving the classifier\n",
    "and keeping the same vectirizer that we know works well as a baseline.\n",
    "we'll then come back to it and play with embeddings and other vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5864852515611515\n",
      "F1 score:  0.36840880893300243\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # A few classifiers to have an initial idea and direction\n",
    "# # I still like the random forest, lets try\n",
    "# clf = RandomForestClassifier(\n",
    "#     max_depth=5\n",
    "#     )\n",
    "# train_test(clf, X_train, y_train, X_eval, y_eval)\n",
    "# # 34s\n",
    "# # Accuracy:  0.5864852515611515\n",
    "# # F1 score:  0.36840880893300243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.582779103416764\n",
      "F1 score:  0.3353283726949207\n"
     ]
    }
   ],
   "source": [
    "# clf = RandomForestClassifier(\n",
    "#     max_depth=5,\n",
    "#     n_estimators=100,\n",
    "#     criterion='entropy',\n",
    "#     min_samples_leaf=1,\n",
    "#     min_samples_split=5,\n",
    "# )\n",
    "# clf = train_test(clf, X_train, y_train, X_eval, y_eval)\n",
    "# # 34s\n",
    "# # Accuracy:  0.582779103416764\n",
    "# # F1 score:  0.3353283726949207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5370614814438747\n",
      "F1 score:  0.679799841980511\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(\n",
    "    max_depth=25,\n",
    "    n_estimators=3,\n",
    "    criterion='gini',\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    ")\n",
    "clf = train_test(clf, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7342742549626847\n",
      "F1 score:  0.7434062162957153\n"
     ]
    }
   ],
   "source": [
    "# clf = RandomForestClassifier(\n",
    "#     max_depth=None,\n",
    "#     n_estimators=3,\n",
    "#     criterion='gini',\n",
    "#     min_samples_leaf=1,\n",
    "#     min_samples_split=2,\n",
    "#     n_jobs=-1,\n",
    "# )\n",
    "# clf = train_test(clf, X_train, y_train, X_eval, y_eval)\n",
    "# # 7min \n",
    "# # Accuracy:  0.7342742549626847\n",
    "# # F1 score:  0.7434062162957153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7min 9s without the n_jobs=-1\n",
    "# Accuracy:  0.7342742549626847\n",
    "# F1 score:  0.7434062162957153\n",
    "# Improving but not much if we compare to the speed and accuracy of the linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]....*\n",
      "optimization finished, #iter = 49\n",
      "Objective value = -33163.060320\n",
      "nSV = 123234\n",
      "Accuracy:  0.8333248718078895\n",
      "F1 score:  0.8358335833583358\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(verbose=1)\n",
    "svm = train_test(svm, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =      3018686     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.09223D+05    |proj g|=  5.71000D+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "*****     38     47      1     0     0   1.324D-01   6.325D+04\n",
      "  F =   63254.813249193365     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "Accuracy:  0.8233741178859725\n",
      "F1 score:  0.8258933039735762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   11.4s finished\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=4000, verbose=1, n_jobs=-1)\n",
    "lr = train_test(lr, X_train, y_train, X_eval, y_eval)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, linear svm is still king but we can now start to add layers of complexity\n",
    "I'm quite sad about random forests, I find them so cool :(\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADA BOOST CLASSIFIER \n",
    "# Using lin SVM as base estimator - use svm linear and probability True\n",
    "# ada = AdaBoostClassifier( \n",
    "#     base_estimator= SVC(kernel='linear', probability=True, verbose=1),\n",
    "#     n_estimators=50,\n",
    "#     learning_rate=1,\n",
    "# )\n",
    "# ada = train_test(ada, X_train, y_train, X_eval, y_eval)\n",
    "# 145min into the computation using only one core............ I'll stop it here\n",
    "# I'm testing in another notebook in the meantime\n",
    "# [LibSVM]..............................................................\n",
    "# Warning: using -h 0 may be faster\n",
    "# *\n",
    "# optimization finished, #iter = 62644\n",
    "# obj = -0.794988, rho = 0.999154\n",
    "# nSV = 125286, nBSV = 125286\n",
    "# Total nSV = 125286\n",
    "# ............................................"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above cell has been running for the last 65min and counting, on a single poor cpu at 100% and I have the impression it hasn't even finished the first base estimator...\n",
    "\n",
    "Here's a few things I found:\n",
    "- I really consider speed as a measure of quality, so when a cell runs for more than 5min on 1/10 of the data I should get suspicious. In this spirit, we can see how using SVC instead of linearSVC does indeed expose us to the paramtere 'probability', but runs extremely slower. \n",
    "- Note how SVM is not base on probability, but on distance from the hyperplane. With optimized linear kernel calling linearSVC we can see how fast it is. Adding probability basically nullifies the optimizations that can be done around not having to compute the distance between every single point (I think), and this 'probability' is furthermore a proxy of probability found by 'Platt calibration'.\n",
    "- I need to reeavaluate from scratch the use of Decision Trees as a basis for either Random Forest (ensemble) or Boosting (sequential improvements on each subsequent tree). From previous testings we can see how deeper trees with more leaves are better, without too much worry about canceling out 'noisy' data.\n",
    "- We should definitely visualize the model, see if for examble SVC is able to perfrctly separate the train data or not. This would give insight on how much we are overfitting and how much we can improve the model by adding regularization or other techniques. I didn't play with the C parameter so in theory as of now we are not regularizing at all and we just try to fit perfectly.\n",
    "\n",
    "Trees:\n",
    "- Diving deeper into these, I think they should work much better if we create a lower dimensional space. As of now the vectorization with 1-4 grams is quite humongous and allows a linearSVC to quickly divide the space but it just overcomplicates a Decision Tree. Using embeddings and PCA could probably help, in an effort to increase the information density of the data, giving more 'context' to each word, not just by n-grams.\n",
    "\n",
    "Vectorization:\n",
    "- We should start playing with embeddings to give more context or reduce the token size.\n",
    "- We could try to visualize some vectorizations of tweets do have a better understanding of what we are feeding our models with. How are we handling the various 'lolololol', 'whyyyy' and such?\n",
    "\n",
    "Labels:\n",
    "- It shouldnt change much theoretically but maybe the libraries would be able to optimize better if we use a True and False instead of 1 and 0. We can try that. Also, same vibe but maybe -1 and 1 instead of 0 and 1. Could potentially speed up slightly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "110min in...\n",
    "Essencially we are throwing every single tweet composed of <140 words into a vector space which is absolutely huge.\n",
    "This assures that linearSVC is able to find hyperplane, but at a huge cost in the case above.\n",
    "What if we bounded the maximum amount of dimensions that the classifier can use?\n",
    "We could bound it by the average number of non-zero entries in the vectorized tweets.\n",
    "\n",
    "Also, we could then play around with C values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It finished first base estimator in 80+min...\n",
    "\n",
    "# [LibSVM]..............................................................\n",
    "# Warning: using -h 0 may be faster\n",
    "# *\n",
    "# optimization finished, #iter = 62644\n",
    "# obj = -0.794988, rho = 0.999154\n",
    "# nSV = 125286, nBSV = 125286\n",
    "# Total nSV = 125286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADA BOOST CLASSIFIER\n",
    "# Using decision tree as base estimator - I won't give u on u beautiful trees\n",
    "adatree = AdaBoostClassifier(\n",
    "    base_estimator= DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c927aa12d5ce5f7072c92979fe584a1fce73005a0de16af9e5cbcd0d6c1397"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
