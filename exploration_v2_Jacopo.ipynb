{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Notebook v2 - Jacopo\n",
    "\n",
    "**Version**: v2\n",
    "\n",
    "## I just Realized I've been using the 10% datasets the whole time...\n",
    "\n",
    "Use glove.twitter.27B as embeddings and/or better preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(df, vocab, embeddings, mode='avg'):\n",
    "    X = np.zeros((df.shape[0], embeddings.shape[1]))\n",
    "    for i, tweet in enumerate(df['tweet']):\n",
    "        words = tweet.split()\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                X[i] += embeddings[vocab[word]]\n",
    "        if mode == 'avg':\n",
    "            X[i] /= len(words)\n",
    "        elif mode == 'sum':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(mode))\n",
    "    return X\n",
    "def load_train_data(path_pos='data/twitter-datasets/train_pos_full.txt', path_neg='data/twitter-datasets/train_neg_full.txt'):\n",
    "    # Load data, txt as csv\n",
    "    #data_path = 'data/twitter-datasets/'\n",
    "    df_train_pos = pd.read_csv(path_pos, sep = '\\t', names = ['tweet'])\n",
    "    df_train_pos['label'] = 1\n",
    "    df_train_neg = pd.read_csv(path_neg, sep = '\\t', names = ['tweet'], on_bad_lines='skip')\n",
    "    df_train_neg['label'] = 0\n",
    "    df_train = pd.concat([df_train_pos, df_train_neg])\n",
    "    print('Train set: ', df_train.shape)\n",
    "    print('Train set positives: ', df_train_pos.shape)\n",
    "    print('Train set negatives: ', df_train_neg.shape)\n",
    "    return df_train   \n",
    "def load_test_data():\n",
    "    # Load test data: id, tweet for each row\n",
    "    data_path = 'data/twitter-datasets/'\n",
    "    df_test = pd.read_csv(data_path + 'test_data.txt', header=None, names=['line'], sep='\\t')\n",
    "    # Extract id and tweet, limit split by 1 so we don't split the tweet (this is v0, at least we keep it intact)\n",
    "    df_test['id'] = df_test['line'].apply(lambda x: x.split(',',1)[0]) \n",
    "    df_test['tweet'] = df_test['line'].apply(lambda x: x.split(',',1)[1])\n",
    "    df_test = df_test.drop('line', axis=1)\n",
    "    return df_test\n",
    "def predict_test_data(X_test, classifier, filename='submission.csv'):\n",
    "    # Predict test data and save to csv\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    df_test['Prediction'] = y_pred\n",
    "    df_test.rename(columns={'id': 'Id'}, inplace=True)\n",
    "    df_test['Prediction'] = df_test['Prediction'].apply(lambda x: -1 if x == 0 else x)\n",
    "    df_test.to_csv(filename, columns=['Id', 'Prediction'], index=False)\n",
    "    return df_test\n",
    "    \n",
    "def predict_test_data_pipeline(df_test, pipe, filename='submission.csv'):\n",
    "    # Predict test data and save to csv\n",
    "    y_pred = pipe.predict(df_test['tweet'])\n",
    "    df_test['Prediction'] = y_pred\n",
    "    df_test.rename(columns={'id': 'Id'}, inplace=True)\n",
    "    df_test['Prediction'] = df_test['Prediction'].apply(lambda x: -1 if x == 0 else x)\n",
    "    df_test.to_csv(filename, columns=['Id', 'Prediction'], index=False)\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load data, txt as csv\n",
    "data_path = 'data/twitter-datasets/'\n",
    "df_train_pos = pd.read_csv(data_path + 'train_pos_full.txt', sep = '\\t', names = ['tweet'])\n",
    "df_train_pos['label'] = 1\n",
    "df_train_neg = pd.read_csv(data_path + 'train_neg_full.txt', sep = '\\t', names = ['tweet'], on_bad_lines='skip')\n",
    "df_train_neg['label'] = 0\n",
    "df_train = pd.concat([df_train_pos, df_train_neg])\n",
    "print('Train set: ', df_train.shape)\n",
    "print('Train set positives: ', df_train_pos.shape)\n",
    "print('Train set negatives: ', df_train_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vecotizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# X_train = vectorizer.fit_transform(df_train['tweet'])\n",
    "# y_train = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # classifier, naive bayes\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# classifier = MultinomialNB()\n",
    "# # k-fold cross validation\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# scores = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "# print('Cross validation scores: ', scores)\n",
    "# print('Mean cross validation score: ', np.mean(scores))\n",
    "# # very fast and good results\n",
    "# # Cross validation scores:  [0.73153344 0.73257685 0.72950358 0.73167175 0.73085818]\n",
    "# # Mean cross validation score:  0.7312287581433473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.fit(X_train, y_train)\n",
    "# # Load test data: id, tweet for each row\n",
    "# df_test = load_test_data()\n",
    "# X_test = vectorizer.transform(df_test['tweet'])\n",
    "# # Predict test data and save to csv\n",
    "# df_test = predict_test_data(X_test, classifier, filename='data/out/submission-v2.csv')\n",
    "# # acc: 0.719 f1: 0.759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now do the same but with n-grams and embeddings\n",
    "# vectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "# X_train = vectorizer.fit_transform(df_train['tweet'])\n",
    "# y_train = df_train['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier, naive bayes, same as before, fast and good results\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "# k-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# scores = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "# print('Cross validation scores: ', scores)\n",
    "# print('Mean cross validation score: ', np.mean(scores))\n",
    "# Cross validation scores:  [0.78193219 0.78204609 0.7791274  0.78229627 0.78140134]\n",
    "# Mean cross validation score:  0.7813606585051834\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tri-grams as well\n",
    "# # vectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "# X_train = vectorizer.fit_transform(df_train['tweet'])\n",
    "# y_train = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # k-fold\n",
    "# scores = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "# print('Cross validation scores: ', scores)\n",
    "# print('Mean cross validation score: ', np.mean(scores))\n",
    "# # Cross validation scores:  [0.80305456 0.8033271  0.80066876 0.80417118 0.80257862]\n",
    "# # Mean cross validation score:  0.8027600430379593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.fit(X_train, y_train)\n",
    "# X_test = vectorizer.transform(df_test['tweet'])\n",
    "# # Predict test data and save to csv\n",
    "# df_test = predict_test_data(X_test, classifier, filename='data/out/submission-v2_1.csv')\n",
    "# # acc: 0.799\tf1: 0.820\n",
    "# # nice, starting to get better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lets keep same vectorizer but use a different classifier\n",
    "# # aparently it makes a big problem having thr n grams, let try with 1-2\n",
    "# # vectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "# X_train = vectorizer.fit_transform(df_train['tweet'])\n",
    "# y_train = df_train['label']\n",
    "# # classifier, random forest\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(\n",
    "#     n_estimators=100, \n",
    "#     max_depth=10, \n",
    "#     n_jobs=-1)\n",
    "# # cast values, otherwise we get: ValueError: buffer source array is read-only\n",
    "# # X_train = X_train.toarray()\n",
    "# # y_train = y_train.toarray()\n",
    "# # # k-fold cross validation\n",
    "# # scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "# # print('Cross validation scores: ', scores)\n",
    "# # print('Mean cross validation score: ', np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vectorizer', 'classifier']\n",
      "parameters:\n",
      "{'vectorizer__analyzer': ['word'],\n",
      " 'vectorizer__binary': [True, False],\n",
      " 'vectorizer__lowercase': [True, False],\n",
      " 'vectorizer__max_df': [0.9, 0.95, 1.0],\n",
      " 'vectorizer__max_features': [1000, 10000, 100000, None],\n",
      " 'vectorizer__min_df': [1, 5, 10, 15],\n",
      " 'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
      " 'vectorizer__stop_words': ['english']}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  21.5s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  22.1s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  22.5s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  23.0s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  22.3s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=True, vectorizer__max_df=0.9, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.4min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=True, vectorizer__max_df=0.9, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.4min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=True, vectorizer__max_df=0.9, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.4min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=True, vectorizer__max_df=0.9, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.4min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=True, vectorizer__max_df=0.9, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.3min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.4min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.4min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.4min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.4min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 3.3min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.4min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 3.6min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 3.7min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 3.7min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 3.7min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=1.0, vectorizer__max_features=10000, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  25.4s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=1.0, vectorizer__max_features=10000, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  24.0s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=1.0, vectorizer__max_features=10000, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  25.8s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=1.0, vectorizer__max_features=10000, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  26.4s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=1.0, vectorizer__max_features=10000, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  26.5s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.1min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacopoferro/opt/anaconda3/envs/ml/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.1min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.1min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__max_features=1000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english; total time= 1.0min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=10000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  23.2s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=10000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  23.0s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=10000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  23.6s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=10000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  22.8s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=10000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english; total time=  23.5s\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 3.3min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 3.5min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 3.7min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 3.8min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=False, vectorizer__lowercase=False, vectorizer__max_df=0.9, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 3.8min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=1.0, vectorizer__max_features=10000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 5.5min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 2.5min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 2.9min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 2.7min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 2.6min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__max_features=None, vectorizer__min_df=10, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 2.7min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=1.0, vectorizer__max_features=10000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 7.6min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=1.0, vectorizer__max_features=10000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 7.5min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=1.0, vectorizer__max_features=10000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 7.5min\n",
      "[CV] END vectorizer__analyzer=word, vectorizer__binary=True, vectorizer__lowercase=True, vectorizer__max_df=1.0, vectorizer__max_features=10000, vectorizer__min_df=15, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english; total time= 7.4min\n",
      "done in 761.731s\n",
      "\n",
      "Best score: 0.766\n",
      "Best parameters set:\n",
      "\tvectorizer__analyzer: 'word'\n",
      "\tvectorizer__binary: True\n",
      "\tvectorizer__lowercase: True\n",
      "\tvectorizer__max_df: 0.95\n",
      "\tvectorizer__max_features: None\n",
      "\tvectorizer__min_df: 10\n",
      "\tvectorizer__ngram_range: (1, 3)\n",
      "\tvectorizer__stop_words: 'english'\n"
     ]
    }
   ],
   "source": [
    "##============ VECTORIZER ================##\n",
    "# I get varous errors, also note that we are not refining at all the vectorizer\n",
    "# let's refine if a bit and keep iterating fast with nayve bayes for a new versione\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "## ========== CLASSIFIER ==========##\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "##========== PIPELINE ==========##\n",
    "#rand search for vectorizer ... how to do it? how to pass the vectorizer to the classifier?\n",
    "# need to give more info to sklearn, so we need to create a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)]\n",
    ")\n",
    "# now we can use the pipeline in the random search, all examples I've seen just add\n",
    "# a prefix with the name of preprocessor or classifier variable, not sure where it's said\n",
    "# explicitly but hey, this is python, we can do whatever we want\n",
    "# found this comment in example code from sklearn about the double underscore:\n",
    "# \"Parameters of pipelines can be set using '__' separated parameter names:\"\n",
    "\n",
    "\n",
    "##========== GRID PARAMS ==========##\n",
    "grid_params = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vectorizer__max_features': [1000, 10000, 100000, None], #max number of features\n",
    "    'vectorizer__min_df': [1, 5, 10, 15], #ignore terms with frequency last than this (nb of docs)\n",
    "    'vectorizer__max_df': [0.9, 0.95, 1.0], #ignore terms with frequency higher than this (ratio of docs in corpus)\n",
    "    'vectorizer__binary': [True, False], #if True, all non zero counts are set to 1 (should be better for our case)\n",
    "    'vectorizer__stop_words': ['english'], #ignore common words, default is english but we can pass a list of words, applies only if analyzer is a 'word'\n",
    "    'vectorizer__analyzer': ['word'],\n",
    "    'vectorizer__lowercase': [True, False], #convert all characters to lowercase before tokenizing\n",
    "    #'classifier__alpha': [1] # else error - Invalid parameter 'vectorizer' for estimator MultinomialNB(). --- fixed, was later\n",
    "}\n",
    "\n",
    "\n",
    "##========== SEARCH AND FIT ==========##\n",
    "rand_search = RandomizedSearchCV(\n",
    "    pipeline, # I forgot to pass the pipeline here, I was still passing the classifier\n",
    "    grid_params,\n",
    "    n_iter=10,\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    ")\n",
    "# stats \n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "pprint(grid_params)\n",
    "t0 = time()\n",
    "\n",
    "# rand_search.fit(X_train, y_train) # this is not working, we need to obtain these from vectorizer, let's try\n",
    "# to put just the data in a way it can be used by the pipeline\n",
    "classifier = rand_search.fit(df_train['tweet'], df_train['label']) # I was not equating this to classifier, had to fit it after again all over\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % rand_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = rand_search.best_estimator_.get_params()\n",
    "for param_name in sorted(grid_params.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure what I was doing below this cell but i dont wanna delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline: ['vectorizer', 'classifier']\n",
    "# parameters:\n",
    "# {'vectorizer__analyzer': ['word'],\n",
    "#  'vectorizer__binary': [True, False],\n",
    "#  'vectorizer__lowercase': [True, False],\n",
    "#  'vectorizer__max_df': [0.9, 0.95, 1.0],\n",
    "#  'vectorizer__max_features': [1000, 10000, 100000, None],\n",
    "#  'vectorizer__min_df': [1, 5, 10, 15],\n",
    "#  'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "#  'vectorizer__stop_words': ['english']}\n",
    "# done in 761.731s\n",
    "# ...\n",
    "# \tvectorizer__max_features: None\n",
    "# \tvectorizer__min_df: 10\n",
    "# \tvectorizer__ngram_range: (1, 3)\n",
    "# \tvectorizer__stop_words: 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.7657478048810253\n",
      "Best params:  {'vectorizer__stop_words': 'english', 'vectorizer__ngram_range': (1, 3), 'vectorizer__min_df': 10, 'vectorizer__max_features': None, 'vectorizer__max_df': 0.95, 'vectorizer__lowercase': True, 'vectorizer__binary': True, 'vectorizer__analyzer': 'word'}\n"
     ]
    }
   ],
   "source": [
    "print('Best score: ', rand_search.best_score_)\n",
    "print('Best params: ', rand_search.best_params_)\n",
    "# Best score:  0.7657478048810253\n",
    "# Best params:  {'vectorizer__stop_words': 'english', \n",
    "# 'vectorizer__ngram_range': (1, 3), \n",
    "# 'vectorizer__min_df': 10, \n",
    "# 'vectorizer__max_features': None, \n",
    "# 'vectorizer__max_df': 0.95, \n",
    "# 'vectorizer__lowercase': True, \n",
    "# 'vectorizer__binary': True, \n",
    "# 'vectorizer__analyzer': 'word'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.80647969 0.80734208 0.80398406 0.80755564 0.8056051 ]\n",
      "Mean:  0.8061933169127382\n"
     ]
    }
   ],
   "source": [
    "# worse than default?\n",
    "# my guess is the english stop words, let's try without them\n",
    "classifier = MultinomialNB()\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5,\n",
    "    max_features=None,\n",
    "    max_df=0.95,\n",
    "    binary=True,\n",
    "    stop_words=None,\n",
    "    lowercase=True,\n",
    "    analyzer='word',\n",
    ")\n",
    "pipeline = Pipeline(steps=[('vectorizer', vectorizer), ('classifier', classifier)])\n",
    "#pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "#kfold\n",
    "scores = cross_val_score(pipeline, df_train['tweet'], df_train['label'], cv=5)\n",
    "print(\"Scores: \", scores)\n",
    "print(\"Mean: \", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7min\n",
    "# Scores:  [0.80647969 0.80734208 0.80398406 0.80755564 0.8056051 ]\n",
    "# Mean:  0.8061933169127382\n",
    "\n",
    "# seems highest so far. need to explore more with n-grams but strategy seems that\n",
    "# more features and data is better\n",
    "\n",
    "# next: try with tfidf specifically\n",
    "# also note that in all tests I've done so far\n",
    "# result in BETTER test score than train score. \n",
    "# Not sure the exact reason, gotta look into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n",
      "                                               tweet  label\n",
      "0  <user> i dunno justin read my mention or not ....      1\n",
      "1  because your logic is so dumb , i won't even c...      1\n",
      "2   <user> just put casper in a box !  looved the...      1\n",
      "3  <user> <user> thanks sir > > don't trip lil ma...      1\n",
      "4  visiting my brother tmr is the bestest birthda...      1\n",
      "  id                                              tweet\n",
      "0  1  sea doo pro sea scooter ( sports with the port...\n",
      "1  2  <user> shucks well i work all week so now i ca...\n",
      "2  3            i cant stay away from bug thats my baby\n",
      "3  4  <user> no ma'am ! ! ! lol im perfectly fine an...\n",
      "4  5  whenever i fall asleep watching the tv , i alw...\n",
      "{'memory': None, 'steps': [('vectorizer', CountVectorizer(binary=True, max_df=0.95, min_df=5, ngram_range=(1, 3))), ('classifier', MultinomialNB())], 'verbose': False, 'vectorizer': CountVectorizer(binary=True, max_df=0.95, min_df=5, ngram_range=(1, 3)), 'classifier': MultinomialNB(), 'vectorizer__analyzer': 'word', 'vectorizer__binary': True, 'vectorizer__decode_error': 'strict', 'vectorizer__dtype': <class 'numpy.int64'>, 'vectorizer__encoding': 'utf-8', 'vectorizer__input': 'content', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.95, 'vectorizer__max_features': None, 'vectorizer__min_df': 5, 'vectorizer__ngram_range': (1, 3), 'vectorizer__preprocessor': None, 'vectorizer__stop_words': None, 'vectorizer__strip_accents': None, 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vectorizer__tokenizer': None, 'vectorizer__vocabulary': None, 'classifier__alpha': 1.0, 'classifier__class_prior': None, 'classifier__fit_prior': True}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sea doo pro sea scooter ( sports with the port...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;user&gt; shucks well i work all week so now i ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>i cant stay away from bug thats my baby</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;user&gt; no ma'am ! ! ! lol im perfectly fine an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>whenever i fall asleep watching the tv , i alw...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>had a nice time w / my friend lastnite</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>&lt;user&gt; no it's not ! please stop !</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>not without my daughter ( dvd two-time oscar (...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>&lt;user&gt; have fun in class sweetcheeks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>making a r . e . a . l . difference . ( get r ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              tweet  Prediction\n",
       "0         1  sea doo pro sea scooter ( sports with the port...          -1\n",
       "1         2  <user> shucks well i work all week so now i ca...           1\n",
       "2         3            i cant stay away from bug thats my baby           1\n",
       "3         4  <user> no ma'am ! ! ! lol im perfectly fine an...           1\n",
       "4         5  whenever i fall asleep watching the tv , i alw...          -1\n",
       "...     ...                                                ...         ...\n",
       "9995   9996             had a nice time w / my friend lastnite           1\n",
       "9996   9997                 <user> no it's not ! please stop !          -1\n",
       "9997   9998  not without my daughter ( dvd two-time oscar (...          -1\n",
       "9998   9999               <user> have fun in class sweetcheeks           1\n",
       "9999  10000  making a r . e . a . l . difference . ( get r ...          -1\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# fit, predict, score\n",
    "df_train = load_train_data()\n",
    "print(df_train.head())\n",
    "#pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "df_test = load_test_data()\n",
    "print(df_test.head())\n",
    "print(pipeline.get_params())\n",
    "predict_test_data_pipeline(df_test, pipeline, filename='data/out/submission-v2_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc: 0.799\tf1: 0.819\n",
    "# actually I shouldnt be surprised,\n",
    "# same model, same params a part from binary that doesnt change anything and\n",
    "# 0.95 max_df which indeed doesnt do much a part from reducing f1 slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.80573934 0.80637596 0.80270269 0.80606884 0.80438881]\n",
      "Mean:  0.8050551296732085\n"
     ]
    }
   ],
   "source": [
    "# what seemed to be woring is increasing n-grams\n",
    "classifier = MultinomialNB()\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 4),\n",
    "    min_df=10,\n",
    "    max_features=None,\n",
    "    max_df=0.98,\n",
    "    binary=True,\n",
    "    stop_words=None,\n",
    "    lowercase=True,\n",
    "    analyzer='word',\n",
    ")\n",
    "pipeline = Pipeline(steps=[('vectorizer', vectorizer), ('classifier', classifier)])\n",
    "#pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "#kfold\n",
    "scores = cross_val_score(pipeline, df_train['tweet'], df_train['label'], cv=5)\n",
    "print(\"Scores: \", scores)\n",
    "print(\"Mean: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7min 50s, similar to 1-3 but we removed a lot of 5-10 occurences features\n",
    "# Scores:  [0.80573934 0.80637596 0.80270269 0.80606884 0.80438881]\n",
    "# Mean:  0.8050551296732085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m X_train, X_eval, y_train, y_eval \u001b[39m=\u001b[39m train_test_split(df_train[\u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m], df_train[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m], test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[1;32m     22\u001b[0m pipeline\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 23\u001b[0m y_pred \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(metrics\u001b[39m.\u001b[39mclassification_report(y_eval, y_pred))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# next: better stop words, twitter specific\n",
    "# also no lowercase\n",
    "# let's try with tfidf\n",
    "classifier = MultinomialNB()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 4),\n",
    "    min_df=5,\n",
    "    max_features=None,\n",
    "    max_df=0.98,\n",
    "    binary=True,\n",
    "    stop_words=None,\n",
    "    lowercase=True,\n",
    "    analyzer='word',\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps=[('vectorizer', vectorizer), ('classifier', classifier)])\n",
    "#pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "# simple split, just to see if it works\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(df_train['tweet'], df_train['label'], test_size=0.2)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_eval)\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1min 30s with 4 grams! nice tfidf\n",
    "# y_pred = pipeline.predict(X_eval)\n",
    "# from sklearn import metrics\n",
    "# print(metrics.classification_report(y_eval, y_pred))\n",
    "#  precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.88      0.76      0.82    247847\n",
    "#            1       0.79      0.89      0.84    243812\n",
    "\n",
    "#     accuracy                           0.83    491659\n",
    "#    macro avg       0.83      0.83      0.83    491659\n",
    "# weighted avg       0.83      0.83      0.83    491659\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.76      0.82    247929\n",
      "           1       0.79      0.90      0.84    243730\n",
      "\n",
      "    accuracy                           0.83    491659\n",
      "   macro avg       0.84      0.83      0.83    491659\n",
      "weighted avg       0.84      0.83      0.83    491659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets lower min_df to 3 and increase max_df to 1\n",
    "classifier = MultinomialNB()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 4),\n",
    "    min_df=3,\n",
    "    max_features=None,\n",
    "    max_df=1.0,\n",
    "    binary=True,\n",
    "    stop_words=None,\n",
    "    lowercase=True,\n",
    "    analyzer='word',\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps=[('vectorizer', vectorizer), ('classifier', classifier)])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(df_train['tweet'], df_train['label'], test_size=0.2)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_eval)\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_eval, y_pred))\n",
    "# 1min 40s\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.88      0.76      0.82    247929\n",
    "#            1       0.79      0.90      0.84    243730\n",
    "\n",
    "#     accuracy                           0.83    491659\n",
    "#    macro avg       0.84      0.83      0.83    491659\n",
    "# weighted avg       0.84      0.83      0.83    491659\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n",
      "                                               tweet  label\n",
      "0  <user> i dunno justin read my mention or not ....      1\n",
      "1  because your logic is so dumb , i won't even c...      1\n",
      "2   <user> just put casper in a box !  looved the...      1\n",
      "3  <user> <user> thanks sir > > don't trip lil ma...      1\n",
      "4  visiting my brother tmr is the bestest birthda...      1\n",
      "  id                                              tweet\n",
      "0  1  sea doo pro sea scooter ( sports with the port...\n",
      "1  2  <user> shucks well i work all week so now i ca...\n",
      "2  3            i cant stay away from bug thats my baby\n",
      "3  4  <user> no ma'am ! ! ! lol im perfectly fine an...\n",
      "4  5  whenever i fall asleep watching the tv , i alw...\n",
      "{'memory': None, 'steps': [('vectorizer', TfidfVectorizer(binary=True, min_df=3, ngram_range=(1, 4))), ('classifier', MultinomialNB())], 'verbose': False, 'vectorizer': TfidfVectorizer(binary=True, min_df=3, ngram_range=(1, 4)), 'classifier': MultinomialNB(), 'vectorizer__analyzer': 'word', 'vectorizer__binary': True, 'vectorizer__decode_error': 'strict', 'vectorizer__dtype': <class 'numpy.float64'>, 'vectorizer__encoding': 'utf-8', 'vectorizer__input': 'content', 'vectorizer__lowercase': True, 'vectorizer__max_df': 1.0, 'vectorizer__max_features': None, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 4), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': None, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': None, 'vectorizer__strip_accents': None, 'vectorizer__sublinear_tf': False, 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vectorizer__tokenizer': None, 'vectorizer__use_idf': True, 'vectorizer__vocabulary': None, 'classifier__alpha': 1.0, 'classifier__class_prior': None, 'classifier__fit_prior': True}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sea doo pro sea scooter ( sports with the port...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;user&gt; shucks well i work all week so now i ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>i cant stay away from bug thats my baby</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;user&gt; no ma'am ! ! ! lol im perfectly fine an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>whenever i fall asleep watching the tv , i alw...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>had a nice time w / my friend lastnite</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>&lt;user&gt; no it's not ! please stop !</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>not without my daughter ( dvd two-time oscar (...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>&lt;user&gt; have fun in class sweetcheeks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>making a r . e . a . l . difference . ( get r ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              tweet  Prediction\n",
       "0         1  sea doo pro sea scooter ( sports with the port...          -1\n",
       "1         2  <user> shucks well i work all week so now i ca...           1\n",
       "2         3            i cant stay away from bug thats my baby           1\n",
       "3         4  <user> no ma'am ! ! ! lol im perfectly fine an...           1\n",
       "4         5  whenever i fall asleep watching the tv , i alw...          -1\n",
       "...     ...                                                ...         ...\n",
       "9995   9996             had a nice time w / my friend lastnite           1\n",
       "9996   9997                 <user> no it's not ! please stop !          -1\n",
       "9997   9998  not without my daughter ( dvd two-time oscar (...          -1\n",
       "9998   9999               <user> have fun in class sweetcheeks           1\n",
       "9999  10000  making a r . e . a . l . difference . ( get r ...          -1\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit, predict, score\n",
    "df_train = load_train_data()\n",
    "print(df_train.head())\n",
    "pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "df_test = load_test_data()\n",
    "print(df_test.head())\n",
    "print(pipeline.get_params())\n",
    "predict_test_data_pipeline(df_test, pipeline, filename='data/out/submission-v2_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc: 0.822\tf1: 0.834\n",
    "# 1min51 from scratch\n",
    "# best so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(binary=True, min_df=3, ngram_range=(1, 4))\n",
      "MultinomialNB()\n"
     ]
    }
   ],
   "source": [
    "# let's save the tfidf vectorizer as a pickle file so we can use it later without having to recompute it\n",
    "# and we can iter more on classifier\n",
    "# TfidfVectorizer(binary=True, min_df=3, ngram_range=(1, 4))\n",
    "# MultinomialNB()\n",
    "print(pipeline.steps[0][1])\n",
    "print(pipeline.steps[1][1])\n",
    "import pickle\n",
    "with open('data/out/trained/tfidf_vectorizer-multinomialNB-pipeline-v2_3.pickle', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86    248065\n",
      "           1       0.85      0.87      0.86    243594\n",
      "\n",
      "    accuracy                           0.86    491659\n",
      "   macro avg       0.86      0.86      0.86    491659\n",
      "weighted avg       0.86      0.86      0.86    491659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's try with a different classifier\n",
    "# SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC()\n",
    "# take the same vectorizer as before in the pipeline, take from the pickle file\n",
    "with open('data/out/trained/tfidf_vectorizer-multinomialNB-pipeline-v2_3.pickle', 'rb') as f:\n",
    "    pipeline = pickle.load(f)\n",
    "pipeline.steps[1] = ('classifier', classifier)\n",
    "\n",
    "# split to check if it works\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(df_train['tweet'], df_train['label'], test_size=0.2)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_eval)\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_eval, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2min 20s\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.87      0.85      0.86    248065\n",
    "#            1       0.85      0.87      0.86    243594\n",
    "\n",
    "#     accuracy                           0.86    491659\n",
    "#    macro avg       0.86      0.86      0.86    491659\n",
    "# weighted avg       0.86      0.86      0.86    491659\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (2458295, 2)\n",
      "Train set positives:  (1218655, 2)\n",
      "Train set negatives:  (1239640, 2)\n",
      "                                               tweet  label\n",
      "0  <user> i dunno justin read my mention or not ....      1\n",
      "1  because your logic is so dumb , i won't even c...      1\n",
      "2   <user> just put casper in a box !  looved the...      1\n",
      "3  <user> <user> thanks sir > > don't trip lil ma...      1\n",
      "4  visiting my brother tmr is the bestest birthda...      1\n",
      "  id                                              tweet\n",
      "0  1  sea doo pro sea scooter ( sports with the port...\n",
      "1  2  <user> shucks well i work all week so now i ca...\n",
      "2  3            i cant stay away from bug thats my baby\n",
      "3  4  <user> no ma'am ! ! ! lol im perfectly fine an...\n",
      "4  5  whenever i fall asleep watching the tv , i alw...\n",
      "{'memory': None, 'steps': [('vectorizer', TfidfVectorizer(binary=True, min_df=3, ngram_range=(1, 4))), ('classifier', LinearSVC())], 'verbose': False, 'vectorizer': TfidfVectorizer(binary=True, min_df=3, ngram_range=(1, 4)), 'classifier': LinearSVC(), 'vectorizer__analyzer': 'word', 'vectorizer__binary': True, 'vectorizer__decode_error': 'strict', 'vectorizer__dtype': <class 'numpy.float64'>, 'vectorizer__encoding': 'utf-8', 'vectorizer__input': 'content', 'vectorizer__lowercase': True, 'vectorizer__max_df': 1.0, 'vectorizer__max_features': None, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 4), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': None, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': None, 'vectorizer__strip_accents': None, 'vectorizer__sublinear_tf': False, 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vectorizer__tokenizer': None, 'vectorizer__use_idf': True, 'vectorizer__vocabulary': None, 'classifier__C': 1.0, 'classifier__class_weight': None, 'classifier__dual': True, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__loss': 'squared_hinge', 'classifier__max_iter': 1000, 'classifier__multi_class': 'ovr', 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__tol': 0.0001, 'classifier__verbose': 0}\n",
      "TfidfVectorizer(binary=True, min_df=3, ngram_range=(1, 4))\n",
      "LinearSVC()\n"
     ]
    }
   ],
   "source": [
    "# fit, predict, score\n",
    "df_train = load_train_data()\n",
    "print(df_train.head())\n",
    "pipeline.fit(df_train['tweet'], df_train['label'])\n",
    "df_test = load_test_data()\n",
    "print(df_test.head())\n",
    "print(pipeline.get_params())\n",
    "predict_test_data_pipeline(df_test, pipeline, filename='data/out/submission-v2_4.csv')\n",
    "\n",
    "# save the pipeline\n",
    "print(pipeline.steps[0][1])\n",
    "print(pipeline.steps[1][1])\n",
    "import pickle\n",
    "with open('data/out/trained/tfidf_vectorizer-linSVC-pipeline-v2_4.pickle', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc: 0.848\tf1: 0.850\n",
    "# 2min 45s from scratch\n",
    "# best so far, nice\n",
    "# next in v3: \n",
    "# more complex preprocessing with external data, vocab, etc (spaCy, gensim, nltk, etc)\n",
    "# more complex classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # csv with word next to its embedding of d = 200\n",
    "# vocab_embeddings = pd.read_csv('data/glove/glove.twitter.27B.25d.txt', sep='\\r', index_col=0, names=['line'], nrows=10000)\n",
    "# vocab_embeddings['word'] = vocab_embeddings.index.str.split(' ', 1).str[0]\n",
    "# vocab_embeddings['embedding'] = vocab_embeddings.index.str.split(' ', 1).str[1]\n",
    "# vocab_embeddings['embedding'] = vocab_embeddings['embedding'].apply(lambda x: np.fromstring(x, dtype=float, sep=' '))\n",
    "# vocab_embeddings = vocab_embeddings.reset_index(drop=True)\n",
    "# vocab_embeddings.head(20)\n",
    "# print(vocab_embeddings.head(-20))\n",
    "# # Build vocabulary\n",
    "# vocab = {}\n",
    "# for i, word in enumerate(vocab_embeddings['word']):\n",
    "#     vocab[word] = i \n",
    "# print('Vocabulary size: ', len(vocab))\n",
    "# # Build embeddings matrix\n",
    "# embeddings = np.zeros((len(vocab), vocab_embeddings['embedding'][0].shape[0]))\n",
    "# for i, embedding in enumerate(vocab_embeddings['embedding']):\n",
    "#     embeddings[i] = embedding\n",
    "# print('Embeddings shape: ', embeddings.shape)\n",
    "# # Build feature matrix\n",
    "# X_train = build_feature_matrix(df_train, vocab, embeddings, mode='avg')\n",
    "# print('Feature matrix shape: ', X_train.shape)\n",
    "# # n-grams\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Build vocabulary \n",
    "# vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=10000)\n",
    "# vectorizer.fit(df_train['tweet'])\n",
    "\n",
    "\n",
    "# # Build feature matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_full = build_feature_matrix(df_train, vocab, embeddings, mode='avg')\n",
    "# y_train_full = df_train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('X_train_full shape: ', X_train_full.shape)\n",
    "# print('y_train_full shape: ', y_train_full.shape)\n",
    "# print('Embeddings shape: ', embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random forest classifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # let's try a shallower structure \n",
    "# clf = RandomForestClassifier(\n",
    "#     n_estimators=250, \n",
    "#     max_depth=5,\n",
    "#     n_jobs=-1,\n",
    "#     min_samples_split=15,\n",
    "#     verbose=2\n",
    "# )\n",
    "# scores = cross_val_score(clf, X_train_full, y_train_full, cv=5)\n",
    "# print('Cross validation scores: ', scores)\n",
    "# print('Mean cross validation score: ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load test data: id, tweet for each row\n",
    "# df_test = load_test_data()\n",
    "# X_test = build_feature_matrix(df_test, vocab, embeddings, mode='avg')\n",
    "\n",
    "# # pred\n",
    "# predict_test_data(X_test, clf, filename='data/out/submission-v2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66c927aa12d5ce5f7072c92979fe584a1fce73005a0de16af9e5cbcd0d6c1397"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
